{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append('/data/condor_shared/users/ssued/RNOGCnn')\n",
    "import utils\n",
    "\n",
    "data_path = Path(\"data/\")\n",
    "event_path = data_path / 'eventbatch_k'\n",
    "\n",
    "eventbatch = utils.obtain_evb('eventbatch.pkl')\n",
    "\n",
    "with open(event_path, 'rb') as file:\n",
    "    eventbatch_k = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_eventbatch = 3741\n",
      "Size of test_eventbatch = 1248\n"
     ]
    }
   ],
   "source": [
    "# Split the eventbatch_k into 75% training and 25% testing\n",
    "# Calculate the split index\n",
    "split_index = int(len(eventbatch) * 0.75)\n",
    "\n",
    "# Split the eventbatch_k into 75% training and 25% testing\n",
    "train_eventbatch = dict(list(eventbatch.items())[:split_index])\n",
    "test_eventbatch = dict(list(eventbatch.items())[split_index:])\n",
    "\n",
    "print(f'Size of train_eventbatch = {len(train_eventbatch.keys())}')\n",
    "print(f'Size of test_eventbatch = {len(test_eventbatch.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple, Dict, List\n",
    "from torch.nn.functional import normalize\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Custom dataset class to work with dictionaries\n",
    "\n",
    "class EventtoData(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling event data.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    events : np.ndarray\n",
    "        Array containing event data.\n",
    "    n_channels : int\n",
    "        Number of channels in the event data.\n",
    "    n_bins : int\n",
    "        Number of bins in the event data.\n",
    "    bin_time : float\n",
    "        Time duration of each bin.\n",
    "    transform : callable, optional\n",
    "        Optional transform to be applied on a sample (currently deprecated).\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    __len__() -> int:\n",
    "        Returns the number of events.\n",
    "    __getitem__(index: int) -> Tuple[np.ndarray, float]:\n",
    "        Returns a tuple (data, mean_SNR) for the given index.\n",
    "    show_event(index: int):\n",
    "        Displays the event data as an image using a utility function.\n",
    "    mean_snr_of(index: int) -> float:\n",
    "        Returns the mean SNR of the event at the given index.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The class now supports initialization with either an events dictionary or a file containing events.\n",
    "    - The event data is converted from a dictionary to a numpy array for easier handling.\n",
    "    - The `__getitem__` method ensures that the data includes a color dimension and converts data types to float32.\n",
    "    \"\"\"\n",
    "    def __init__(self, events=None, events_f=None, transform=None):\n",
    "        if not events_f and not events:\n",
    "            raise ValueError(\"Must include either events file or events to construct EventData object.\")\n",
    "        if events_f:\n",
    "            with open(events_f, 'rb') as file:\n",
    "                events_dict = pickle.load(file)\n",
    "        else:\n",
    "            events_dict = events\n",
    "\n",
    "        # Must convert from dictionary to arrays\n",
    "        self.events = np.array([events[key] for key in events.keys()])\n",
    "        first_key = next(iter(events_dict))\n",
    "        self.n_channels = events_dict[first_key]['data'].shape[0]\n",
    "        self.n_bins = events_dict[first_key]['data'].shape[1]\n",
    "        self.bin_time = events_dict[first_key]['bin_time'].item()\n",
    "        self.transform = transform # deprecated for now\n",
    "\n",
    "    def show_event(self, index : int):\n",
    "        utils.plot_image(self.events[index])\n",
    "\n",
    "    def mean_snr_of(self, index : int):\n",
    "        return self.events[index]['mean_SNR'].item()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.events)\n",
    "\n",
    "    # Must overwrite __getitem__():\n",
    "    def __getitem__(self, index : int) -> Tuple:\n",
    "        item_data_uncolored = self.events[index]['data']\n",
    "        item_data = torch.tensor(np.array([item_data_uncolored]).astype('float32')) # Must add a color dimension\n",
    "        item_snr = torch.tensor(np.array(self.events[index]['mean_SNR']).astype('float32'))\n",
    "\n",
    "        item = (normalize(item_data),item_snr)\n",
    "\n",
    "        return item # Returns (data, mean_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data into DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = EventtoData(train_eventbatch)\n",
    "test_data = EventtoData(test_eventbatch)\n",
    "\n",
    "BATCH_SIZE = 50 # Small batch size just to test batch size feature\n",
    "\n",
    "train_data_loader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_data_loader = DataLoader(dataset = test_data, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Let there be light!\n",
    "    \n",
    "class RNOG_CNN_alpha_1(nn.Module):\n",
    "\n",
    "    def __init__(self,input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        KERNEL_WIDTH = 5\n",
    "        KERNEL_HEIGHT = 2\n",
    "        KERNEL_SIZE = (KERNEL_HEIGHT,KERNEL_WIDTH)\n",
    "\n",
    "        self.conv_block_1=nn.Sequential( # Blocks are collections of layers\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=(3,12),\n",
    "                      stride=1), # Establish hyper parameters #\n",
    "            #nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=output_shape,\n",
    "                      kernel_size=(2,14), # Will output (1,10)\n",
    "                      stride=1),\n",
    "            #nn.ReLU(),\n",
    "            # nn.Conv2d(in_channels=hidden_units,\n",
    "            #           out_channels=hidden_units,\n",
    "            #           kernel_size=(2,4), \n",
    "            #           stride=1),\n",
    "            # #nn.ReLU(),\n",
    "            # nn.Conv2d(in_channels=hidden_units,\n",
    "            #           out_channels=output_shape,\n",
    "            #           kernel_size=(1,5),\n",
    "            #           stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_block_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "RNOG_CNN_alpha_1                         [1, 1, 1, 1]              --\n",
       "├─Sequential: 1-1                        [1, 1, 1, 1]              --\n",
       "│    └─Conv2d: 2-1                       [1, 10, 2, 14]            370\n",
       "│    └─Conv2d: 2-2                       [1, 1, 1, 1]              281\n",
       "==========================================================================================\n",
       "Total params: 651\n",
       "Trainable params: 651\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model_test = RNOG_CNN_alpha_1(1,10,1)\n",
    "\n",
    "summary(model_test, input_size=[1, 1, 4, 25]) # do a test pass through of an example input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_step function:\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss\n",
    "    #train_loss = 0\n",
    "    train_loss_arr = []\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "        y = y.view_as(y_pred)\n",
    "\n",
    "        # 2. Calculate and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        #train_loss += loss.item()\n",
    "        train_loss_arr.append(loss)\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Adjust metrics to get average loss per batch\n",
    "    #train_loss = train_loss / len(dataloader)\n",
    "    train_loss = max(train_loss_arr)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test step function:\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test values\n",
    "    #test_loss = 0\n",
    "    test_loss_arr = []\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "            y = y.view_as(test_pred)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            #test_loss += loss.item()\n",
    "            test_loss_arr.append(loss)\n",
    "            \n",
    "    # Adjust metrics to get average loss per batch \n",
    "    #test_loss = test_loss / len(dataloader)\n",
    "    test_loss = max(test_loss_arr)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function (Combines both steps)\n",
    "\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5,\n",
    "          targ_loss = 0):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"test_loss\": [],\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model=model,\n",
    "                                dataloader=train_dataloader,\n",
    "                                loss_fn=loss_fn,\n",
    "                                optimizer=optimizer)\n",
    "        test_loss = test_step(model=model,\n",
    "                              dataloader=test_dataloader,\n",
    "                              loss_fn=loss_fn)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch} | \"\n",
    "            f\"train_loss: {train_loss:.10f} | \"\n",
    "            f\"test_loss: {test_loss:.10f} | \"\n",
    "        )\n",
    "        if scheduler:\n",
    "            print(f\"learning_rate: {scheduler.get_last_lr():.10f}\")\n",
    "\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        # Ensure all data is moved to CPU and converted to float for storage\n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "\n",
    "        if targ_loss != 0:\n",
    "            #epochs += 1\n",
    "            curr_loss = results['train_loss'][epoch]\n",
    "            if curr_loss <= targ_loss:\n",
    "                break\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43df6dca5d1471896be99d552832e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | train_loss: 170.3528137207 | test_loss: 10.0248327255 | \n",
      "Epoch: 1 | train_loss: 171.2724456787 | test_loss: 10.0572042465 | \n",
      "Epoch: 2 | train_loss: 171.1938323975 | test_loss: 10.0418796539 | \n",
      "Epoch: 3 | train_loss: 171.1125946045 | test_loss: 10.0250778198 | \n",
      "Epoch: 4 | train_loss: 171.0181427002 | test_loss: 10.0119676590 | \n",
      "Epoch: 5 | train_loss: 170.9223785400 | test_loss: 10.0019979477 | \n",
      "Epoch: 6 | train_loss: 170.8283843994 | test_loss: 9.9941768646 | \n",
      "Epoch: 7 | train_loss: 170.7358398438 | test_loss: 9.9877414703 | \n",
      "Epoch: 8 | train_loss: 170.6431884766 | test_loss: 9.9821395874 | \n",
      "Epoch: 9 | train_loss: 170.5489654541 | test_loss: 9.9769916534 | \n",
      "Epoch: 10 | train_loss: 170.4515686035 | test_loss: 9.9720039368 | \n",
      "Epoch: 11 | train_loss: 170.3495178223 | test_loss: 9.9669198990 | \n",
      "Epoch: 12 | train_loss: 170.2412872314 | test_loss: 9.9614915848 | \n",
      "Epoch: 13 | train_loss: 170.1255493164 | test_loss: 9.9554576874 | \n",
      "Epoch: 14 | train_loss: 170.0008239746 | test_loss: 9.9485330582 | \n",
      "Epoch: 15 | train_loss: 169.8658752441 | test_loss: 9.9404230118 | \n",
      "Epoch: 16 | train_loss: 169.7194671631 | test_loss: 9.9308090210 | \n",
      "Epoch: 17 | train_loss: 169.5609130859 | test_loss: 9.9193906784 | \n",
      "Epoch: 18 | train_loss: 169.3896636963 | test_loss: 9.9058961868 | \n",
      "Epoch: 19 | train_loss: 169.2056427002 | test_loss: 9.8901071548 | \n",
      "Epoch: 20 | train_loss: 169.0090179443 | test_loss: 9.8718910217 | \n",
      "Epoch: 21 | train_loss: 168.8004913330 | test_loss: 9.8512039185 | \n",
      "Epoch: 22 | train_loss: 168.5808563232 | test_loss: 9.8281106949 | \n",
      "Epoch: 23 | train_loss: 168.3514404297 | test_loss: 9.8027620316 | \n",
      "Epoch: 24 | train_loss: 168.1135101318 | test_loss: 9.7753934860 | \n",
      "Epoch: 25 | train_loss: 167.8685150146 | test_loss: 9.7462987900 | \n",
      "Epoch: 26 | train_loss: 167.6179809570 | test_loss: 9.7157917023 | \n",
      "Epoch: 27 | train_loss: 167.3633270264 | test_loss: 9.6841964722 | \n",
      "Epoch: 28 | train_loss: 167.1058044434 | test_loss: 9.6518192291 | \n",
      "Epoch: 29 | train_loss: 166.8465881348 | test_loss: 9.6189422607 | \n",
      "Epoch: 30 | train_loss: 166.5864257812 | test_loss: 9.5857992172 | \n",
      "Epoch: 31 | train_loss: 166.3261108398 | test_loss: 9.5525932312 | \n",
      "Epoch: 32 | train_loss: 166.0662078857 | test_loss: 9.5194778442 | \n",
      "Epoch: 33 | train_loss: 165.8070373535 | test_loss: 9.4865770340 | \n",
      "Epoch: 34 | train_loss: 165.5487823486 | test_loss: 9.4539833069 | \n",
      "Epoch: 35 | train_loss: 165.2918090820 | test_loss: 9.4217624664 | \n",
      "Epoch: 36 | train_loss: 165.0360412598 | test_loss: 9.3899669647 | \n",
      "Epoch: 37 | train_loss: 164.7816772461 | test_loss: 9.3586282730 | \n",
      "Epoch: 38 | train_loss: 164.5288696289 | test_loss: 9.3277778625 | \n",
      "Epoch: 39 | train_loss: 164.2775573730 | test_loss: 9.2974319458 | \n",
      "Epoch: 40 | train_loss: 164.0279083252 | test_loss: 9.2676057816 | \n",
      "Epoch: 41 | train_loss: 163.7801513672 | test_loss: 9.2383155823 | \n",
      "Epoch: 42 | train_loss: 163.5341949463 | test_loss: 9.2095689774 | \n",
      "Epoch: 43 | train_loss: 163.2903289795 | test_loss: 9.1813735962 | \n",
      "Epoch: 44 | train_loss: 163.0485687256 | test_loss: 9.1537408829 | \n",
      "Epoch: 45 | train_loss: 162.8090362549 | test_loss: 9.1266794205 | \n",
      "Epoch: 46 | train_loss: 162.5718841553 | test_loss: 9.1001920700 | \n",
      "Epoch: 47 | train_loss: 162.3371582031 | test_loss: 9.0742864609 | \n",
      "Epoch: 48 | train_loss: 162.1049804688 | test_loss: 9.0489645004 | \n",
      "Epoch: 49 | train_loss: 161.8753967285 | test_loss: 9.0242290497 | \n",
      "Epoch: 50 | train_loss: 161.6484375000 | test_loss: 9.0000801086 | \n",
      "Epoch: 51 | train_loss: 161.4242248535 | test_loss: 8.9765195847 | \n",
      "Epoch: 52 | train_loss: 161.2027130127 | test_loss: 8.9535522461 | \n",
      "Epoch: 53 | train_loss: 160.9840087891 | test_loss: 8.9311637878 | \n",
      "Epoch: 54 | train_loss: 160.7680206299 | test_loss: 8.9093570709 | \n",
      "Epoch: 55 | train_loss: 160.5548095703 | test_loss: 8.8881254196 | \n",
      "Epoch: 56 | train_loss: 160.3443908691 | test_loss: 8.8674631119 | \n",
      "Epoch: 57 | train_loss: 160.1367340088 | test_loss: 8.8473653793 | \n",
      "Epoch: 58 | train_loss: 159.9317626953 | test_loss: 8.8278198242 | \n",
      "Epoch: 59 | train_loss: 159.7295074463 | test_loss: 8.8088188171 | \n",
      "Epoch: 60 | train_loss: 159.5299224854 | test_loss: 8.7903556824 | \n",
      "Epoch: 61 | train_loss: 159.3329925537 | test_loss: 8.7724208832 | \n",
      "Epoch: 62 | train_loss: 159.1386718750 | test_loss: 8.7550020218 | \n",
      "Epoch: 63 | train_loss: 158.9468994141 | test_loss: 8.7380857468 | \n",
      "Epoch: 64 | train_loss: 158.7576446533 | test_loss: 8.7216634750 | \n",
      "Epoch: 65 | train_loss: 158.5708618164 | test_loss: 8.7057237625 | \n",
      "Epoch: 66 | train_loss: 158.3864593506 | test_loss: 8.6902532578 | \n",
      "Epoch: 67 | train_loss: 158.2043914795 | test_loss: 8.6752376556 | \n",
      "Epoch: 68 | train_loss: 158.0245666504 | test_loss: 8.6606645584 | \n",
      "Epoch: 69 | train_loss: 157.8469696045 | test_loss: 8.6465215683 | \n",
      "Epoch: 70 | train_loss: 157.6714477539 | test_loss: 8.6327934265 | \n",
      "Epoch: 71 | train_loss: 157.4979248047 | test_loss: 8.6194648743 | \n",
      "Epoch: 72 | train_loss: 157.3264007568 | test_loss: 8.6065235138 | \n",
      "Epoch: 73 | train_loss: 157.1567230225 | test_loss: 8.5939540863 | \n",
      "Epoch: 74 | train_loss: 156.9888916016 | test_loss: 8.5817422867 | \n",
      "Epoch: 75 | train_loss: 156.8227691650 | test_loss: 8.5698709488 | \n",
      "Epoch: 76 | train_loss: 156.6583557129 | test_loss: 8.5583286285 | \n",
      "Epoch: 77 | train_loss: 156.4955291748 | test_loss: 8.5470981598 | \n",
      "Epoch: 78 | train_loss: 156.3342132568 | test_loss: 8.5361661911 | \n",
      "Epoch: 79 | train_loss: 156.1744537354 | test_loss: 8.5255117416 | \n",
      "Epoch: 80 | train_loss: 156.0160675049 | test_loss: 8.5151243210 | \n",
      "Epoch: 81 | train_loss: 155.8590850830 | test_loss: 8.5049819946 | \n",
      "Epoch: 82 | train_loss: 155.7033843994 | test_loss: 8.4950675964 | \n",
      "Epoch: 83 | train_loss: 155.5489807129 | test_loss: 8.4853649139 | \n",
      "Epoch: 84 | train_loss: 155.3957519531 | test_loss: 8.4758529663 | \n",
      "Epoch: 85 | train_loss: 155.2436370850 | test_loss: 8.4665126801 | \n",
      "Epoch: 86 | train_loss: 155.0926818848 | test_loss: 8.4573183060 | \n",
      "Epoch: 87 | train_loss: 154.9427185059 | test_loss: 8.4482545853 | \n",
      "Epoch: 88 | train_loss: 154.7937164307 | test_loss: 8.4392938614 | \n",
      "Epoch: 89 | train_loss: 154.6456298828 | test_loss: 8.4304141998 | \n",
      "Epoch: 90 | train_loss: 154.4983825684 | test_loss: 8.4215917587 | \n",
      "Epoch: 91 | train_loss: 154.3518981934 | test_loss: 8.4127988815 | \n",
      "Epoch: 92 | train_loss: 154.2061767578 | test_loss: 8.4040164948 | \n",
      "Epoch: 93 | train_loss: 154.0610351562 | test_loss: 8.3952150345 | \n",
      "Epoch: 94 | train_loss: 153.9164886475 | test_loss: 8.3863735199 | \n",
      "Epoch: 95 | train_loss: 153.7723999023 | test_loss: 8.3774681091 | \n",
      "Epoch: 96 | train_loss: 153.6287841797 | test_loss: 8.3684740067 | \n",
      "Epoch: 97 | train_loss: 153.4854583740 | test_loss: 8.3593740463 | \n",
      "Epoch: 98 | train_loss: 153.3424224854 | test_loss: 8.3501405716 | \n",
      "Epoch: 99 | train_loss: 153.1995544434 | test_loss: 8.3407630920 | \n",
      "Epoch: 100 | train_loss: 153.0568237305 | test_loss: 8.3312206268 | \n",
      "Epoch: 101 | train_loss: 152.9141387939 | test_loss: 8.3214998245 | \n",
      "Epoch: 102 | train_loss: 152.7714538574 | test_loss: 8.3115911484 | \n",
      "Epoch: 103 | train_loss: 152.6286926270 | test_loss: 8.3014802933 | \n",
      "Epoch: 104 | train_loss: 152.4857940674 | test_loss: 8.2911653519 | \n",
      "Epoch: 105 | train_loss: 152.3427734375 | test_loss: 8.2806358337 | \n",
      "Epoch: 106 | train_loss: 152.1995544434 | test_loss: 8.2698898315 | \n",
      "Epoch: 107 | train_loss: 152.0560607910 | test_loss: 8.2589282990 | \n",
      "Epoch: 108 | train_loss: 151.9123535156 | test_loss: 8.2477512360 | \n",
      "Epoch: 109 | train_loss: 151.7683258057 | test_loss: 8.2363615036 | \n",
      "Epoch: 110 | train_loss: 151.6239776611 | test_loss: 8.2247676849 | \n",
      "Epoch: 111 | train_loss: 151.4793395996 | test_loss: 8.2129707336 | \n",
      "Epoch: 112 | train_loss: 151.3343505859 | test_loss: 8.2009792328 | \n",
      "Epoch: 113 | train_loss: 151.1890106201 | test_loss: 8.1888046265 | \n",
      "Epoch: 114 | train_loss: 151.0433044434 | test_loss: 8.1764516830 | \n",
      "Epoch: 115 | train_loss: 150.8972778320 | test_loss: 8.1639318466 | \n",
      "Epoch: 116 | train_loss: 150.7508850098 | test_loss: 8.1512546539 | \n",
      "Epoch: 117 | train_loss: 150.6040954590 | test_loss: 8.1384286880 | \n",
      "Epoch: 118 | train_loss: 150.4569854736 | test_loss: 8.1254625320 | \n",
      "Epoch: 119 | train_loss: 150.3095245361 | test_loss: 8.1123666763 | \n",
      "Epoch: 120 | train_loss: 150.1617126465 | test_loss: 8.0991477966 | \n",
      "Epoch: 121 | train_loss: 150.0135955811 | test_loss: 8.0858125687 | \n",
      "Epoch: 122 | train_loss: 149.8651428223 | test_loss: 8.0723705292 | \n",
      "Epoch: 123 | train_loss: 149.7163696289 | test_loss: 8.0588274002 | \n",
      "Epoch: 124 | train_loss: 149.5673217773 | test_loss: 8.0451831818 | \n",
      "Epoch: 125 | train_loss: 149.4179534912 | test_loss: 8.0314521790 | \n",
      "Epoch: 126 | train_loss: 149.2682952881 | test_loss: 8.0176277161 | \n",
      "Epoch: 127 | train_loss: 149.1184234619 | test_loss: 8.0037231445 | \n",
      "Epoch: 128 | train_loss: 148.9682464600 | test_loss: 7.9897389412 | \n",
      "Epoch: 129 | train_loss: 148.8178100586 | test_loss: 7.9756746292 | \n",
      "Epoch: 130 | train_loss: 148.6671600342 | test_loss: 7.9615340233 | \n",
      "Epoch: 131 | train_loss: 148.5162811279 | test_loss: 7.9473218918 | \n",
      "Epoch: 132 | train_loss: 148.3651580811 | test_loss: 7.9330415726 | \n",
      "Epoch: 133 | train_loss: 148.2138214111 | test_loss: 7.9186873436 | \n",
      "Epoch: 134 | train_loss: 148.0622711182 | test_loss: 7.9042682648 | \n",
      "Epoch: 135 | train_loss: 147.9104919434 | test_loss: 7.8897829056 | \n",
      "Epoch: 136 | train_loss: 147.7584991455 | test_loss: 7.8752317429 | \n",
      "Epoch: 137 | train_loss: 147.6063232422 | test_loss: 7.8606176376 | \n",
      "Epoch: 138 | train_loss: 147.4539184570 | test_loss: 7.8459415436 | \n",
      "Epoch: 139 | train_loss: 147.3013153076 | test_loss: 7.8312029839 | \n",
      "Epoch: 140 | train_loss: 147.1484985352 | test_loss: 7.8164024353 | \n",
      "Epoch: 141 | train_loss: 146.9954833984 | test_loss: 7.8015460968 | \n",
      "Epoch: 142 | train_loss: 146.8422393799 | test_loss: 7.7866292000 | \n",
      "Epoch: 143 | train_loss: 146.6888122559 | test_loss: 7.7716546059 | \n",
      "Epoch: 144 | train_loss: 146.5351562500 | test_loss: 7.7566246986 | \n",
      "Epoch: 145 | train_loss: 146.3812866211 | test_loss: 7.7415409088 | \n",
      "Epoch: 146 | train_loss: 146.2272338867 | test_loss: 7.7264018059 | \n",
      "Epoch: 147 | train_loss: 146.0729522705 | test_loss: 7.7112102509 | \n",
      "Epoch: 148 | train_loss: 145.9184875488 | test_loss: 7.6959648132 | \n",
      "Epoch: 149 | train_loss: 145.7638092041 | test_loss: 7.6806745529 | \n",
      "Epoch: 150 | train_loss: 145.6089324951 | test_loss: 7.6653337479 | \n",
      "Epoch: 151 | train_loss: 145.4538726807 | test_loss: 7.6499457359 | \n",
      "Epoch: 152 | train_loss: 145.2985687256 | test_loss: 7.6345157623 | \n",
      "Epoch: 153 | train_loss: 145.1431121826 | test_loss: 7.6190433502 | \n",
      "Epoch: 154 | train_loss: 144.9874572754 | test_loss: 7.6035313606 | \n",
      "Epoch: 155 | train_loss: 144.8316650391 | test_loss: 7.5879817009 | \n",
      "Epoch: 156 | train_loss: 144.6757354736 | test_loss: 7.5723972321 | \n",
      "Epoch: 157 | train_loss: 144.5196533203 | test_loss: 7.5567841530 | \n",
      "Epoch: 158 | train_loss: 144.3634643555 | test_loss: 7.5411405563 | \n",
      "Epoch: 159 | train_loss: 144.2071685791 | test_loss: 7.5254716873 | \n",
      "Epoch: 160 | train_loss: 144.0507812500 | test_loss: 7.5097846985 | \n",
      "Epoch: 161 | train_loss: 143.8943481445 | test_loss: 7.4940795898 | \n",
      "Epoch: 162 | train_loss: 143.7378845215 | test_loss: 7.4783568382 | \n",
      "Epoch: 163 | train_loss: 143.5813903809 | test_loss: 7.4626274109 | \n",
      "Epoch: 164 | train_loss: 143.4249572754 | test_loss: 7.4468884468 | \n",
      "Epoch: 165 | train_loss: 143.2685394287 | test_loss: 7.4311475754 | \n",
      "Epoch: 166 | train_loss: 143.1122131348 | test_loss: 7.4154052734 | \n",
      "Epoch: 167 | train_loss: 142.9559783936 | test_loss: 7.3996696472 | \n",
      "Epoch: 168 | train_loss: 142.7998657227 | test_loss: 7.3839411736 | \n",
      "Epoch: 169 | train_loss: 142.6439208984 | test_loss: 7.3682255745 | \n",
      "Epoch: 170 | train_loss: 142.4881896973 | test_loss: 7.3525252342 | \n",
      "Epoch: 171 | train_loss: 142.3326721191 | test_loss: 7.3368468285 | \n",
      "Epoch: 172 | train_loss: 142.1773986816 | test_loss: 7.3211913109 | \n",
      "Epoch: 173 | train_loss: 142.0224304199 | test_loss: 7.3055615425 | \n",
      "Epoch: 174 | train_loss: 141.8677673340 | test_loss: 7.2899675369 | \n",
      "Epoch: 175 | train_loss: 141.7134552002 | test_loss: 7.2744078636 | \n",
      "Epoch: 176 | train_loss: 141.5595550537 | test_loss: 7.2588844299 | \n",
      "Epoch: 177 | train_loss: 141.4060211182 | test_loss: 7.2434043884 | \n",
      "Epoch: 178 | train_loss: 141.2529449463 | test_loss: 7.2279729843 | \n",
      "Epoch: 179 | train_loss: 141.1003875732 | test_loss: 7.2125921249 | \n",
      "Epoch: 180 | train_loss: 140.9483032227 | test_loss: 7.1972675323 | \n",
      "Epoch: 181 | train_loss: 140.7967834473 | test_loss: 7.1819953918 | \n",
      "Epoch: 182 | train_loss: 140.6458129883 | test_loss: 7.1667876244 | \n",
      "Epoch: 183 | train_loss: 140.4954528809 | test_loss: 7.1516423225 | \n",
      "Epoch: 184 | train_loss: 140.3457794189 | test_loss: 7.1365690231 | \n",
      "Epoch: 185 | train_loss: 140.1967315674 | test_loss: 7.1215610504 | \n",
      "Epoch: 186 | train_loss: 140.0483551025 | test_loss: 7.1066327095 | \n",
      "Epoch: 187 | train_loss: 139.9007263184 | test_loss: 7.0917816162 | \n",
      "Epoch: 188 | train_loss: 139.7538452148 | test_loss: 7.0770125389 | \n",
      "Epoch: 189 | train_loss: 139.6077270508 | test_loss: 7.0623273849 | \n",
      "Epoch: 190 | train_loss: 139.4623870850 | test_loss: 7.0477290154 | \n",
      "Epoch: 191 | train_loss: 139.3178863525 | test_loss: 7.0332159996 | \n",
      "Epoch: 192 | train_loss: 139.1742095947 | test_loss: 7.0187973976 | \n",
      "Epoch: 193 | train_loss: 139.0313720703 | test_loss: 7.0044732094 | \n",
      "Epoch: 194 | train_loss: 138.8893890381 | test_loss: 6.9902415276 | \n",
      "Epoch: 195 | train_loss: 138.7482910156 | test_loss: 6.9761095047 | \n",
      "Epoch: 196 | train_loss: 138.6080932617 | test_loss: 6.9620771408 | \n",
      "Epoch: 197 | train_loss: 138.4687194824 | test_loss: 6.9481425285 | \n",
      "Epoch: 198 | train_loss: 138.3302612305 | test_loss: 6.9343070984 | \n",
      "Epoch: 199 | train_loss: 138.1926727295 | test_loss: 6.9205765724 | \n",
      "Epoch: 200 | train_loss: 138.0559234619 | test_loss: 6.9069476128 | \n",
      "Epoch: 201 | train_loss: 137.9200439453 | test_loss: 6.8934178352 | \n",
      "Epoch: 202 | train_loss: 137.7850189209 | test_loss: 6.8799953461 | \n",
      "Epoch: 203 | train_loss: 137.6508483887 | test_loss: 6.8666725159 | \n",
      "Epoch: 204 | train_loss: 137.5174255371 | test_loss: 6.8534550667 | \n",
      "Epoch: 205 | train_loss: 137.3849029541 | test_loss: 6.8403334618 | \n",
      "Epoch: 206 | train_loss: 137.2531127930 | test_loss: 6.8273148537 | \n",
      "Epoch: 207 | train_loss: 137.1221313477 | test_loss: 6.8143959045 | \n",
      "Epoch: 208 | train_loss: 136.9918823242 | test_loss: 6.8015761375 | \n",
      "Epoch: 209 | train_loss: 136.8624420166 | test_loss: 6.7888560295 | \n",
      "Epoch: 210 | train_loss: 136.7336883545 | test_loss: 6.7762336731 | \n",
      "Epoch: 211 | train_loss: 136.6057281494 | test_loss: 6.7637104988 | \n",
      "Epoch: 212 | train_loss: 136.4784851074 | test_loss: 6.7512803078 | \n",
      "Epoch: 213 | train_loss: 136.3519592285 | test_loss: 6.7389526367 | \n",
      "Epoch: 214 | train_loss: 136.2261505127 | test_loss: 6.7267217636 | \n",
      "Epoch: 215 | train_loss: 136.1010131836 | test_loss: 6.7145857811 | \n",
      "Epoch: 216 | train_loss: 135.9765930176 | test_loss: 6.7025532722 | \n",
      "Epoch: 217 | train_loss: 135.8528747559 | test_loss: 6.6906213760 | \n",
      "Epoch: 218 | train_loss: 135.7298431396 | test_loss: 6.6787900925 | \n",
      "Epoch: 219 | train_loss: 135.6074371338 | test_loss: 6.6670598984 | \n",
      "Epoch: 220 | train_loss: 135.4856872559 | test_loss: 6.6554408073 | \n",
      "Epoch: 221 | train_loss: 135.3645782471 | test_loss: 6.6439247131 | \n",
      "Epoch: 222 | train_loss: 135.2441253662 | test_loss: 6.6325240135 | \n",
      "Epoch: 223 | train_loss: 135.1242828369 | test_loss: 6.6212353706 | \n",
      "Epoch: 224 | train_loss: 135.0050354004 | test_loss: 6.6100616455 | \n",
      "Epoch: 225 | train_loss: 134.8863677979 | test_loss: 6.5990071297 | \n",
      "Epoch: 226 | train_loss: 134.7682495117 | test_loss: 6.5880751610 | \n",
      "Epoch: 227 | train_loss: 134.6507873535 | test_loss: 6.5772681236 | \n",
      "Epoch: 228 | train_loss: 134.5338592529 | test_loss: 6.5665831566 | \n",
      "Epoch: 229 | train_loss: 134.4174346924 | test_loss: 6.5560297966 | \n",
      "Epoch: 230 | train_loss: 134.3016357422 | test_loss: 6.5456089973 | \n",
      "Epoch: 231 | train_loss: 134.1863555908 | test_loss: 6.5353150368 | \n",
      "Epoch: 232 | train_loss: 134.0715942383 | test_loss: 6.5251636505 | \n",
      "Epoch: 233 | train_loss: 133.9574127197 | test_loss: 6.5151429176 | \n",
      "Epoch: 234 | train_loss: 133.8437500000 | test_loss: 6.5052614212 | \n",
      "Epoch: 235 | train_loss: 133.7306213379 | test_loss: 6.4955167770 | \n",
      "Epoch: 236 | train_loss: 133.6180877686 | test_loss: 6.4859147072 | \n",
      "Epoch: 237 | train_loss: 133.5061187744 | test_loss: 6.4764475822 | \n",
      "Epoch: 238 | train_loss: 133.3946838379 | test_loss: 6.4671254158 | \n",
      "Epoch: 239 | train_loss: 133.2838134766 | test_loss: 6.4627971649 | \n",
      "Epoch: 240 | train_loss: 133.1735229492 | test_loss: 6.4630613327 | \n",
      "Epoch: 241 | train_loss: 133.0638732910 | test_loss: 6.4633221626 | \n",
      "Epoch: 242 | train_loss: 132.9547729492 | test_loss: 6.4635787010 | \n",
      "Epoch: 243 | train_loss: 132.8463134766 | test_loss: 6.4638257027 | \n",
      "Epoch: 244 | train_loss: 132.7384490967 | test_loss: 6.4640750885 | \n",
      "Epoch: 245 | train_loss: 132.6312255859 | test_loss: 6.4643211365 | \n",
      "Epoch: 246 | train_loss: 132.5246429443 | test_loss: 6.4645667076 | \n",
      "Epoch: 247 | train_loss: 132.4187011719 | test_loss: 6.4648118019 | \n",
      "Epoch: 248 | train_loss: 132.3134307861 | test_loss: 6.4650554657 | \n",
      "Epoch: 249 | train_loss: 132.2088012695 | test_loss: 6.4652976990 | \n",
      "Epoch: 250 | train_loss: 132.1048736572 | test_loss: 6.4655451775 | \n",
      "Epoch: 251 | train_loss: 132.0016479492 | test_loss: 6.4657912254 | \n",
      "Epoch: 252 | train_loss: 131.8991241455 | test_loss: 6.4660377502 | \n",
      "Epoch: 253 | train_loss: 131.7972717285 | test_loss: 6.4662866592 | \n",
      "Epoch: 254 | train_loss: 131.6961517334 | test_loss: 6.4665331841 | \n",
      "Epoch: 255 | train_loss: 131.5957336426 | test_loss: 6.4667792320 | \n",
      "Epoch: 256 | train_loss: 131.4960174561 | test_loss: 6.4670271873 | \n",
      "Epoch: 257 | train_loss: 131.3970642090 | test_loss: 6.4672760963 | \n",
      "Epoch: 258 | train_loss: 131.2988128662 | test_loss: 6.4675264359 | \n",
      "Epoch: 259 | train_loss: 131.2012939453 | test_loss: 6.4677748680 | \n",
      "Epoch: 260 | train_loss: 131.1045532227 | test_loss: 6.4680223465 | \n",
      "Epoch: 261 | train_loss: 131.0084686279 | test_loss: 6.4682683945 | \n",
      "Epoch: 262 | train_loss: 130.9132080078 | test_loss: 6.4685144424 | \n",
      "Epoch: 263 | train_loss: 130.8186035156 | test_loss: 6.4687595367 | \n",
      "Epoch: 264 | train_loss: 130.7247772217 | test_loss: 6.4690008163 | \n",
      "Epoch: 265 | train_loss: 130.6316833496 | test_loss: 6.4692420959 | \n",
      "Epoch: 266 | train_loss: 130.5393218994 | test_loss: 6.4694805145 | \n",
      "Epoch: 267 | train_loss: 130.4476928711 | test_loss: 6.4697155952 | \n",
      "Epoch: 268 | train_loss: 130.3568115234 | test_loss: 6.4699487686 | \n",
      "Epoch: 269 | train_loss: 130.2666778564 | test_loss: 6.4701776505 | \n",
      "Epoch: 270 | train_loss: 130.1772308350 | test_loss: 6.4704051018 | \n",
      "Epoch: 271 | train_loss: 130.0885620117 | test_loss: 6.4706268311 | \n",
      "Epoch: 272 | train_loss: 130.0005493164 | test_loss: 6.4708471298 | \n",
      "Epoch: 273 | train_loss: 129.9133148193 | test_loss: 6.4710597992 | \n",
      "Epoch: 274 | train_loss: 129.8267822266 | test_loss: 6.4712700844 | \n",
      "Epoch: 275 | train_loss: 129.7409515381 | test_loss: 6.4714746475 | \n",
      "Epoch: 276 | train_loss: 129.6558532715 | test_loss: 6.4716773033 | \n",
      "Epoch: 277 | train_loss: 129.5714721680 | test_loss: 6.4718699455 | \n",
      "Epoch: 278 | train_loss: 129.4877929688 | test_loss: 6.4720597267 | \n",
      "Epoch: 279 | train_loss: 129.4048004150 | test_loss: 6.4722442627 | \n",
      "Epoch: 280 | train_loss: 129.3225402832 | test_loss: 6.4724273682 | \n",
      "Epoch: 281 | train_loss: 129.2409057617 | test_loss: 6.4725995064 | \n",
      "Epoch: 282 | train_loss: 129.1600952148 | test_loss: 6.4727692604 | \n",
      "Epoch: 283 | train_loss: 129.0798492432 | test_loss: 6.4729294777 | \n",
      "Epoch: 284 | train_loss: 129.0003204346 | test_loss: 6.4730863571 | \n",
      "Epoch: 285 | train_loss: 128.9214782715 | test_loss: 6.4732336998 | \n",
      "Epoch: 286 | train_loss: 128.8432464600 | test_loss: 6.4733791351 | \n",
      "Epoch: 287 | train_loss: 128.7657775879 | test_loss: 6.4735131264 | \n",
      "Epoch: 288 | train_loss: 128.6889495850 | test_loss: 6.4736442566 | \n",
      "Epoch: 289 | train_loss: 128.6127319336 | test_loss: 6.4737696648 | \n",
      "Epoch: 290 | train_loss: 128.5372467041 | test_loss: 6.4738898277 | \n",
      "Epoch: 291 | train_loss: 128.4623413086 | test_loss: 6.4740018845 | \n",
      "Epoch: 292 | train_loss: 128.3881225586 | test_loss: 6.4740996361 | \n",
      "Epoch: 293 | train_loss: 128.3145294189 | test_loss: 6.4742054939 | \n",
      "Epoch: 294 | train_loss: 128.2415313721 | test_loss: 6.4742918015 | \n",
      "Epoch: 295 | train_loss: 128.1691894531 | test_loss: 6.4743800163 | \n",
      "Epoch: 296 | train_loss: 128.0975036621 | test_loss: 6.4744553566 | \n",
      "Epoch: 297 | train_loss: 128.0263977051 | test_loss: 6.4745240211 | \n",
      "Epoch: 298 | train_loss: 127.9559173584 | test_loss: 6.4745850563 | \n",
      "Epoch: 299 | train_loss: 127.8860321045 | test_loss: 6.4746446609 | \n",
      "Epoch: 300 | train_loss: 127.8167495728 | test_loss: 6.4746937752 | \n",
      "Epoch: 301 | train_loss: 127.7481231689 | test_loss: 6.4747352600 | \n",
      "Epoch: 302 | train_loss: 127.6800231934 | test_loss: 6.4747691154 | \n",
      "Epoch: 303 | train_loss: 127.6125106812 | test_loss: 6.4747924805 | \n",
      "Epoch: 304 | train_loss: 127.5456161499 | test_loss: 6.4748134613 | \n",
      "Epoch: 305 | train_loss: 127.4792556763 | test_loss: 6.4748244286 | \n",
      "Epoch: 306 | train_loss: 127.4134826660 | test_loss: 6.4748296738 | \n",
      "Epoch: 307 | train_loss: 127.3483123779 | test_loss: 6.4748253822 | \n",
      "Epoch: 308 | train_loss: 127.2836837769 | test_loss: 6.4748134613 | \n",
      "Epoch: 309 | train_loss: 127.2195892334 | test_loss: 6.4747982025 | \n",
      "Epoch: 310 | train_loss: 127.1560745239 | test_loss: 6.4747714996 | \n",
      "Epoch: 311 | train_loss: 127.0930786133 | test_loss: 6.4747371674 | \n",
      "Epoch: 312 | train_loss: 127.0306243896 | test_loss: 6.4746985435 | \n",
      "Epoch: 313 | train_loss: 126.9687118530 | test_loss: 6.4746479988 | \n",
      "Epoch: 314 | train_loss: 126.9073333740 | test_loss: 6.4745879173 | \n",
      "Epoch: 315 | train_loss: 126.8465042114 | test_loss: 6.4745244980 | \n",
      "Epoch: 316 | train_loss: 126.7861785889 | test_loss: 6.4744548798 | \n",
      "Epoch: 317 | train_loss: 126.7263793945 | test_loss: 6.4743738174 | \n",
      "Epoch: 318 | train_loss: 126.6670684814 | test_loss: 6.4742803574 | \n",
      "Epoch: 319 | train_loss: 126.6082839966 | test_loss: 6.4741864204 | \n",
      "Epoch: 320 | train_loss: 126.5499725342 | test_loss: 6.4740800858 | \n",
      "Epoch: 321 | train_loss: 126.4921798706 | test_loss: 6.4739656448 | \n",
      "Epoch: 322 | train_loss: 126.4348602295 | test_loss: 6.4738440514 | \n",
      "Epoch: 323 | train_loss: 126.3780593872 | test_loss: 6.4737138748 | \n",
      "Epoch: 324 | train_loss: 126.3217391968 | test_loss: 6.4735784531 | \n",
      "Epoch: 325 | train_loss: 126.2659149170 | test_loss: 6.4734330177 | \n",
      "Epoch: 326 | train_loss: 126.2105178833 | test_loss: 6.4732775688 | \n",
      "Epoch: 327 | train_loss: 126.1555938721 | test_loss: 6.4731206894 | \n",
      "Epoch: 328 | train_loss: 126.1011505127 | test_loss: 6.4729466438 | \n",
      "Epoch: 329 | train_loss: 126.0472259521 | test_loss: 6.4727697372 | \n",
      "Epoch: 330 | train_loss: 125.9936904907 | test_loss: 6.4725856781 | \n",
      "Epoch: 331 | train_loss: 125.9406051636 | test_loss: 6.4723901749 | \n",
      "Epoch: 332 | train_loss: 125.8880310059 | test_loss: 6.4721903801 | \n",
      "Epoch: 333 | train_loss: 125.8358383179 | test_loss: 6.4719810486 | \n",
      "Epoch: 334 | train_loss: 125.7841033936 | test_loss: 6.4717640877 | \n",
      "Epoch: 335 | train_loss: 125.7328033447 | test_loss: 6.4715380669 | \n",
      "Epoch: 336 | train_loss: 125.6819839478 | test_loss: 6.4713039398 | \n",
      "Epoch: 337 | train_loss: 125.6315460205 | test_loss: 6.4710655212 | \n",
      "Epoch: 338 | train_loss: 125.5815124512 | test_loss: 6.4708137512 | \n",
      "Epoch: 339 | train_loss: 125.5319366455 | test_loss: 6.4705591202 | \n",
      "Epoch: 340 | train_loss: 125.4827804565 | test_loss: 6.4702925682 | \n",
      "Epoch: 341 | train_loss: 125.4340209961 | test_loss: 6.4700188637 | \n",
      "Epoch: 342 | train_loss: 125.3856735229 | test_loss: 6.4697380066 | \n",
      "Epoch: 343 | train_loss: 125.3377380371 | test_loss: 6.4694490433 | \n",
      "Epoch: 344 | train_loss: 125.2902069092 | test_loss: 6.4691591263 | \n",
      "Epoch: 345 | train_loss: 125.2430572510 | test_loss: 6.4688544273 | \n",
      "Epoch: 346 | train_loss: 125.1962814331 | test_loss: 6.4685406685 | \n",
      "Epoch: 347 | train_loss: 125.1499404907 | test_loss: 6.4682245255 | \n",
      "Epoch: 348 | train_loss: 125.1040039062 | test_loss: 6.4678974152 | \n",
      "Epoch: 349 | train_loss: 125.0583953857 | test_loss: 6.4675626755 | \n",
      "Epoch: 350 | train_loss: 125.0131912231 | test_loss: 6.4672203064 | \n",
      "Epoch: 351 | train_loss: 124.9683685303 | test_loss: 6.4668745995 | \n",
      "Epoch: 352 | train_loss: 124.9239044189 | test_loss: 6.4665174484 | \n",
      "Epoch: 353 | train_loss: 124.8798217773 | test_loss: 6.4661536217 | \n",
      "Epoch: 354 | train_loss: 124.8360824585 | test_loss: 6.4657835960 | \n",
      "Epoch: 355 | train_loss: 124.7927169800 | test_loss: 6.4654097557 | \n",
      "Epoch: 356 | train_loss: 124.7497329712 | test_loss: 6.4650225639 | \n",
      "Epoch: 357 | train_loss: 124.7070541382 | test_loss: 6.4646315575 | \n",
      "Epoch: 358 | train_loss: 124.6648178101 | test_loss: 6.4642353058 | \n",
      "Epoch: 359 | train_loss: 124.6228637695 | test_loss: 6.4638252258 | \n",
      "Epoch: 360 | train_loss: 124.5812301636 | test_loss: 6.4634137154 | \n",
      "Epoch: 361 | train_loss: 124.5399780273 | test_loss: 6.4629950523 | \n",
      "Epoch: 362 | train_loss: 124.4990692139 | test_loss: 6.4625697136 | \n",
      "Epoch: 363 | train_loss: 124.4584884644 | test_loss: 6.4621334076 | \n",
      "Epoch: 364 | train_loss: 124.4182052612 | test_loss: 6.4616961479 | \n",
      "Epoch: 365 | train_loss: 124.3783264160 | test_loss: 6.4612483978 | \n",
      "Epoch: 366 | train_loss: 124.3387222290 | test_loss: 6.4607973099 | \n",
      "Epoch: 367 | train_loss: 124.2994537354 | test_loss: 6.4603352547 | \n",
      "Epoch: 368 | train_loss: 124.2604980469 | test_loss: 6.4598689079 | \n",
      "Epoch: 369 | train_loss: 124.2218933105 | test_loss: 6.4594001770 | \n",
      "Epoch: 370 | train_loss: 124.1835479736 | test_loss: 6.4589185715 | \n",
      "Epoch: 371 | train_loss: 124.1455535889 | test_loss: 6.4584345818 | \n",
      "Epoch: 372 | train_loss: 124.1078643799 | test_loss: 6.4579401016 | \n",
      "Epoch: 373 | train_loss: 124.0704421997 | test_loss: 6.4574418068 | \n",
      "Epoch: 374 | train_loss: 124.0333786011 | test_loss: 6.4569396973 | \n",
      "Epoch: 375 | train_loss: 123.9965591431 | test_loss: 6.4564275742 | \n",
      "Epoch: 376 | train_loss: 123.9600982666 | test_loss: 6.4559159279 | \n",
      "Epoch: 377 | train_loss: 123.9238739014 | test_loss: 6.4553880692 | \n",
      "Epoch: 378 | train_loss: 123.8879699707 | test_loss: 6.4548621178 | \n",
      "Epoch: 379 | train_loss: 123.8523406982 | test_loss: 6.4543275833 | \n",
      "Epoch: 380 | train_loss: 123.8170013428 | test_loss: 6.4537901878 | \n",
      "Epoch: 381 | train_loss: 123.7819747925 | test_loss: 6.4532423019 | \n",
      "Epoch: 382 | train_loss: 123.7472229004 | test_loss: 6.4526886940 | \n",
      "Epoch: 383 | train_loss: 123.7127227783 | test_loss: 6.4521327019 | \n",
      "Epoch: 384 | train_loss: 123.6785278320 | test_loss: 6.4515686035 | \n",
      "Epoch: 385 | train_loss: 123.6445693970 | test_loss: 6.4509997368 | \n",
      "Epoch: 386 | train_loss: 123.6109161377 | test_loss: 6.4504265785 | \n",
      "Epoch: 387 | train_loss: 123.5775299072 | test_loss: 6.4498462677 | \n",
      "Epoch: 388 | train_loss: 123.5443954468 | test_loss: 6.4492626190 | \n",
      "Epoch: 389 | train_loss: 123.5115203857 | test_loss: 6.4486694336 | \n",
      "Epoch: 390 | train_loss: 123.4788970947 | test_loss: 6.4480757713 | \n",
      "Epoch: 391 | train_loss: 123.4465942383 | test_loss: 6.4474730492 | \n",
      "Epoch: 392 | train_loss: 123.4144439697 | test_loss: 6.4468650818 | \n",
      "Epoch: 393 | train_loss: 123.3826370239 | test_loss: 6.4462547302 | \n",
      "Epoch: 394 | train_loss: 123.3510437012 | test_loss: 6.4456372261 | \n",
      "Epoch: 395 | train_loss: 123.3197174072 | test_loss: 6.4450135231 | \n",
      "Epoch: 396 | train_loss: 123.2886657715 | test_loss: 6.4443888664 | \n",
      "Epoch: 397 | train_loss: 123.2577896118 | test_loss: 6.4437561035 | \n",
      "Epoch: 398 | train_loss: 123.2271881104 | test_loss: 6.4431176186 | \n",
      "Epoch: 399 | train_loss: 123.1968154907 | test_loss: 6.4424767494 | \n",
      "Epoch: 400 | train_loss: 123.1667022705 | test_loss: 6.4418272972 | \n",
      "Epoch: 401 | train_loss: 123.1368255615 | test_loss: 6.4411745071 | \n",
      "Epoch: 402 | train_loss: 123.1071701050 | test_loss: 6.4405193329 | \n",
      "Epoch: 403 | train_loss: 123.0777282715 | test_loss: 6.4398603439 | \n",
      "Epoch: 404 | train_loss: 123.0485458374 | test_loss: 6.4391899109 | \n",
      "Epoch: 405 | train_loss: 123.0195617676 | test_loss: 6.4385175705 | \n",
      "Epoch: 406 | train_loss: 122.9908142090 | test_loss: 6.4378428459 | \n",
      "Epoch: 407 | train_loss: 122.9623031616 | test_loss: 6.4371643066 | \n",
      "Epoch: 408 | train_loss: 122.9340362549 | test_loss: 6.4364800453 | \n",
      "Epoch: 409 | train_loss: 122.9059371948 | test_loss: 6.4357872009 | \n",
      "Epoch: 410 | train_loss: 122.8780441284 | test_loss: 6.4350957870 | \n",
      "Epoch: 411 | train_loss: 122.8504028320 | test_loss: 6.4343938828 | \n",
      "Epoch: 412 | train_loss: 122.8229980469 | test_loss: 6.4336900711 | \n",
      "Epoch: 413 | train_loss: 122.7957534790 | test_loss: 6.4329862595 | \n",
      "Epoch: 414 | train_loss: 122.7687301636 | test_loss: 6.4322757721 | \n",
      "Epoch: 415 | train_loss: 122.7419433594 | test_loss: 6.4315586090 | \n",
      "Epoch: 416 | train_loss: 122.7153396606 | test_loss: 6.4308385849 | \n",
      "Epoch: 417 | train_loss: 122.6889343262 | test_loss: 6.4301128387 | \n",
      "Epoch: 418 | train_loss: 122.6627655029 | test_loss: 6.4293799400 | \n",
      "Epoch: 419 | train_loss: 122.6367263794 | test_loss: 6.4286475182 | \n",
      "Epoch: 420 | train_loss: 122.6109542847 | test_loss: 6.4279103279 | \n",
      "Epoch: 421 | train_loss: 122.5853576660 | test_loss: 6.4271678925 | \n",
      "Epoch: 422 | train_loss: 122.5599594116 | test_loss: 6.4264197350 | \n",
      "Epoch: 423 | train_loss: 122.5347595215 | test_loss: 6.4256715775 | \n",
      "Epoch: 424 | train_loss: 122.5096969604 | test_loss: 6.4249134064 | \n",
      "Epoch: 425 | train_loss: 122.4848632812 | test_loss: 6.4241580963 | \n",
      "Epoch: 426 | train_loss: 122.4602050781 | test_loss: 6.4233946800 | \n",
      "Epoch: 427 | train_loss: 122.4357681274 | test_loss: 6.4226269722 | \n",
      "Epoch: 428 | train_loss: 122.4114837646 | test_loss: 6.4218611717 | \n",
      "Epoch: 429 | train_loss: 122.3874053955 | test_loss: 6.4210839272 | \n",
      "Epoch: 430 | train_loss: 122.3635025024 | test_loss: 6.4203057289 | \n",
      "Epoch: 431 | train_loss: 122.3397750854 | test_loss: 6.4195227623 | \n",
      "Epoch: 432 | train_loss: 122.3162002563 | test_loss: 6.4187374115 | \n",
      "Epoch: 433 | train_loss: 122.2928314209 | test_loss: 6.4179472923 | \n",
      "Epoch: 434 | train_loss: 122.2696228027 | test_loss: 6.4171538353 | \n",
      "Epoch: 435 | train_loss: 122.2465896606 | test_loss: 6.4163575172 | \n",
      "Epoch: 436 | train_loss: 122.2237243652 | test_loss: 6.4155550003 | \n",
      "Epoch: 437 | train_loss: 122.2009963989 | test_loss: 6.4147529602 | \n",
      "Epoch: 438 | train_loss: 122.1785354614 | test_loss: 6.4139471054 | \n",
      "Epoch: 439 | train_loss: 122.1561508179 | test_loss: 6.4131317139 | \n",
      "Epoch: 440 | train_loss: 122.1339569092 | test_loss: 6.4123191833 | \n",
      "Epoch: 441 | train_loss: 122.1119232178 | test_loss: 6.4114952087 | \n",
      "Epoch: 442 | train_loss: 122.0900573730 | test_loss: 6.4106774330 | \n",
      "Epoch: 443 | train_loss: 122.0683517456 | test_loss: 6.4098548889 | \n",
      "Epoch: 444 | train_loss: 122.0468139648 | test_loss: 6.4090209007 | \n",
      "Epoch: 445 | train_loss: 122.0254287720 | test_loss: 6.4081878662 | \n",
      "Epoch: 446 | train_loss: 122.0041961670 | test_loss: 6.4073529243 | \n",
      "Epoch: 447 | train_loss: 121.9831237793 | test_loss: 6.4065122604 | \n",
      "Epoch: 448 | train_loss: 121.9621810913 | test_loss: 6.4056677818 | \n",
      "Epoch: 449 | train_loss: 121.9414443970 | test_loss: 6.4048223495 | \n",
      "Epoch: 450 | train_loss: 121.9207839966 | test_loss: 6.4039750099 | \n",
      "Epoch: 451 | train_loss: 121.9003143311 | test_loss: 6.4031200409 | \n",
      "Epoch: 452 | train_loss: 121.8800125122 | test_loss: 6.4022607803 | \n",
      "Epoch: 453 | train_loss: 121.8598556519 | test_loss: 6.4014034271 | \n",
      "Epoch: 454 | train_loss: 121.8398132324 | test_loss: 6.4005389214 | \n",
      "Epoch: 455 | train_loss: 121.8199386597 | test_loss: 6.3996758461 | \n",
      "Epoch: 456 | train_loss: 121.8001937866 | test_loss: 6.3988099098 | \n",
      "Epoch: 457 | train_loss: 121.7806167603 | test_loss: 6.3979320526 | \n",
      "Epoch: 458 | train_loss: 121.7611389160 | test_loss: 6.3970575333 | \n",
      "Epoch: 459 | train_loss: 121.7417984009 | test_loss: 6.3961772919 | \n",
      "Epoch: 460 | train_loss: 121.7226257324 | test_loss: 6.3952946663 | \n",
      "Epoch: 461 | train_loss: 121.7036209106 | test_loss: 6.3944096565 | \n",
      "Epoch: 462 | train_loss: 121.6846694946 | test_loss: 6.3935227394 | \n",
      "Epoch: 463 | train_loss: 121.6658782959 | test_loss: 6.3926329613 | \n",
      "Epoch: 464 | train_loss: 121.6472396851 | test_loss: 6.3917360306 | \n",
      "Epoch: 465 | train_loss: 121.6287536621 | test_loss: 6.3908414841 | \n",
      "Epoch: 466 | train_loss: 121.6103744507 | test_loss: 6.3899421692 | \n",
      "Epoch: 467 | train_loss: 121.5921325684 | test_loss: 6.3890361786 | \n",
      "Epoch: 468 | train_loss: 121.5740051270 | test_loss: 6.3881263733 | \n",
      "Epoch: 469 | train_loss: 121.5560379028 | test_loss: 6.3872218132 | \n",
      "Epoch: 470 | train_loss: 121.5381317139 | test_loss: 6.3863081932 | \n",
      "Epoch: 471 | train_loss: 121.5204391479 | test_loss: 6.3853926659 | \n",
      "Epoch: 472 | train_loss: 121.5027999878 | test_loss: 6.3844780922 | \n",
      "Epoch: 473 | train_loss: 121.4853057861 | test_loss: 6.3835563660 | \n",
      "Epoch: 474 | train_loss: 121.4679260254 | test_loss: 6.3826313019 | \n",
      "Epoch: 475 | train_loss: 121.4506835938 | test_loss: 6.3817057610 | \n",
      "Epoch: 476 | train_loss: 121.4335479736 | test_loss: 6.3807802200 | \n",
      "Epoch: 477 | train_loss: 121.4165267944 | test_loss: 6.3798465729 | \n",
      "Epoch: 478 | train_loss: 121.3996353149 | test_loss: 6.3789153099 | \n",
      "Epoch: 479 | train_loss: 121.3828735352 | test_loss: 6.3779697418 | \n",
      "Epoch: 480 | train_loss: 121.3662109375 | test_loss: 6.3770375252 | \n",
      "Epoch: 481 | train_loss: 121.3496704102 | test_loss: 6.3760991096 | \n",
      "Epoch: 482 | train_loss: 121.3332519531 | test_loss: 6.3751525879 | \n",
      "Epoch: 483 | train_loss: 121.3169174194 | test_loss: 6.3741984367 | \n",
      "Epoch: 484 | train_loss: 121.3006744385 | test_loss: 6.3732519150 | \n",
      "Epoch: 485 | train_loss: 121.2845993042 | test_loss: 6.3722934723 | \n",
      "Epoch: 486 | train_loss: 121.2686233521 | test_loss: 6.3713407516 | \n",
      "Epoch: 487 | train_loss: 121.2527236938 | test_loss: 6.3703846931 | \n",
      "Epoch: 488 | train_loss: 121.2369308472 | test_loss: 6.3694195747 | \n",
      "Epoch: 489 | train_loss: 121.2212829590 | test_loss: 6.3684582710 | \n",
      "Epoch: 490 | train_loss: 121.2057189941 | test_loss: 6.3674860001 | \n",
      "Epoch: 491 | train_loss: 121.1902542114 | test_loss: 6.3665232658 | \n",
      "Epoch: 492 | train_loss: 121.1749114990 | test_loss: 6.3655562401 | \n",
      "Epoch: 493 | train_loss: 121.1596908569 | test_loss: 6.3645806313 | \n",
      "Epoch: 494 | train_loss: 121.1445617676 | test_loss: 6.3636045456 | \n",
      "Epoch: 495 | train_loss: 121.1294860840 | test_loss: 6.3626289368 | \n",
      "Epoch: 496 | train_loss: 121.1145477295 | test_loss: 6.3616447449 | \n",
      "Epoch: 497 | train_loss: 121.0997161865 | test_loss: 6.3606648445 | \n",
      "Epoch: 498 | train_loss: 121.0849533081 | test_loss: 6.3596806526 | \n",
      "Epoch: 499 | train_loss: 121.0702896118 | test_loss: 6.3586883545 | \n",
      "Epoch: 500 | train_loss: 121.0557937622 | test_loss: 6.3577008247 | \n",
      "Epoch: 501 | train_loss: 121.0412979126 | test_loss: 6.3567066193 | \n",
      "Epoch: 502 | train_loss: 121.0269622803 | test_loss: 6.3557133675 | \n",
      "Epoch: 503 | train_loss: 121.0126647949 | test_loss: 6.3547172546 | \n",
      "Epoch: 504 | train_loss: 120.9985580444 | test_loss: 6.3537168503 | \n",
      "Epoch: 505 | train_loss: 120.9844818115 | test_loss: 6.3527193069 | \n",
      "Epoch: 506 | train_loss: 120.9704818726 | test_loss: 6.3517127037 | \n",
      "Epoch: 507 | train_loss: 120.9565734863 | test_loss: 6.3507065773 | \n",
      "Epoch: 508 | train_loss: 120.9427642822 | test_loss: 6.3497004509 | \n",
      "Epoch: 509 | train_loss: 120.9290618896 | test_loss: 6.3486890793 | \n",
      "Epoch: 510 | train_loss: 120.9154586792 | test_loss: 6.3476734161 | \n",
      "Epoch: 511 | train_loss: 120.9019012451 | test_loss: 6.3466625214 | \n",
      "Epoch: 512 | train_loss: 120.8884735107 | test_loss: 6.3456425667 | \n",
      "Epoch: 513 | train_loss: 120.8750686646 | test_loss: 6.3446240425 | \n",
      "Epoch: 514 | train_loss: 120.8618087769 | test_loss: 6.3436017036 | \n",
      "Epoch: 515 | train_loss: 120.8486251831 | test_loss: 6.3425798416 | \n",
      "Epoch: 516 | train_loss: 120.8355255127 | test_loss: 6.3415546417 | \n",
      "Epoch: 517 | train_loss: 120.8224792480 | test_loss: 6.3405256271 | \n",
      "Epoch: 518 | train_loss: 120.8095474243 | test_loss: 6.3394989967 | \n",
      "Epoch: 519 | train_loss: 120.7967224121 | test_loss: 6.3384699821 | \n",
      "Epoch: 520 | train_loss: 120.7839355469 | test_loss: 6.3374304771 | \n",
      "Epoch: 521 | train_loss: 120.7712402344 | test_loss: 6.3363952637 | \n",
      "Epoch: 522 | train_loss: 120.7586135864 | test_loss: 6.3353590965 | \n",
      "Epoch: 523 | train_loss: 120.7460632324 | test_loss: 6.3343162537 | \n",
      "Epoch: 524 | train_loss: 120.7336044312 | test_loss: 6.3332743645 | \n",
      "Epoch: 525 | train_loss: 120.7212295532 | test_loss: 6.3322296143 | \n",
      "Epoch: 526 | train_loss: 120.7089614868 | test_loss: 6.3311853409 | \n",
      "Epoch: 527 | train_loss: 120.6966857910 | test_loss: 6.3301377296 | \n",
      "Epoch: 528 | train_loss: 120.6845703125 | test_loss: 6.3290877342 | \n",
      "Epoch: 529 | train_loss: 120.6724700928 | test_loss: 6.3280382156 | \n",
      "Epoch: 530 | train_loss: 120.6604995728 | test_loss: 6.3269877434 | \n",
      "Epoch: 531 | train_loss: 120.6485443115 | test_loss: 6.3259334564 | \n",
      "Epoch: 532 | train_loss: 120.6367187500 | test_loss: 6.3248720169 | \n",
      "Epoch: 533 | train_loss: 120.6249389648 | test_loss: 6.3238139153 | \n",
      "Epoch: 534 | train_loss: 120.6132202148 | test_loss: 6.3227500916 | \n",
      "Epoch: 535 | train_loss: 120.6015853882 | test_loss: 6.3216896057 | \n",
      "Epoch: 536 | train_loss: 120.5900115967 | test_loss: 6.3206214905 | \n",
      "Epoch: 537 | train_loss: 120.5785369873 | test_loss: 6.3195581436 | \n",
      "Epoch: 538 | train_loss: 120.5670776367 | test_loss: 6.3184909821 | \n",
      "Epoch: 539 | train_loss: 120.5557403564 | test_loss: 6.3174190521 | \n",
      "Epoch: 540 | train_loss: 120.5444259644 | test_loss: 6.3163475990 | \n",
      "Epoch: 541 | train_loss: 120.5332031250 | test_loss: 6.3152770996 | \n",
      "Epoch: 542 | train_loss: 120.5220718384 | test_loss: 6.3141980171 | \n",
      "Epoch: 543 | train_loss: 120.5109481812 | test_loss: 6.3131198883 | \n",
      "Epoch: 544 | train_loss: 120.4999389648 | test_loss: 6.3120384216 | \n",
      "Epoch: 545 | train_loss: 120.4889831543 | test_loss: 6.3109602928 | \n",
      "Epoch: 546 | train_loss: 120.4780731201 | test_loss: 6.3098797798 | \n",
      "Epoch: 547 | train_loss: 120.4672775269 | test_loss: 6.3087992668 | \n",
      "Epoch: 548 | train_loss: 120.4565200806 | test_loss: 6.3077087402 | \n",
      "Epoch: 549 | train_loss: 120.4458007812 | test_loss: 6.3066191673 | \n",
      "Epoch: 550 | train_loss: 120.4351577759 | test_loss: 6.3055348396 | \n",
      "Epoch: 551 | train_loss: 120.4245986938 | test_loss: 6.3044395447 | \n",
      "Epoch: 552 | train_loss: 120.4140930176 | test_loss: 6.3033518791 | \n",
      "Epoch: 553 | train_loss: 120.4036331177 | test_loss: 6.3022542000 | \n",
      "Epoch: 554 | train_loss: 120.3932113647 | test_loss: 6.3011612892 | \n",
      "Epoch: 555 | train_loss: 120.3829116821 | test_loss: 6.3000659943 | \n",
      "Epoch: 556 | train_loss: 120.3726501465 | test_loss: 6.2989606857 | \n",
      "Epoch: 557 | train_loss: 120.3624343872 | test_loss: 6.2978610992 | \n",
      "Epoch: 558 | train_loss: 120.3522567749 | test_loss: 6.2967548370 | \n",
      "Epoch: 559 | train_loss: 120.3421783447 | test_loss: 6.2956500053 | \n",
      "Epoch: 560 | train_loss: 120.3321456909 | test_loss: 6.2945508957 | \n",
      "Epoch: 561 | train_loss: 120.3221740723 | test_loss: 6.2934370041 | \n",
      "Epoch: 562 | train_loss: 120.3122253418 | test_loss: 6.2923274040 | \n",
      "Epoch: 563 | train_loss: 120.3023529053 | test_loss: 6.2912197113 | \n",
      "Epoch: 564 | train_loss: 120.2925567627 | test_loss: 6.2901015282 | \n",
      "Epoch: 565 | train_loss: 120.2827835083 | test_loss: 6.2889866829 | \n",
      "Epoch: 566 | train_loss: 120.2730636597 | test_loss: 6.2878704071 | \n",
      "Epoch: 567 | train_loss: 120.2634506226 | test_loss: 6.2867569923 | \n",
      "Epoch: 568 | train_loss: 120.2538146973 | test_loss: 6.2856321335 | \n",
      "Epoch: 569 | train_loss: 120.2442855835 | test_loss: 6.2845063210 | \n",
      "Epoch: 570 | train_loss: 120.2347946167 | test_loss: 6.2833819389 | \n",
      "Epoch: 571 | train_loss: 120.2253723145 | test_loss: 6.2822589874 | \n",
      "Epoch: 572 | train_loss: 120.2159576416 | test_loss: 6.2811307907 | \n",
      "Epoch: 573 | train_loss: 120.2066574097 | test_loss: 6.2800054550 | \n",
      "Epoch: 574 | train_loss: 120.1973419189 | test_loss: 6.2788724899 | \n",
      "Epoch: 575 | train_loss: 120.1881027222 | test_loss: 6.2777385712 | \n",
      "Epoch: 576 | train_loss: 120.1789245605 | test_loss: 6.2766075134 | \n",
      "Epoch: 577 | train_loss: 120.1698303223 | test_loss: 6.2754683495 | \n",
      "Epoch: 578 | train_loss: 120.1607131958 | test_loss: 6.2743296623 | \n",
      "Epoch: 579 | train_loss: 120.1517105103 | test_loss: 6.2731938362 | \n",
      "Epoch: 580 | train_loss: 120.1427078247 | test_loss: 6.2720513344 | \n",
      "Epoch: 581 | train_loss: 120.1337585449 | test_loss: 6.2709083557 | \n",
      "Epoch: 582 | train_loss: 120.1248550415 | test_loss: 6.2697629929 | \n",
      "Epoch: 583 | train_loss: 120.1160125732 | test_loss: 6.2686142921 | \n",
      "Epoch: 584 | train_loss: 120.1072158813 | test_loss: 6.2674655914 | \n",
      "Epoch: 585 | train_loss: 120.0984573364 | test_loss: 6.2663183212 | \n",
      "Epoch: 586 | train_loss: 120.0897293091 | test_loss: 6.2651648521 | \n",
      "Epoch: 587 | train_loss: 120.0810317993 | test_loss: 6.2640123367 | \n",
      "Epoch: 588 | train_loss: 120.0724182129 | test_loss: 6.2628583908 | \n",
      "Epoch: 589 | train_loss: 120.0638275146 | test_loss: 6.2617015839 | \n",
      "Epoch: 590 | train_loss: 120.0553054810 | test_loss: 6.2605438232 | \n",
      "Epoch: 591 | train_loss: 120.0467681885 | test_loss: 6.2593841553 | \n",
      "Epoch: 592 | train_loss: 120.0383224487 | test_loss: 6.2582221031 | \n",
      "Epoch: 593 | train_loss: 120.0298919678 | test_loss: 6.2570533752 | \n",
      "Epoch: 594 | train_loss: 120.0215301514 | test_loss: 6.2558875084 | \n",
      "Epoch: 595 | train_loss: 120.0131530762 | test_loss: 6.2547163963 | \n",
      "Epoch: 596 | train_loss: 120.0048751831 | test_loss: 6.2535533905 | \n",
      "Epoch: 597 | train_loss: 119.9966201782 | test_loss: 6.2523851395 | \n",
      "Epoch: 598 | train_loss: 119.9883956909 | test_loss: 6.2512116432 | \n",
      "Epoch: 599 | train_loss: 119.9802169800 | test_loss: 6.2500343323 | \n",
      "Epoch: 600 | train_loss: 119.9720535278 | test_loss: 6.2488651276 | \n",
      "Epoch: 601 | train_loss: 119.9639434814 | test_loss: 6.2476868629 | \n",
      "Epoch: 602 | train_loss: 119.9558868408 | test_loss: 6.2465100288 | \n",
      "Epoch: 603 | train_loss: 119.9478454590 | test_loss: 6.2453331947 | \n",
      "Epoch: 604 | train_loss: 119.9398117065 | test_loss: 6.2441511154 | \n",
      "Epoch: 605 | train_loss: 119.9318771362 | test_loss: 6.2429676056 | \n",
      "Epoch: 606 | train_loss: 119.9239273071 | test_loss: 6.2417869568 | \n",
      "Epoch: 607 | train_loss: 119.9160232544 | test_loss: 6.2406029701 | \n",
      "Epoch: 608 | train_loss: 119.9081802368 | test_loss: 6.2394227982 | \n",
      "Epoch: 609 | train_loss: 119.9003601074 | test_loss: 6.2382359505 | \n",
      "Epoch: 610 | train_loss: 119.8925399780 | test_loss: 6.2370491028 | \n",
      "Epoch: 611 | train_loss: 119.8848037720 | test_loss: 6.2358646393 | \n",
      "Epoch: 612 | train_loss: 119.8770904541 | test_loss: 6.2346816063 | \n",
      "Epoch: 613 | train_loss: 119.8694152832 | test_loss: 6.2334938049 | \n",
      "Epoch: 614 | train_loss: 119.8617248535 | test_loss: 6.2323055267 | \n",
      "Epoch: 615 | train_loss: 119.8541030884 | test_loss: 6.2311162949 | \n",
      "Epoch: 616 | train_loss: 119.8465270996 | test_loss: 6.2299304008 | \n",
      "Epoch: 617 | train_loss: 119.8389358521 | test_loss: 6.2287468910 | \n",
      "Epoch: 618 | train_loss: 119.8314132690 | test_loss: 6.2275581360 | \n",
      "Epoch: 619 | train_loss: 119.8239059448 | test_loss: 6.2263698578 | \n",
      "Epoch: 620 | train_loss: 119.8164291382 | test_loss: 6.2251863480 | \n",
      "Epoch: 621 | train_loss: 119.8090133667 | test_loss: 6.2239985466 | \n",
      "Epoch: 622 | train_loss: 119.8016204834 | test_loss: 6.2228116989 | \n",
      "Epoch: 623 | train_loss: 119.7942276001 | test_loss: 6.2216281891 | \n",
      "Epoch: 624 | train_loss: 119.7868881226 | test_loss: 6.2204422951 | \n",
      "Epoch: 625 | train_loss: 119.7795639038 | test_loss: 6.2192630768 | \n",
      "Epoch: 626 | train_loss: 119.7723159790 | test_loss: 6.2180814743 | \n",
      "Epoch: 627 | train_loss: 119.7650375366 | test_loss: 6.2168989182 | \n",
      "Epoch: 628 | train_loss: 119.7578506470 | test_loss: 6.2157201767 | \n",
      "Epoch: 629 | train_loss: 119.7507019043 | test_loss: 6.2145433426 | \n",
      "Epoch: 630 | train_loss: 119.7435455322 | test_loss: 6.2133631706 | \n",
      "Epoch: 631 | train_loss: 119.7364349365 | test_loss: 6.2121820450 | \n",
      "Epoch: 632 | train_loss: 119.7293548584 | test_loss: 6.2110095024 | \n",
      "Epoch: 633 | train_loss: 119.7223205566 | test_loss: 6.2098317146 | \n",
      "Epoch: 634 | train_loss: 119.7153015137 | test_loss: 6.2086567879 | \n",
      "Epoch: 635 | train_loss: 119.7083282471 | test_loss: 6.2074866295 | \n",
      "Epoch: 636 | train_loss: 119.7013778687 | test_loss: 6.2063140869 | \n",
      "Epoch: 637 | train_loss: 119.6944503784 | test_loss: 6.2051458359 | \n",
      "Epoch: 638 | train_loss: 119.6875610352 | test_loss: 6.2039737701 | \n",
      "Epoch: 639 | train_loss: 119.6807327271 | test_loss: 6.2028088570 | \n",
      "Epoch: 640 | train_loss: 119.6739196777 | test_loss: 6.2016406059 | \n",
      "Epoch: 641 | train_loss: 119.6670837402 | test_loss: 6.2004780769 | \n",
      "Epoch: 642 | train_loss: 119.6603088379 | test_loss: 6.1993107796 | \n",
      "Epoch: 643 | train_loss: 119.6535568237 | test_loss: 6.1981468201 | \n",
      "Epoch: 644 | train_loss: 119.6468582153 | test_loss: 6.1969890594 | \n",
      "Epoch: 645 | train_loss: 119.6402282715 | test_loss: 6.1958284378 | \n",
      "Epoch: 646 | train_loss: 119.6335906982 | test_loss: 6.1946721077 | \n",
      "Epoch: 647 | train_loss: 119.6269607544 | test_loss: 6.1935105324 | \n",
      "Epoch: 648 | train_loss: 119.6204071045 | test_loss: 6.1923561096 | \n",
      "Epoch: 649 | train_loss: 119.6138381958 | test_loss: 6.1912021637 | \n",
      "Epoch: 650 | train_loss: 119.6073760986 | test_loss: 6.1900434494 | \n",
      "Epoch: 651 | train_loss: 119.6008605957 | test_loss: 6.1888942719 | \n",
      "Epoch: 652 | train_loss: 119.5944213867 | test_loss: 6.1877422333 | \n",
      "Epoch: 653 | train_loss: 119.5879592896 | test_loss: 6.1865930557 | \n",
      "Epoch: 654 | train_loss: 119.5815811157 | test_loss: 6.1854448318 | \n",
      "Epoch: 655 | train_loss: 119.5752334595 | test_loss: 6.1842994690 | \n",
      "Epoch: 656 | train_loss: 119.5689086914 | test_loss: 6.1831531525 | \n",
      "Epoch: 657 | train_loss: 119.5625762939 | test_loss: 6.1820082664 | \n",
      "Epoch: 658 | train_loss: 119.5562973022 | test_loss: 6.1809706688 | \n",
      "Epoch: 659 | train_loss: 119.5500946045 | test_loss: 6.1813659668 | \n",
      "Epoch: 660 | train_loss: 119.5438308716 | test_loss: 6.1817646027 | \n",
      "Epoch: 661 | train_loss: 119.5376968384 | test_loss: 6.1821489334 | \n",
      "Epoch: 662 | train_loss: 119.5315017700 | test_loss: 6.1825404167 | \n",
      "Epoch: 663 | train_loss: 119.5253601074 | test_loss: 6.1829242706 | \n",
      "Epoch: 664 | train_loss: 119.5192489624 | test_loss: 6.1833043098 | \n",
      "Epoch: 665 | train_loss: 119.5131912231 | test_loss: 6.1836833954 | \n",
      "Epoch: 666 | train_loss: 119.5071716309 | test_loss: 6.1840672493 | \n",
      "Epoch: 667 | train_loss: 119.5011138916 | test_loss: 6.1844444275 | \n",
      "Epoch: 668 | train_loss: 119.4951553345 | test_loss: 6.1848187447 | \n",
      "Epoch: 669 | train_loss: 119.4891433716 | test_loss: 6.1851911545 | \n",
      "Epoch: 670 | train_loss: 119.4832305908 | test_loss: 6.1855669022 | \n",
      "Epoch: 671 | train_loss: 119.4773025513 | test_loss: 6.1859340668 | \n",
      "Epoch: 672 | train_loss: 119.4714431763 | test_loss: 6.1863026619 | \n",
      "Epoch: 673 | train_loss: 119.4655456543 | test_loss: 6.1866621971 | \n",
      "Epoch: 674 | train_loss: 119.4597244263 | test_loss: 6.1870293617 | \n",
      "Epoch: 675 | train_loss: 119.4539337158 | test_loss: 6.1873884201 | \n",
      "Epoch: 676 | train_loss: 119.4481124878 | test_loss: 6.1877436638 | \n",
      "Epoch: 677 | train_loss: 119.4423751831 | test_loss: 6.1880998611 | \n",
      "Epoch: 678 | train_loss: 119.4366226196 | test_loss: 6.1884570122 | \n",
      "Epoch: 679 | train_loss: 119.4309310913 | test_loss: 6.1888041496 | \n",
      "Epoch: 680 | train_loss: 119.4252548218 | test_loss: 6.1891560555 | \n",
      "Epoch: 681 | train_loss: 119.4195709229 | test_loss: 6.1895017624 | \n",
      "Epoch: 682 | train_loss: 119.4139556885 | test_loss: 6.1898460388 | \n",
      "Epoch: 683 | train_loss: 119.4083709717 | test_loss: 6.1901874542 | \n",
      "Epoch: 684 | train_loss: 119.4027862549 | test_loss: 6.1905336380 | \n",
      "Epoch: 685 | train_loss: 119.3971862793 | test_loss: 6.1908750534 | \n",
      "Epoch: 686 | train_loss: 119.3916625977 | test_loss: 6.1912083626 | \n",
      "Epoch: 687 | train_loss: 119.3861389160 | test_loss: 6.1915459633 | \n",
      "Epoch: 688 | train_loss: 119.3806457520 | test_loss: 6.1918749809 | \n",
      "Epoch: 689 | train_loss: 119.3751449585 | test_loss: 6.1922054291 | \n",
      "Epoch: 690 | train_loss: 119.3697204590 | test_loss: 6.1925339699 | \n",
      "Epoch: 691 | train_loss: 119.3643188477 | test_loss: 6.1928601265 | \n",
      "Epoch: 692 | train_loss: 119.3588867188 | test_loss: 6.1931891441 | \n",
      "Epoch: 693 | train_loss: 119.3535385132 | test_loss: 6.1935124397 | \n",
      "Epoch: 694 | train_loss: 119.3481445312 | test_loss: 6.1938323975 | \n",
      "Epoch: 695 | train_loss: 119.3428192139 | test_loss: 6.1941547394 | \n",
      "Epoch: 696 | train_loss: 119.3374938965 | test_loss: 6.1944737434 | \n",
      "Epoch: 697 | train_loss: 119.3322372437 | test_loss: 6.1947855949 | \n",
      "Epoch: 698 | train_loss: 119.3269653320 | test_loss: 6.1951003075 | \n",
      "Epoch: 699 | train_loss: 119.3217086792 | test_loss: 6.1954097748 | \n",
      "Epoch: 700 | train_loss: 119.3164672852 | test_loss: 6.1957173347 | \n",
      "Epoch: 701 | train_loss: 119.3112487793 | test_loss: 6.1960310936 | \n",
      "Epoch: 702 | train_loss: 119.3060607910 | test_loss: 6.1963286400 | \n",
      "Epoch: 703 | train_loss: 119.3008956909 | test_loss: 6.1966304779 | \n",
      "Epoch: 704 | train_loss: 119.2957458496 | test_loss: 6.1969337463 | \n",
      "Epoch: 705 | train_loss: 119.2906188965 | test_loss: 6.1972346306 | \n",
      "Epoch: 706 | train_loss: 119.2854766846 | test_loss: 6.1975312233 | \n",
      "Epoch: 707 | train_loss: 119.2804260254 | test_loss: 6.1978220940 | \n",
      "Epoch: 708 | train_loss: 119.2753448486 | test_loss: 6.1981172562 | \n",
      "Epoch: 709 | train_loss: 119.2703018188 | test_loss: 6.1984052658 | \n",
      "Epoch: 710 | train_loss: 119.2652664185 | test_loss: 6.1986951828 | \n",
      "Epoch: 711 | train_loss: 119.2602615356 | test_loss: 6.1989860535 | \n",
      "Epoch: 712 | train_loss: 119.2552719116 | test_loss: 6.1992712021 | \n",
      "Epoch: 713 | train_loss: 119.2502899170 | test_loss: 6.1995520592 | \n",
      "Epoch: 714 | train_loss: 119.2453613281 | test_loss: 6.1998348236 | \n",
      "Epoch: 715 | train_loss: 119.2404022217 | test_loss: 6.2001166344 | \n",
      "Epoch: 716 | train_loss: 119.2355194092 | test_loss: 6.2003955841 | \n",
      "Epoch: 717 | train_loss: 119.2305984497 | test_loss: 6.2006731033 | \n",
      "Epoch: 718 | train_loss: 119.2257156372 | test_loss: 6.2009458542 | \n",
      "Epoch: 719 | train_loss: 119.2208557129 | test_loss: 6.2012133598 | \n",
      "Epoch: 720 | train_loss: 119.2160186768 | test_loss: 6.2014884949 | \n",
      "Epoch: 721 | train_loss: 119.2111740112 | test_loss: 6.2017531395 | \n",
      "Epoch: 722 | train_loss: 119.2063674927 | test_loss: 6.2020225525 | \n",
      "Epoch: 723 | train_loss: 119.2015609741 | test_loss: 6.2022905350 | \n",
      "Epoch: 724 | train_loss: 119.1968154907 | test_loss: 6.2025508881 | \n",
      "Epoch: 725 | train_loss: 119.1920394897 | test_loss: 6.2028088570 | \n",
      "Epoch: 726 | train_loss: 119.1872482300 | test_loss: 6.2030677795 | \n",
      "Epoch: 727 | train_loss: 119.1825561523 | test_loss: 6.2033271790 | \n",
      "Epoch: 728 | train_loss: 119.1778335571 | test_loss: 6.2035822868 | \n",
      "Epoch: 729 | train_loss: 119.1731338501 | test_loss: 6.2038331032 | \n",
      "Epoch: 730 | train_loss: 119.1684799194 | test_loss: 6.2040834427 | \n",
      "Epoch: 731 | train_loss: 119.1638107300 | test_loss: 6.2043361664 | \n",
      "Epoch: 732 | train_loss: 119.1591491699 | test_loss: 6.2045817375 | \n",
      "Epoch: 733 | train_loss: 119.1545639038 | test_loss: 6.2048268318 | \n",
      "Epoch: 734 | train_loss: 119.1499404907 | test_loss: 6.2050709724 | \n",
      "Epoch: 735 | train_loss: 119.1453094482 | test_loss: 6.2053098679 | \n",
      "Epoch: 736 | train_loss: 119.1407241821 | test_loss: 6.2055482864 | \n",
      "Epoch: 737 | train_loss: 119.1361618042 | test_loss: 6.2057890892 | \n",
      "Epoch: 738 | train_loss: 119.1316299438 | test_loss: 6.2060241699 | \n",
      "Epoch: 739 | train_loss: 119.1270828247 | test_loss: 6.2062621117 | \n",
      "Epoch: 740 | train_loss: 119.1225967407 | test_loss: 6.2064924240 | \n",
      "Epoch: 741 | train_loss: 119.1180496216 | test_loss: 6.2067203522 | \n",
      "Epoch: 742 | train_loss: 119.1136016846 | test_loss: 6.2069478035 | \n",
      "Epoch: 743 | train_loss: 119.1091384888 | test_loss: 6.2071781158 | \n",
      "Epoch: 744 | train_loss: 119.1046752930 | test_loss: 6.2073998451 | \n",
      "Epoch: 745 | train_loss: 119.1002655029 | test_loss: 6.2076234818 | \n",
      "Epoch: 746 | train_loss: 119.0957946777 | test_loss: 6.2078461647 | \n",
      "Epoch: 747 | train_loss: 119.0914382935 | test_loss: 6.2080640793 | \n",
      "Epoch: 748 | train_loss: 119.0870437622 | test_loss: 6.2082796097 | \n",
      "Epoch: 749 | train_loss: 119.0826644897 | test_loss: 6.2084960938 | \n",
      "Epoch: 750 | train_loss: 119.0783004761 | test_loss: 6.2087030411 | \n",
      "Epoch: 751 | train_loss: 119.0739974976 | test_loss: 6.2089138031 | \n",
      "Epoch: 752 | train_loss: 119.0696716309 | test_loss: 6.2091221809 | \n",
      "Epoch: 753 | train_loss: 119.0653533936 | test_loss: 6.2093257904 | \n",
      "Epoch: 754 | train_loss: 119.0610733032 | test_loss: 6.2095279694 | \n",
      "Epoch: 755 | train_loss: 119.0567855835 | test_loss: 6.2097296715 | \n",
      "Epoch: 756 | train_loss: 119.0526046753 | test_loss: 6.2099285126 | \n",
      "Epoch: 757 | train_loss: 119.0483474731 | test_loss: 6.2101240158 | \n",
      "Epoch: 758 | train_loss: 119.0441131592 | test_loss: 6.2103190422 | \n",
      "Epoch: 759 | train_loss: 119.0399093628 | test_loss: 6.2105083466 | \n",
      "Epoch: 760 | train_loss: 119.0357589722 | test_loss: 6.2107000351 | \n",
      "Epoch: 761 | train_loss: 119.0316314697 | test_loss: 6.2108869553 | \n",
      "Epoch: 762 | train_loss: 119.0274963379 | test_loss: 6.2110700607 | \n",
      "Epoch: 763 | train_loss: 119.0233917236 | test_loss: 6.2112522125 | \n",
      "Epoch: 764 | train_loss: 119.0192947388 | test_loss: 6.2114329338 | \n",
      "Epoch: 765 | train_loss: 119.0152130127 | test_loss: 6.2116065025 | \n",
      "Epoch: 766 | train_loss: 119.0111618042 | test_loss: 6.2117834091 | \n",
      "Epoch: 767 | train_loss: 119.0071182251 | test_loss: 6.2119488716 | \n",
      "Epoch: 768 | train_loss: 119.0030822754 | test_loss: 6.2121171951 | \n",
      "Epoch: 769 | train_loss: 118.9991149902 | test_loss: 6.2122845650 | \n",
      "Epoch: 770 | train_loss: 118.9951400757 | test_loss: 6.2124409676 | \n",
      "Epoch: 771 | train_loss: 118.9912109375 | test_loss: 6.2125978470 | \n",
      "Epoch: 772 | train_loss: 118.9872589111 | test_loss: 6.2127518654 | \n",
      "Epoch: 773 | train_loss: 118.9833755493 | test_loss: 6.2129063606 | \n",
      "Epoch: 774 | train_loss: 118.9794921875 | test_loss: 6.2130579948 | \n",
      "Epoch: 775 | train_loss: 118.9756317139 | test_loss: 6.2132043839 | \n",
      "Epoch: 776 | train_loss: 118.9717559814 | test_loss: 6.2133460045 | \n",
      "Epoch: 777 | train_loss: 118.9679412842 | test_loss: 6.2134857178 | \n",
      "Epoch: 778 | train_loss: 118.9641189575 | test_loss: 6.2136216164 | \n",
      "Epoch: 779 | train_loss: 118.9603500366 | test_loss: 6.2137575150 | \n",
      "Epoch: 780 | train_loss: 118.9565658569 | test_loss: 6.2138915062 | \n",
      "Epoch: 781 | train_loss: 118.9528198242 | test_loss: 6.2140202522 | \n",
      "Epoch: 782 | train_loss: 118.9490890503 | test_loss: 6.2141499519 | \n",
      "Epoch: 783 | train_loss: 118.9453582764 | test_loss: 6.2142763138 | \n",
      "Epoch: 784 | train_loss: 118.9416961670 | test_loss: 6.2144012451 | \n",
      "Epoch: 785 | train_loss: 118.9379806519 | test_loss: 6.2145252228 | \n",
      "Epoch: 786 | train_loss: 118.9343261719 | test_loss: 6.2146458626 | \n",
      "Epoch: 787 | train_loss: 118.9306564331 | test_loss: 6.2147650719 | \n",
      "Epoch: 788 | train_loss: 118.9270248413 | test_loss: 6.2148823738 | \n",
      "Epoch: 789 | train_loss: 118.9233627319 | test_loss: 6.2149977684 | \n",
      "Epoch: 790 | train_loss: 118.9197845459 | test_loss: 6.2151093483 | \n",
      "Epoch: 791 | train_loss: 118.9161682129 | test_loss: 6.2152256966 | \n",
      "Epoch: 792 | train_loss: 118.9125976562 | test_loss: 6.2153348923 | \n",
      "Epoch: 793 | train_loss: 118.9090423584 | test_loss: 6.2154431343 | \n",
      "Epoch: 794 | train_loss: 118.9054946899 | test_loss: 6.2155528069 | \n",
      "Epoch: 795 | train_loss: 118.9019699097 | test_loss: 6.2156634331 | \n",
      "Epoch: 796 | train_loss: 118.8984756470 | test_loss: 6.2157683372 | \n",
      "Epoch: 797 | train_loss: 118.8949584961 | test_loss: 6.2158756256 | \n",
      "Epoch: 798 | train_loss: 118.8914489746 | test_loss: 6.2159852982 | \n",
      "Epoch: 799 | train_loss: 118.8879776001 | test_loss: 6.2160906792 | \n",
      "Epoch: 800 | train_loss: 118.8845291138 | test_loss: 6.2162017822 | \n",
      "Epoch: 801 | train_loss: 118.8811264038 | test_loss: 6.2163071632 | \n",
      "Epoch: 802 | train_loss: 118.8776779175 | test_loss: 6.2164111137 | \n",
      "Epoch: 803 | train_loss: 118.8742370605 | test_loss: 6.2165155411 | \n",
      "Epoch: 804 | train_loss: 118.8708496094 | test_loss: 6.2166166306 | \n",
      "Epoch: 805 | train_loss: 118.8674697876 | test_loss: 6.2167243958 | \n",
      "Epoch: 806 | train_loss: 118.8641204834 | test_loss: 6.2168259621 | \n",
      "Epoch: 807 | train_loss: 118.8607482910 | test_loss: 6.2169303894 | \n",
      "Epoch: 808 | train_loss: 118.8574447632 | test_loss: 6.2170305252 | \n",
      "Epoch: 809 | train_loss: 118.8541183472 | test_loss: 6.2171325684 | \n",
      "Epoch: 810 | train_loss: 118.8508224487 | test_loss: 6.2172284126 | \n",
      "Epoch: 811 | train_loss: 118.8475189209 | test_loss: 6.2173337936 | \n",
      "Epoch: 812 | train_loss: 118.8442611694 | test_loss: 6.2174334526 | \n",
      "Epoch: 813 | train_loss: 118.8409729004 | test_loss: 6.2175335884 | \n",
      "Epoch: 814 | train_loss: 118.8377380371 | test_loss: 6.2176322937 | \n",
      "Epoch: 815 | train_loss: 118.8344955444 | test_loss: 6.2177319527 | \n",
      "Epoch: 816 | train_loss: 118.8313446045 | test_loss: 6.2178287506 | \n",
      "Epoch: 817 | train_loss: 118.8281250000 | test_loss: 6.2179255486 | \n",
      "Epoch: 818 | train_loss: 118.8249816895 | test_loss: 6.2180266380 | \n",
      "Epoch: 819 | train_loss: 118.8218002319 | test_loss: 6.2181215286 | \n",
      "Epoch: 820 | train_loss: 118.8186721802 | test_loss: 6.2182164192 | \n",
      "Epoch: 821 | train_loss: 118.8155593872 | test_loss: 6.2183146477 | \n",
      "Epoch: 822 | train_loss: 118.8124389648 | test_loss: 6.2184076309 | \n",
      "Epoch: 823 | train_loss: 118.8093490601 | test_loss: 6.2185029984 | \n",
      "Epoch: 824 | train_loss: 118.8062820435 | test_loss: 6.2185969353 | \n",
      "Epoch: 825 | train_loss: 118.8031997681 | test_loss: 6.2186975479 | \n",
      "Epoch: 826 | train_loss: 118.8001403809 | test_loss: 6.2187852859 | \n",
      "Epoch: 827 | train_loss: 118.7971191406 | test_loss: 6.2188816071 | \n",
      "Epoch: 828 | train_loss: 118.7941207886 | test_loss: 6.2189731598 | \n",
      "Epoch: 829 | train_loss: 118.7911224365 | test_loss: 6.2190656662 | \n",
      "Epoch: 830 | train_loss: 118.7881546021 | test_loss: 6.2191576958 | \n",
      "Epoch: 831 | train_loss: 118.7852172852 | test_loss: 6.2192511559 | \n",
      "Epoch: 832 | train_loss: 118.7822647095 | test_loss: 6.2193408012 | \n",
      "Epoch: 833 | train_loss: 118.7793197632 | test_loss: 6.2194342613 | \n",
      "Epoch: 834 | train_loss: 118.7763977051 | test_loss: 6.2195196152 | \n",
      "Epoch: 835 | train_loss: 118.7735061646 | test_loss: 6.2196073532 | \n",
      "Epoch: 836 | train_loss: 118.7706069946 | test_loss: 6.2196984291 | \n",
      "Epoch: 837 | train_loss: 118.7677459717 | test_loss: 6.2197875977 | \n",
      "Epoch: 838 | train_loss: 118.7648925781 | test_loss: 6.2198686600 | \n",
      "Epoch: 839 | train_loss: 118.7620544434 | test_loss: 6.2199554443 | \n",
      "Epoch: 840 | train_loss: 118.7592391968 | test_loss: 6.2200412750 | \n",
      "Epoch: 841 | train_loss: 118.7564544678 | test_loss: 6.2201261520 | \n",
      "Epoch: 842 | train_loss: 118.7536697388 | test_loss: 6.2202134132 | \n",
      "Epoch: 843 | train_loss: 118.7509002686 | test_loss: 6.2202982903 | \n",
      "Epoch: 844 | train_loss: 118.7481384277 | test_loss: 6.2203850746 | \n",
      "Epoch: 845 | train_loss: 118.7453613281 | test_loss: 6.2204632759 | \n",
      "Epoch: 846 | train_loss: 118.7426452637 | test_loss: 6.2205467224 | \n",
      "Epoch: 847 | train_loss: 118.7399597168 | test_loss: 6.2206287384 | \n",
      "Epoch: 848 | train_loss: 118.7372817993 | test_loss: 6.2207078934 | \n",
      "Epoch: 849 | train_loss: 118.7345581055 | test_loss: 6.2207875252 | \n",
      "Epoch: 850 | train_loss: 118.7318954468 | test_loss: 6.2208652496 | \n",
      "Epoch: 851 | train_loss: 118.7292480469 | test_loss: 6.2209448814 | \n",
      "Epoch: 852 | train_loss: 118.7265930176 | test_loss: 6.2210240364 | \n",
      "Epoch: 853 | train_loss: 118.7239685059 | test_loss: 6.2211017609 | \n",
      "Epoch: 854 | train_loss: 118.7213897705 | test_loss: 6.2211718559 | \n",
      "Epoch: 855 | train_loss: 118.7187805176 | test_loss: 6.2212529182 | \n",
      "Epoch: 856 | train_loss: 118.7161941528 | test_loss: 6.2213292122 | \n",
      "Epoch: 857 | train_loss: 118.7136154175 | test_loss: 6.2214007378 | \n",
      "Epoch: 858 | train_loss: 118.7110671997 | test_loss: 6.2214727402 | \n",
      "Epoch: 859 | train_loss: 118.7084884644 | test_loss: 6.2215447426 | \n",
      "Epoch: 860 | train_loss: 118.7059783936 | test_loss: 6.2216172218 | \n",
      "Epoch: 861 | train_loss: 118.7034835815 | test_loss: 6.2216877937 | \n",
      "Epoch: 862 | train_loss: 118.7009735107 | test_loss: 6.2217574120 | \n",
      "Epoch: 863 | train_loss: 118.6984939575 | test_loss: 6.2218322754 | \n",
      "Epoch: 864 | train_loss: 118.6959991455 | test_loss: 6.2219023705 | \n",
      "Epoch: 865 | train_loss: 118.6935348511 | test_loss: 6.2219676971 | \n",
      "Epoch: 866 | train_loss: 118.6910705566 | test_loss: 6.2220368385 | \n",
      "Epoch: 867 | train_loss: 118.6886520386 | test_loss: 6.2221021652 | \n",
      "Epoch: 868 | train_loss: 118.6862030029 | test_loss: 6.2221703529 | \n",
      "Epoch: 869 | train_loss: 118.6838073730 | test_loss: 6.2222385406 | \n",
      "Epoch: 870 | train_loss: 118.6813888550 | test_loss: 6.2222962379 | \n",
      "Epoch: 871 | train_loss: 118.6790237427 | test_loss: 6.2223620415 | \n",
      "Epoch: 872 | train_loss: 118.6766433716 | test_loss: 6.2224245071 | \n",
      "Epoch: 873 | train_loss: 118.6742553711 | test_loss: 6.2224860191 | \n",
      "Epoch: 874 | train_loss: 118.6719360352 | test_loss: 6.2225494385 | \n",
      "Epoch: 875 | train_loss: 118.6696090698 | test_loss: 6.2226099968 | \n",
      "Epoch: 876 | train_loss: 118.6672668457 | test_loss: 6.2226719856 | \n",
      "Epoch: 877 | train_loss: 118.6649322510 | test_loss: 6.2227253914 | \n",
      "Epoch: 878 | train_loss: 118.6626358032 | test_loss: 6.2227864265 | \n",
      "Epoch: 879 | train_loss: 118.6603546143 | test_loss: 6.2228407860 | \n",
      "Epoch: 880 | train_loss: 118.6580657959 | test_loss: 6.2229022980 | \n",
      "Epoch: 881 | train_loss: 118.6557998657 | test_loss: 6.2229499817 | \n",
      "Epoch: 882 | train_loss: 118.6535339355 | test_loss: 6.2230076790 | \n",
      "Epoch: 883 | train_loss: 118.6512756348 | test_loss: 6.2230548859 | \n",
      "Epoch: 884 | train_loss: 118.6490631104 | test_loss: 6.2231111526 | \n",
      "Epoch: 885 | train_loss: 118.6468353271 | test_loss: 6.2231621742 | \n",
      "Epoch: 886 | train_loss: 118.6445999146 | test_loss: 6.2232127190 | \n",
      "Epoch: 887 | train_loss: 118.6424255371 | test_loss: 6.2232618332 | \n",
      "Epoch: 888 | train_loss: 118.6402130127 | test_loss: 6.2233085632 | \n",
      "Epoch: 889 | train_loss: 118.6380386353 | test_loss: 6.2233567238 | \n",
      "Epoch: 890 | train_loss: 118.6358489990 | test_loss: 6.2233996391 | \n",
      "Epoch: 891 | train_loss: 118.6336975098 | test_loss: 6.2234520912 | \n",
      "Epoch: 892 | train_loss: 118.6315460205 | test_loss: 6.2234940529 | \n",
      "Epoch: 893 | train_loss: 118.6294021606 | test_loss: 6.2235436440 | \n",
      "Epoch: 894 | train_loss: 118.6272888184 | test_loss: 6.2235927582 | \n",
      "Epoch: 895 | train_loss: 118.6252059937 | test_loss: 6.2236304283 | \n",
      "Epoch: 896 | train_loss: 118.6230773926 | test_loss: 6.2236723900 | \n",
      "Epoch: 897 | train_loss: 118.6209793091 | test_loss: 6.2237110138 | \n",
      "Epoch: 898 | train_loss: 118.6188964844 | test_loss: 6.2237467766 | \n",
      "Epoch: 899 | train_loss: 118.6168441772 | test_loss: 6.2237901688 | \n",
      "Epoch: 900 | train_loss: 118.6147766113 | test_loss: 6.2238311768 | \n",
      "Epoch: 901 | train_loss: 118.6126785278 | test_loss: 6.2238659859 | \n",
      "Epoch: 902 | train_loss: 118.6106567383 | test_loss: 6.2239050865 | \n",
      "Epoch: 903 | train_loss: 118.6086349487 | test_loss: 6.2239384651 | \n",
      "Epoch: 904 | train_loss: 118.6066131592 | test_loss: 6.2239770889 | \n",
      "Epoch: 905 | train_loss: 118.6045913696 | test_loss: 6.2240123749 | \n",
      "Epoch: 906 | train_loss: 118.6026000977 | test_loss: 6.2240409851 | \n",
      "Epoch: 907 | train_loss: 118.6005859375 | test_loss: 6.2240748405 | \n",
      "Epoch: 908 | train_loss: 118.5985870361 | test_loss: 6.2241082191 | \n",
      "Epoch: 909 | train_loss: 118.5965881348 | test_loss: 6.2241363525 | \n",
      "Epoch: 910 | train_loss: 118.5946807861 | test_loss: 6.2241687775 | \n",
      "Epoch: 911 | train_loss: 118.5927124023 | test_loss: 6.2241959572 | \n",
      "Epoch: 912 | train_loss: 118.5907745361 | test_loss: 6.2242236137 | \n",
      "Epoch: 913 | train_loss: 118.5888671875 | test_loss: 6.2242488861 | \n",
      "Epoch: 914 | train_loss: 118.5868911743 | test_loss: 6.2242808342 | \n",
      "Epoch: 915 | train_loss: 118.5849838257 | test_loss: 6.2243099213 | \n",
      "Epoch: 916 | train_loss: 118.5830764771 | test_loss: 6.2243328094 | \n",
      "Epoch: 917 | train_loss: 118.5811996460 | test_loss: 6.2243523598 | \n",
      "Epoch: 918 | train_loss: 118.5793151855 | test_loss: 6.2243781090 | \n",
      "Epoch: 919 | train_loss: 118.5774154663 | test_loss: 6.2244000435 | \n",
      "Epoch: 920 | train_loss: 118.5755462646 | test_loss: 6.2244262695 | \n",
      "Epoch: 921 | train_loss: 118.5736846924 | test_loss: 6.2244434357 | \n",
      "Epoch: 922 | train_loss: 118.5718765259 | test_loss: 6.2244634628 | \n",
      "Epoch: 923 | train_loss: 118.5700225830 | test_loss: 6.2244825363 | \n",
      "Epoch: 924 | train_loss: 118.5681610107 | test_loss: 6.2244982719 | \n",
      "Epoch: 925 | train_loss: 118.5663299561 | test_loss: 6.2245206833 | \n",
      "Epoch: 926 | train_loss: 118.5645141602 | test_loss: 6.2245368958 | \n",
      "Epoch: 927 | train_loss: 118.5626831055 | test_loss: 6.2245488167 | \n",
      "Epoch: 928 | train_loss: 118.5608901978 | test_loss: 6.2245674133 | \n",
      "Epoch: 929 | train_loss: 118.5590820312 | test_loss: 6.2245831490 | \n",
      "Epoch: 930 | train_loss: 118.5573272705 | test_loss: 6.2245941162 | \n",
      "Epoch: 931 | train_loss: 118.5555496216 | test_loss: 6.2246050835 | \n",
      "Epoch: 932 | train_loss: 118.5537414551 | test_loss: 6.2246165276 | \n",
      "Epoch: 933 | train_loss: 118.5519943237 | test_loss: 6.2246294022 | \n",
      "Epoch: 934 | train_loss: 118.5502548218 | test_loss: 6.2246394157 | \n",
      "Epoch: 935 | train_loss: 118.5485229492 | test_loss: 6.2246489525 | \n",
      "Epoch: 936 | train_loss: 118.5467834473 | test_loss: 6.2246594429 | \n",
      "Epoch: 937 | train_loss: 118.5450515747 | test_loss: 6.2246685028 | \n",
      "Epoch: 938 | train_loss: 118.5433425903 | test_loss: 6.2246732712 | \n",
      "Epoch: 939 | train_loss: 118.5416488647 | test_loss: 6.2246809006 | \n",
      "Epoch: 940 | train_loss: 118.5398941040 | test_loss: 6.2246885300 | \n",
      "Epoch: 941 | train_loss: 118.5382308960 | test_loss: 6.2246952057 | \n",
      "Epoch: 942 | train_loss: 118.5365600586 | test_loss: 6.2246994972 | \n",
      "Epoch: 943 | train_loss: 118.5348892212 | test_loss: 6.2247056961 | \n",
      "Epoch: 944 | train_loss: 118.5332107544 | test_loss: 6.2247114182 | \n",
      "Epoch: 945 | train_loss: 118.5315551758 | test_loss: 6.2247123718 | \n",
      "Epoch: 946 | train_loss: 118.5298919678 | test_loss: 6.2247190475 | \n",
      "Epoch: 947 | train_loss: 118.5282745361 | test_loss: 6.2247190475 | \n",
      "Epoch: 948 | train_loss: 118.5266113281 | test_loss: 6.2247242928 | \n",
      "Epoch: 949 | train_loss: 118.5249786377 | test_loss: 6.2247185707 | \n",
      "Epoch: 950 | train_loss: 118.5233459473 | test_loss: 6.2247204781 | \n",
      "Epoch: 951 | train_loss: 118.5217514038 | test_loss: 6.2247190475 | \n",
      "Epoch: 952 | train_loss: 118.5200958252 | test_loss: 6.2247219086 | \n",
      "Epoch: 953 | train_loss: 118.5185089111 | test_loss: 6.2247161865 | \n",
      "Epoch: 954 | train_loss: 118.5169143677 | test_loss: 6.2247176170 | \n",
      "Epoch: 955 | train_loss: 118.5153427124 | test_loss: 6.2247090340 | \n",
      "Epoch: 956 | train_loss: 118.5137100220 | test_loss: 6.2247104645 | \n",
      "Epoch: 957 | train_loss: 118.5121765137 | test_loss: 6.2247047424 | \n",
      "Epoch: 958 | train_loss: 118.5106124878 | test_loss: 6.2247028351 | \n",
      "Epoch: 959 | train_loss: 118.5090637207 | test_loss: 6.2246966362 | \n",
      "Epoch: 960 | train_loss: 118.5075302124 | test_loss: 6.2246928215 | \n",
      "Epoch: 961 | train_loss: 118.5059661865 | test_loss: 6.2246861458 | \n",
      "Epoch: 962 | train_loss: 118.5043945312 | test_loss: 6.2246794701 | \n",
      "Epoch: 963 | train_loss: 118.5029220581 | test_loss: 6.2246742249 | \n",
      "Epoch: 964 | train_loss: 118.5013885498 | test_loss: 6.2246675491 | \n",
      "Epoch: 965 | train_loss: 118.4998626709 | test_loss: 6.2246594429 | \n",
      "Epoch: 966 | train_loss: 118.4983596802 | test_loss: 6.2246508598 | \n",
      "Epoch: 967 | train_loss: 118.4968566895 | test_loss: 6.2246441841 | \n",
      "Epoch: 968 | train_loss: 118.4953613281 | test_loss: 6.2246294022 | \n",
      "Epoch: 969 | train_loss: 118.4938583374 | test_loss: 6.2246222496 | \n",
      "Epoch: 970 | train_loss: 118.4923858643 | test_loss: 6.2246093750 | \n",
      "Epoch: 971 | train_loss: 118.4909057617 | test_loss: 6.2246012688 | \n",
      "Epoch: 972 | train_loss: 118.4894561768 | test_loss: 6.2245922089 | \n",
      "Epoch: 973 | train_loss: 118.4879989624 | test_loss: 6.2245759964 | \n",
      "Epoch: 974 | train_loss: 118.4865341187 | test_loss: 6.2245659828 | \n",
      "Epoch: 975 | train_loss: 118.4851150513 | test_loss: 6.2245569229 | \n",
      "Epoch: 976 | train_loss: 118.4836730957 | test_loss: 6.2245402336 | \n",
      "Epoch: 977 | train_loss: 118.4821853638 | test_loss: 6.2245316505 | \n",
      "Epoch: 978 | train_loss: 118.4808044434 | test_loss: 6.2245106697 | \n",
      "Epoch: 979 | train_loss: 118.4793853760 | test_loss: 6.2245001793 | \n",
      "Epoch: 980 | train_loss: 118.4779510498 | test_loss: 6.2244868279 | \n",
      "Epoch: 981 | train_loss: 118.4765625000 | test_loss: 6.2244725227 | \n",
      "Epoch: 982 | train_loss: 118.4751663208 | test_loss: 6.2244567871 | \n",
      "Epoch: 983 | train_loss: 118.4737091064 | test_loss: 6.2244420052 | \n",
      "Epoch: 984 | train_loss: 118.4723663330 | test_loss: 6.2244305611 | \n",
      "Epoch: 985 | train_loss: 118.4710083008 | test_loss: 6.2244095802 | \n",
      "Epoch: 986 | train_loss: 118.4696121216 | test_loss: 6.2243938446 | \n",
      "Epoch: 987 | train_loss: 118.4682617188 | test_loss: 6.2243723869 | \n",
      "Epoch: 988 | train_loss: 118.4669036865 | test_loss: 6.2243547440 | \n",
      "Epoch: 989 | train_loss: 118.4655380249 | test_loss: 6.2243351936 | \n",
      "Epoch: 990 | train_loss: 118.4641799927 | test_loss: 6.2243208885 | \n",
      "Epoch: 991 | train_loss: 118.4628295898 | test_loss: 6.2243041992 | \n",
      "Epoch: 992 | train_loss: 118.4615020752 | test_loss: 6.2242846489 | \n",
      "Epoch: 993 | train_loss: 118.4601669312 | test_loss: 6.2242631912 | \n",
      "Epoch: 994 | train_loss: 118.4588165283 | test_loss: 6.2242479324 | \n",
      "Epoch: 995 | train_loss: 118.4575195312 | test_loss: 6.2242274284 | \n",
      "Epoch: 996 | train_loss: 118.4562377930 | test_loss: 6.2242054939 | \n",
      "Epoch: 997 | train_loss: 118.4549026489 | test_loss: 6.2241849899 | \n",
      "Epoch: 998 | train_loss: 118.4536132812 | test_loss: 6.2241673470 | \n",
      "Epoch: 999 | train_loss: 118.4523544312 | test_loss: 6.2241516113 | \n",
      "Epoch: 1000 | train_loss: 118.4510650635 | test_loss: 6.2241239548 | \n",
      "Epoch: 1001 | train_loss: 118.4497299194 | test_loss: 6.2241044044 | \n",
      "Epoch: 1002 | train_loss: 118.4484786987 | test_loss: 6.2240839005 | \n",
      "Epoch: 1003 | train_loss: 118.4472198486 | test_loss: 6.2240600586 | \n",
      "Epoch: 1004 | train_loss: 118.4459533691 | test_loss: 6.2240343094 | \n",
      "Epoch: 1005 | train_loss: 118.4447097778 | test_loss: 6.2240123749 | \n",
      "Epoch: 1006 | train_loss: 118.4434661865 | test_loss: 6.2239904404 | \n",
      "Epoch: 1007 | train_loss: 118.4421844482 | test_loss: 6.2239627838 | \n",
      "Epoch: 1008 | train_loss: 118.4409790039 | test_loss: 6.2239403725 | \n",
      "Epoch: 1009 | train_loss: 118.4397354126 | test_loss: 6.2239203453 | \n",
      "Epoch: 1010 | train_loss: 118.4385223389 | test_loss: 6.2238950729 | \n",
      "Epoch: 1011 | train_loss: 118.4372940063 | test_loss: 6.2238764763 | \n",
      "Epoch: 1012 | train_loss: 118.4360733032 | test_loss: 6.2238483429 | \n",
      "Epoch: 1013 | train_loss: 118.4348602295 | test_loss: 6.2238225937 | \n",
      "Epoch: 1014 | train_loss: 118.4336547852 | test_loss: 6.2237958908 | \n",
      "Epoch: 1015 | train_loss: 118.4324722290 | test_loss: 6.2237706184 | \n",
      "Epoch: 1016 | train_loss: 118.4312667847 | test_loss: 6.2237505913 | \n",
      "Epoch: 1017 | train_loss: 118.4300689697 | test_loss: 6.2237248421 | \n",
      "Epoch: 1018 | train_loss: 118.4289169312 | test_loss: 6.2236976624 | \n",
      "Epoch: 1019 | train_loss: 118.4277114868 | test_loss: 6.2236738205 | \n",
      "Epoch: 1020 | train_loss: 118.4265594482 | test_loss: 6.2236466408 | \n",
      "Epoch: 1021 | train_loss: 118.4253845215 | test_loss: 6.2236175537 | \n",
      "Epoch: 1022 | train_loss: 118.4241790771 | test_loss: 6.2235932350 | \n",
      "Epoch: 1023 | train_loss: 118.4230651855 | test_loss: 6.2235679626 | \n",
      "Epoch: 1024 | train_loss: 118.4219360352 | test_loss: 6.2235374451 | \n",
      "Epoch: 1025 | train_loss: 118.4207611084 | test_loss: 6.2235150337 | \n",
      "Epoch: 1026 | train_loss: 118.4196014404 | test_loss: 6.2234845161 | \n",
      "Epoch: 1027 | train_loss: 118.4184570312 | test_loss: 6.2234606743 | \n",
      "Epoch: 1028 | train_loss: 118.4173736572 | test_loss: 6.2234315872 | \n",
      "Epoch: 1029 | train_loss: 118.4162216187 | test_loss: 6.2234086990 | \n",
      "Epoch: 1030 | train_loss: 118.4151153564 | test_loss: 6.2233796120 | \n",
      "Epoch: 1031 | train_loss: 118.4139938354 | test_loss: 6.2233471870 | \n",
      "Epoch: 1032 | train_loss: 118.4128875732 | test_loss: 6.2233228683 | \n",
      "Epoch: 1033 | train_loss: 118.4117736816 | test_loss: 6.2232928276 | \n",
      "Epoch: 1034 | train_loss: 118.4106826782 | test_loss: 6.2232685089 | \n",
      "Epoch: 1035 | train_loss: 118.4095764160 | test_loss: 6.2232384682 | \n",
      "Epoch: 1036 | train_loss: 118.4084854126 | test_loss: 6.2232122421 | \n",
      "Epoch: 1037 | train_loss: 118.4073944092 | test_loss: 6.2231864929 | \n",
      "Epoch: 1038 | train_loss: 118.4063186646 | test_loss: 6.2231535912 | \n",
      "Epoch: 1039 | train_loss: 118.4052429199 | test_loss: 6.2231245041 | \n",
      "Epoch: 1040 | train_loss: 118.4041595459 | test_loss: 6.2230944633 | \n",
      "Epoch: 1041 | train_loss: 118.4030685425 | test_loss: 6.2230682373 | \n",
      "Epoch: 1042 | train_loss: 118.4019927979 | test_loss: 6.2230362892 | \n",
      "Epoch: 1043 | train_loss: 118.4009704590 | test_loss: 6.2230095863 | \n",
      "Epoch: 1044 | train_loss: 118.3999023438 | test_loss: 6.2229795456 | \n",
      "Epoch: 1045 | train_loss: 118.3988342285 | test_loss: 6.2229475975 | \n",
      "Epoch: 1046 | train_loss: 118.3978042603 | test_loss: 6.2229218483 | \n",
      "Epoch: 1047 | train_loss: 118.3967361450 | test_loss: 6.2228922844 | \n",
      "Epoch: 1048 | train_loss: 118.3957290649 | test_loss: 6.2228631973 | \n",
      "Epoch: 1049 | train_loss: 118.3946838379 | test_loss: 6.2228322029 | \n",
      "Epoch: 1050 | train_loss: 118.3936233521 | test_loss: 6.2228031158 | \n",
      "Epoch: 1051 | train_loss: 118.3926467896 | test_loss: 6.2227768898 | \n",
      "Epoch: 1052 | train_loss: 118.3915710449 | test_loss: 6.2227497101 | \n",
      "Epoch: 1053 | train_loss: 118.3905868530 | test_loss: 6.2227172852 | \n",
      "Epoch: 1054 | train_loss: 118.3895874023 | test_loss: 6.2226881981 | \n",
      "Epoch: 1055 | train_loss: 118.3885650635 | test_loss: 6.2226600647 | \n",
      "Epoch: 1056 | train_loss: 118.3875656128 | test_loss: 6.2226319313 | \n",
      "Epoch: 1057 | train_loss: 118.3865432739 | test_loss: 6.2226023674 | \n",
      "Epoch: 1058 | train_loss: 118.3855361938 | test_loss: 6.2225732803 | \n",
      "Epoch: 1059 | train_loss: 118.3845825195 | test_loss: 6.2225437164 | \n",
      "Epoch: 1060 | train_loss: 118.3836135864 | test_loss: 6.2225146294 | \n",
      "Epoch: 1061 | train_loss: 118.3825988770 | test_loss: 6.2224864960 | \n",
      "Epoch: 1062 | train_loss: 118.3816146851 | test_loss: 6.2224597931 | \n",
      "Epoch: 1063 | train_loss: 118.3806533813 | test_loss: 6.2224302292 | \n",
      "Epoch: 1064 | train_loss: 118.3796844482 | test_loss: 6.2224011421 | \n",
      "Epoch: 1065 | train_loss: 118.3787231445 | test_loss: 6.2223691940 | \n",
      "Epoch: 1066 | train_loss: 118.3777313232 | test_loss: 6.2223453522 | \n",
      "Epoch: 1067 | train_loss: 118.3767852783 | test_loss: 6.2223134041 | \n",
      "Epoch: 1068 | train_loss: 118.3758392334 | test_loss: 6.2222871780 | \n",
      "Epoch: 1069 | train_loss: 118.3748855591 | test_loss: 6.2222590446 | \n",
      "Epoch: 1070 | train_loss: 118.3739624023 | test_loss: 6.2222328186 | \n",
      "Epoch: 1071 | train_loss: 118.3729858398 | test_loss: 6.2222027779 | \n",
      "Epoch: 1072 | train_loss: 118.3720474243 | test_loss: 6.2221708298 | \n",
      "Epoch: 1073 | train_loss: 118.3711166382 | test_loss: 6.2221422195 | \n",
      "Epoch: 1074 | train_loss: 118.3701858521 | test_loss: 6.2221140862 | \n",
      "Epoch: 1075 | train_loss: 118.3692703247 | test_loss: 6.2220873833 | \n",
      "Epoch: 1076 | train_loss: 118.3683166504 | test_loss: 6.2220611572 | \n",
      "Epoch: 1077 | train_loss: 118.3674240112 | test_loss: 6.2220344543 | \n",
      "Epoch: 1078 | train_loss: 118.3665237427 | test_loss: 6.2220082283 | \n",
      "Epoch: 1079 | train_loss: 118.3656158447 | test_loss: 6.2219786644 | \n",
      "Epoch: 1080 | train_loss: 118.3647003174 | test_loss: 6.2219519615 | \n",
      "Epoch: 1081 | train_loss: 118.3637924194 | test_loss: 6.2219195366 | \n",
      "Epoch: 1082 | train_loss: 118.3628692627 | test_loss: 6.2218952179 | \n",
      "Epoch: 1083 | train_loss: 118.3619689941 | test_loss: 6.2218708992 | \n",
      "Epoch: 1084 | train_loss: 118.3610839844 | test_loss: 6.2218456268 | \n",
      "Epoch: 1085 | train_loss: 118.3601989746 | test_loss: 6.2218174934 | \n",
      "Epoch: 1086 | train_loss: 118.3593063354 | test_loss: 6.2217893600 | \n",
      "Epoch: 1087 | train_loss: 118.3584289551 | test_loss: 6.2217655182 | \n",
      "Epoch: 1088 | train_loss: 118.3575363159 | test_loss: 6.2217431068 | \n",
      "Epoch: 1089 | train_loss: 118.3566513062 | test_loss: 6.2217116356 | \n",
      "Epoch: 1090 | train_loss: 118.3557510376 | test_loss: 6.2216906548 | \n",
      "Epoch: 1091 | train_loss: 118.3548431396 | test_loss: 6.2216649055 | \n",
      "Epoch: 1092 | train_loss: 118.3540039062 | test_loss: 6.2216396332 | \n",
      "Epoch: 1093 | train_loss: 118.3531646729 | test_loss: 6.2216143608 | \n",
      "Epoch: 1094 | train_loss: 118.3523025513 | test_loss: 6.2215881348 | \n",
      "Epoch: 1095 | train_loss: 118.3514633179 | test_loss: 6.2215723991 | \n",
      "Epoch: 1096 | train_loss: 118.3505630493 | test_loss: 6.2215442657 | \n",
      "Epoch: 1097 | train_loss: 118.3497238159 | test_loss: 6.2215242386 | \n",
      "Epoch: 1098 | train_loss: 118.3488388062 | test_loss: 6.2215013504 | \n",
      "Epoch: 1099 | train_loss: 118.3479995728 | test_loss: 6.2214789391 | \n",
      "Epoch: 1100 | train_loss: 118.3471984863 | test_loss: 6.2214570045 | \n",
      "Epoch: 1101 | train_loss: 118.3463668823 | test_loss: 6.2214336395 | \n",
      "Epoch: 1102 | train_loss: 118.3454971313 | test_loss: 6.2214121819 | \n",
      "Epoch: 1103 | train_loss: 118.3446502686 | test_loss: 6.2213916779 | \n",
      "Epoch: 1104 | train_loss: 118.3438262939 | test_loss: 6.2213716507 | \n",
      "Epoch: 1105 | train_loss: 118.3430175781 | test_loss: 6.2213521004 | \n",
      "Epoch: 1106 | train_loss: 118.3421783447 | test_loss: 6.2213311195 | \n",
      "Epoch: 1107 | train_loss: 118.3413085938 | test_loss: 6.2213134766 | \n",
      "Epoch: 1108 | train_loss: 118.3405380249 | test_loss: 6.2212915421 | \n",
      "Epoch: 1109 | train_loss: 118.3397293091 | test_loss: 6.2212738991 | \n",
      "Epoch: 1110 | train_loss: 118.3388977051 | test_loss: 6.2212538719 | \n",
      "Epoch: 1111 | train_loss: 118.3381042480 | test_loss: 6.2212371826 | \n",
      "Epoch: 1112 | train_loss: 118.3372650146 | test_loss: 6.2212181091 | \n",
      "Epoch: 1113 | train_loss: 118.3364639282 | test_loss: 6.2212023735 | \n",
      "Epoch: 1114 | train_loss: 118.3356628418 | test_loss: 6.2211852074 | \n",
      "Epoch: 1115 | train_loss: 118.3348693848 | test_loss: 6.2211689949 | \n",
      "Epoch: 1116 | train_loss: 118.3340759277 | test_loss: 6.2211527824 | \n",
      "Epoch: 1117 | train_loss: 118.3332824707 | test_loss: 6.2211384773 | \n",
      "Epoch: 1118 | train_loss: 118.3325119019 | test_loss: 6.2211279869 | \n",
      "Epoch: 1119 | train_loss: 118.3317184448 | test_loss: 6.2211112976 | \n",
      "Epoch: 1120 | train_loss: 118.3309478760 | test_loss: 6.2210979462 | \n",
      "Epoch: 1121 | train_loss: 118.3301391602 | test_loss: 6.2210831642 | \n",
      "Epoch: 1122 | train_loss: 118.3293533325 | test_loss: 6.2210688591 | \n",
      "Epoch: 1123 | train_loss: 118.3285369873 | test_loss: 6.2210564613 | \n",
      "Epoch: 1124 | train_loss: 118.3278045654 | test_loss: 6.2210454941 | \n",
      "Epoch: 1125 | train_loss: 118.3269958496 | test_loss: 6.2210307121 | \n",
      "Epoch: 1126 | train_loss: 118.3262405396 | test_loss: 6.2210197449 | \n",
      "Epoch: 1127 | train_loss: 118.3255081177 | test_loss: 6.2210078239 | \n",
      "Epoch: 1128 | train_loss: 118.3247451782 | test_loss: 6.2209959030 | \n",
      "Epoch: 1129 | train_loss: 118.3239822388 | test_loss: 6.2209835052 | \n",
      "Epoch: 1130 | train_loss: 118.3232040405 | test_loss: 6.2209758759 | \n",
      "Epoch: 1131 | train_loss: 118.3224487305 | test_loss: 6.2209668159 | \n",
      "Epoch: 1132 | train_loss: 118.3216781616 | test_loss: 6.2209553719 | \n",
      "Epoch: 1133 | train_loss: 118.3209075928 | test_loss: 6.2209434509 | \n",
      "Epoch: 1134 | train_loss: 118.3201980591 | test_loss: 6.2209339142 | \n",
      "Epoch: 1135 | train_loss: 118.3194274902 | test_loss: 6.2209253311 | \n",
      "Epoch: 1136 | train_loss: 118.3186798096 | test_loss: 6.2209162712 | \n",
      "Epoch: 1137 | train_loss: 118.3179473877 | test_loss: 6.2209053040 | \n",
      "Epoch: 1138 | train_loss: 118.3172149658 | test_loss: 6.2208991051 | \n",
      "Epoch: 1139 | train_loss: 118.3164825439 | test_loss: 6.2208929062 | \n",
      "Epoch: 1140 | train_loss: 118.3157196045 | test_loss: 6.2208857536 | \n",
      "Epoch: 1141 | train_loss: 118.3150024414 | test_loss: 6.2208752632 | \n",
      "Epoch: 1142 | train_loss: 118.3142776489 | test_loss: 6.2208695412 | \n",
      "Epoch: 1143 | train_loss: 118.3135375977 | test_loss: 6.2208604813 | \n",
      "Epoch: 1144 | train_loss: 118.3127899170 | test_loss: 6.2208495140 | \n",
      "Epoch: 1145 | train_loss: 118.3120880127 | test_loss: 6.2208442688 | \n",
      "Epoch: 1146 | train_loss: 118.3113479614 | test_loss: 6.2208356857 | \n",
      "Epoch: 1147 | train_loss: 118.3106384277 | test_loss: 6.2208271027 | \n",
      "Epoch: 1148 | train_loss: 118.3099212646 | test_loss: 6.2208161354 | \n",
      "Epoch: 1149 | train_loss: 118.3092193604 | test_loss: 6.2208085060 | \n",
      "Epoch: 1150 | train_loss: 118.3085021973 | test_loss: 6.2208018303 | \n",
      "Epoch: 1151 | train_loss: 118.3078231812 | test_loss: 6.2207989693 | \n",
      "Epoch: 1152 | train_loss: 118.3071212769 | test_loss: 6.2207860947 | \n",
      "Epoch: 1153 | train_loss: 118.3063888550 | test_loss: 6.2207818031 | \n",
      "Epoch: 1154 | train_loss: 118.3057022095 | test_loss: 6.2207765579 | \n",
      "Epoch: 1155 | train_loss: 118.3050460815 | test_loss: 6.2207679749 | \n",
      "Epoch: 1156 | train_loss: 118.3043518066 | test_loss: 6.2207627296 | \n",
      "Epoch: 1157 | train_loss: 118.3036346436 | test_loss: 6.2207531929 | \n",
      "Epoch: 1158 | train_loss: 118.3029785156 | test_loss: 6.2207427025 | \n",
      "Epoch: 1159 | train_loss: 118.3022918701 | test_loss: 6.2207322121 | \n",
      "Epoch: 1160 | train_loss: 118.3015899658 | test_loss: 6.2207269669 | \n",
      "Epoch: 1161 | train_loss: 118.3009185791 | test_loss: 6.2207183838 | \n",
      "Epoch: 1162 | train_loss: 118.3002471924 | test_loss: 6.2207059860 | \n",
      "Epoch: 1163 | train_loss: 118.2995605469 | test_loss: 6.2206959724 | \n",
      "Epoch: 1164 | train_loss: 118.2988739014 | test_loss: 6.2206878662 | \n",
      "Epoch: 1165 | train_loss: 118.2982406616 | test_loss: 6.2206754684 | \n",
      "Epoch: 1166 | train_loss: 118.2975692749 | test_loss: 6.2206659317 | \n",
      "Epoch: 1167 | train_loss: 118.2969360352 | test_loss: 6.2206530571 | \n",
      "Epoch: 1168 | train_loss: 118.2962570190 | test_loss: 6.2206478119 | \n",
      "Epoch: 1169 | train_loss: 118.2955780029 | test_loss: 6.2206349373 | \n",
      "Epoch: 1170 | train_loss: 118.2949523926 | test_loss: 6.2206254005 | \n",
      "Epoch: 1171 | train_loss: 118.2942657471 | test_loss: 6.2206120491 | \n",
      "Epoch: 1172 | train_loss: 118.2936325073 | test_loss: 6.2206025124 | \n",
      "Epoch: 1173 | train_loss: 118.2930068970 | test_loss: 6.2205939293 | \n",
      "Epoch: 1174 | train_loss: 118.2923812866 | test_loss: 6.2205824852 | \n",
      "Epoch: 1175 | train_loss: 118.2917098999 | test_loss: 6.2205715179 | \n",
      "Epoch: 1176 | train_loss: 118.2910614014 | test_loss: 6.2205629349 | \n",
      "Epoch: 1177 | train_loss: 118.2904891968 | test_loss: 6.2205486298 | \n",
      "Epoch: 1178 | train_loss: 118.2898330688 | test_loss: 6.2205371857 | \n",
      "Epoch: 1179 | train_loss: 118.2892074585 | test_loss: 6.2205228806 | \n",
      "Epoch: 1180 | train_loss: 118.2885742188 | test_loss: 6.2205095291 | \n",
      "Epoch: 1181 | train_loss: 118.2879791260 | test_loss: 6.2204957008 | \n",
      "Epoch: 1182 | train_loss: 118.2873611450 | test_loss: 6.2204799652 | \n",
      "Epoch: 1183 | train_loss: 118.2867279053 | test_loss: 6.2204656601 | \n",
      "Epoch: 1184 | train_loss: 118.2861099243 | test_loss: 6.2204542160 | \n",
      "Epoch: 1185 | train_loss: 118.2855072021 | test_loss: 6.2204351425 | \n",
      "Epoch: 1186 | train_loss: 118.2849044800 | test_loss: 6.2204194069 | \n",
      "Epoch: 1187 | train_loss: 118.2843170166 | test_loss: 6.2204060555 | \n",
      "Epoch: 1188 | train_loss: 118.2837142944 | test_loss: 6.2203955650 | \n",
      "Epoch: 1189 | train_loss: 118.2831039429 | test_loss: 6.2203788757 | \n",
      "Epoch: 1190 | train_loss: 118.2825088501 | test_loss: 6.2203612328 | \n",
      "Epoch: 1191 | train_loss: 118.2819213867 | test_loss: 6.2203474045 | \n",
      "Epoch: 1192 | train_loss: 118.2813186646 | test_loss: 6.2203302383 | \n",
      "Epoch: 1193 | train_loss: 118.2807312012 | test_loss: 6.2203159332 | \n",
      "Epoch: 1194 | train_loss: 118.2801208496 | test_loss: 6.2203001976 | \n",
      "Epoch: 1195 | train_loss: 118.2795486450 | test_loss: 6.2202801704 | \n",
      "Epoch: 1196 | train_loss: 118.2789611816 | test_loss: 6.2202606201 | \n",
      "Epoch: 1197 | train_loss: 118.2783966064 | test_loss: 6.2202453613 | \n",
      "Epoch: 1198 | train_loss: 118.2777938843 | test_loss: 6.2202296257 | \n",
      "Epoch: 1199 | train_loss: 118.2772064209 | test_loss: 6.2202129364 | \n",
      "Epoch: 1200 | train_loss: 118.2766494751 | test_loss: 6.2201952934 | \n",
      "Epoch: 1201 | train_loss: 118.2760848999 | test_loss: 6.2201747894 | \n",
      "Epoch: 1202 | train_loss: 118.2755203247 | test_loss: 6.2201576233 | \n",
      "Epoch: 1203 | train_loss: 118.2749710083 | test_loss: 6.2201404572 | \n",
      "Epoch: 1204 | train_loss: 118.2744064331 | test_loss: 6.2201213837 | \n",
      "Epoch: 1205 | train_loss: 118.2738113403 | test_loss: 6.2200984955 | \n",
      "Epoch: 1206 | train_loss: 118.2732696533 | test_loss: 6.2200803757 | \n",
      "Epoch: 1207 | train_loss: 118.2727127075 | test_loss: 6.2200608253 | \n",
      "Epoch: 1208 | train_loss: 118.2721710205 | test_loss: 6.2200398445 | \n",
      "Epoch: 1209 | train_loss: 118.2716217041 | test_loss: 6.2200236320 | \n",
      "Epoch: 1210 | train_loss: 118.2710571289 | test_loss: 6.2200026512 | \n",
      "Epoch: 1211 | train_loss: 118.2705154419 | test_loss: 6.2199840546 | \n",
      "Epoch: 1212 | train_loss: 118.2699890137 | test_loss: 6.2199621201 | \n",
      "Epoch: 1213 | train_loss: 118.2694396973 | test_loss: 6.2199463844 | \n",
      "Epoch: 1214 | train_loss: 118.2689285278 | test_loss: 6.2199277878 | \n",
      "Epoch: 1215 | train_loss: 118.2683792114 | test_loss: 6.2199096680 | \n",
      "Epoch: 1216 | train_loss: 118.2678604126 | test_loss: 6.2198863029 | \n",
      "Epoch: 1217 | train_loss: 118.2673416138 | test_loss: 6.2198653221 | \n",
      "Epoch: 1218 | train_loss: 118.2667999268 | test_loss: 6.2198429108 | \n",
      "Epoch: 1219 | train_loss: 118.2662506104 | test_loss: 6.2198238373 | \n",
      "Epoch: 1220 | train_loss: 118.2657012939 | test_loss: 6.2198052406 | \n",
      "Epoch: 1221 | train_loss: 118.2652053833 | test_loss: 6.2197847366 | \n",
      "Epoch: 1222 | train_loss: 118.2646865845 | test_loss: 6.2197656631 | \n",
      "Epoch: 1223 | train_loss: 118.2641677856 | test_loss: 6.2197413445 | \n",
      "Epoch: 1224 | train_loss: 118.2636489868 | test_loss: 6.2197184563 | \n",
      "Epoch: 1225 | train_loss: 118.2631378174 | test_loss: 6.2197003365 | \n",
      "Epoch: 1226 | train_loss: 118.2626266479 | test_loss: 6.2196803093 | \n",
      "Epoch: 1227 | train_loss: 118.2621307373 | test_loss: 6.2196540833 | \n",
      "Epoch: 1228 | train_loss: 118.2616119385 | test_loss: 6.2196354866 | \n",
      "Epoch: 1229 | train_loss: 118.2611312866 | test_loss: 6.2196111679 | \n",
      "Epoch: 1230 | train_loss: 118.2606124878 | test_loss: 6.2195930481 | \n",
      "Epoch: 1231 | train_loss: 118.2600860596 | test_loss: 6.2195625305 | \n",
      "Epoch: 1232 | train_loss: 118.2595977783 | test_loss: 6.2195491791 | \n",
      "Epoch: 1233 | train_loss: 118.2590942383 | test_loss: 6.2195219994 | \n",
      "Epoch: 1234 | train_loss: 118.2585754395 | test_loss: 6.2195062637 | \n",
      "Epoch: 1235 | train_loss: 118.2580795288 | test_loss: 6.2194766998 | \n",
      "Epoch: 1236 | train_loss: 118.2575912476 | test_loss: 6.2194538116 | \n",
      "Epoch: 1237 | train_loss: 118.2570800781 | test_loss: 6.2194304466 | \n",
      "Epoch: 1238 | train_loss: 118.2566299438 | test_loss: 6.2194118500 | \n",
      "Epoch: 1239 | train_loss: 118.2561340332 | test_loss: 6.2193894386 | \n",
      "Epoch: 1240 | train_loss: 118.2556762695 | test_loss: 6.2193651199 | \n",
      "Epoch: 1241 | train_loss: 118.2551498413 | test_loss: 6.2193446159 | \n",
      "Epoch: 1242 | train_loss: 118.2546997070 | test_loss: 6.2193169594 | \n",
      "Epoch: 1243 | train_loss: 118.2542114258 | test_loss: 6.2192974091 | \n",
      "Epoch: 1244 | train_loss: 118.2537689209 | test_loss: 6.2192759514 | \n",
      "Epoch: 1245 | train_loss: 118.2532806396 | test_loss: 6.2192506790 | \n",
      "Epoch: 1246 | train_loss: 118.2527770996 | test_loss: 6.2192254066 | \n",
      "Epoch: 1247 | train_loss: 118.2522659302 | test_loss: 6.2192034721 | \n",
      "Epoch: 1248 | train_loss: 118.2518234253 | test_loss: 6.2191786766 | \n",
      "Epoch: 1249 | train_loss: 118.2513732910 | test_loss: 6.2191605568 | \n",
      "Epoch: 1250 | train_loss: 118.2508850098 | test_loss: 6.2191371918 | \n",
      "Epoch: 1251 | train_loss: 118.2504196167 | test_loss: 6.2191090584 | \n",
      "Epoch: 1252 | train_loss: 118.2499923706 | test_loss: 6.2190861702 | \n",
      "Epoch: 1253 | train_loss: 118.2495346069 | test_loss: 6.2190666199 | \n",
      "Epoch: 1254 | train_loss: 118.2490463257 | test_loss: 6.2190413475 | \n",
      "Epoch: 1255 | train_loss: 118.2486114502 | test_loss: 6.2190165520 | \n",
      "Epoch: 1256 | train_loss: 118.2481384277 | test_loss: 6.2189960480 | \n",
      "Epoch: 1257 | train_loss: 118.2476959229 | test_loss: 6.2189631462 | \n",
      "Epoch: 1258 | train_loss: 118.2472152710 | test_loss: 6.2189445496 | \n",
      "Epoch: 1259 | train_loss: 118.2467803955 | test_loss: 6.2189183235 | \n",
      "Epoch: 1260 | train_loss: 118.2463150024 | test_loss: 6.2188944817 | \n",
      "Epoch: 1261 | train_loss: 118.2458572388 | test_loss: 6.2188720703 | \n",
      "Epoch: 1262 | train_loss: 118.2454071045 | test_loss: 6.2188472748 | \n",
      "Epoch: 1263 | train_loss: 118.2449645996 | test_loss: 6.2188215256 | \n",
      "Epoch: 1264 | train_loss: 118.2445144653 | test_loss: 6.2187976837 | \n",
      "Epoch: 1265 | train_loss: 118.2441101074 | test_loss: 6.2187709808 | \n",
      "Epoch: 1266 | train_loss: 118.2436752319 | test_loss: 6.2187500000 | \n",
      "Epoch: 1267 | train_loss: 118.2432250977 | test_loss: 6.2187256813 | \n",
      "Epoch: 1268 | train_loss: 118.2427902222 | test_loss: 6.2187004089 | \n",
      "Epoch: 1269 | train_loss: 118.2423248291 | test_loss: 6.2186732292 | \n",
      "Epoch: 1270 | train_loss: 118.2419128418 | test_loss: 6.2186512947 | \n",
      "Epoch: 1271 | train_loss: 118.2414627075 | test_loss: 6.2186298370 | \n",
      "Epoch: 1272 | train_loss: 118.2410430908 | test_loss: 6.2186055183 | \n",
      "Epoch: 1273 | train_loss: 118.2405929565 | test_loss: 6.2185778618 | \n",
      "Epoch: 1274 | train_loss: 118.2401733398 | test_loss: 6.2185554504 | \n",
      "Epoch: 1275 | train_loss: 118.2397537231 | test_loss: 6.2185273170 | \n",
      "Epoch: 1276 | train_loss: 118.2392959595 | test_loss: 6.2185072899 | \n",
      "Epoch: 1277 | train_loss: 118.2389144897 | test_loss: 6.2184801102 | \n",
      "Epoch: 1278 | train_loss: 118.2384872437 | test_loss: 6.2184514999 | \n",
      "Epoch: 1279 | train_loss: 118.2380447388 | test_loss: 6.2184267044 | \n",
      "Epoch: 1280 | train_loss: 118.2376480103 | test_loss: 6.2184028625 | \n",
      "Epoch: 1281 | train_loss: 118.2372283936 | test_loss: 6.2183799744 | \n",
      "Epoch: 1282 | train_loss: 118.2368164062 | test_loss: 6.2183518410 | \n",
      "Epoch: 1283 | train_loss: 118.2363891602 | test_loss: 6.2183213234 | \n",
      "Epoch: 1284 | train_loss: 118.2359771729 | test_loss: 6.2183012962 | \n",
      "Epoch: 1285 | train_loss: 118.2355880737 | test_loss: 6.2182774544 | \n",
      "Epoch: 1286 | train_loss: 118.2351837158 | test_loss: 6.2182531357 | \n",
      "Epoch: 1287 | train_loss: 118.2347335815 | test_loss: 6.2182245255 | \n",
      "Epoch: 1288 | train_loss: 118.2343292236 | test_loss: 6.2181987762 | \n",
      "Epoch: 1289 | train_loss: 118.2339630127 | test_loss: 6.2181696892 | \n",
      "Epoch: 1290 | train_loss: 118.2335510254 | test_loss: 6.2181401253 | \n",
      "Epoch: 1291 | train_loss: 118.2331085205 | test_loss: 6.2181153297 | \n",
      "Epoch: 1292 | train_loss: 118.2327041626 | test_loss: 6.2180924416 | \n",
      "Epoch: 1293 | train_loss: 118.2323303223 | test_loss: 6.2180681229 | \n",
      "Epoch: 1294 | train_loss: 118.2319030762 | test_loss: 6.2180390358 | \n",
      "Epoch: 1295 | train_loss: 118.2315063477 | test_loss: 6.2180156708 | \n",
      "Epoch: 1296 | train_loss: 118.2311019897 | test_loss: 6.2179870605 | \n",
      "Epoch: 1297 | train_loss: 118.2307357788 | test_loss: 6.2179622650 | \n",
      "Epoch: 1298 | train_loss: 118.2303237915 | test_loss: 6.2179341316 | \n",
      "Epoch: 1299 | train_loss: 118.2299041748 | test_loss: 6.2179136276 | \n",
      "Epoch: 1300 | train_loss: 118.2294921875 | test_loss: 6.2178859711 | \n",
      "Epoch: 1301 | train_loss: 118.2290954590 | test_loss: 6.2178564072 | \n",
      "Epoch: 1302 | train_loss: 118.2287368774 | test_loss: 6.2178320885 | \n",
      "Epoch: 1303 | train_loss: 118.2283325195 | test_loss: 6.2178034782 | \n",
      "Epoch: 1304 | train_loss: 118.2279205322 | test_loss: 6.2177715302 | \n",
      "Epoch: 1305 | train_loss: 118.2275772095 | test_loss: 6.2177476883 | \n",
      "Epoch: 1306 | train_loss: 118.2271804810 | test_loss: 6.2177238464 | \n",
      "Epoch: 1307 | train_loss: 118.2268142700 | test_loss: 6.2176942825 | \n",
      "Epoch: 1308 | train_loss: 118.2263946533 | test_loss: 6.2176680565 | \n",
      "Epoch: 1309 | train_loss: 118.2260437012 | test_loss: 6.2176446915 | \n",
      "Epoch: 1310 | train_loss: 118.2256622314 | test_loss: 6.2176146507 | \n",
      "Epoch: 1311 | train_loss: 118.2252655029 | test_loss: 6.2175889015 | \n",
      "Epoch: 1312 | train_loss: 118.2249298096 | test_loss: 6.2175593376 | \n",
      "Epoch: 1313 | train_loss: 118.2245330811 | test_loss: 6.2175331116 | \n",
      "Epoch: 1314 | train_loss: 118.2241287231 | test_loss: 6.2175097466 | \n",
      "Epoch: 1315 | train_loss: 118.2238006592 | test_loss: 6.2174777985 | \n",
      "Epoch: 1316 | train_loss: 118.2233657837 | test_loss: 6.2174510956 | \n",
      "Epoch: 1317 | train_loss: 118.2230148315 | test_loss: 6.2174229622 | \n",
      "Epoch: 1318 | train_loss: 118.2226638794 | test_loss: 6.2173957825 | \n",
      "Epoch: 1319 | train_loss: 118.2222824097 | test_loss: 6.2173676491 | \n",
      "Epoch: 1320 | train_loss: 118.2219467163 | test_loss: 6.2173380852 | \n",
      "Epoch: 1321 | train_loss: 118.2215270996 | test_loss: 6.2173171043 | \n",
      "Epoch: 1322 | train_loss: 118.2211837769 | test_loss: 6.2172932625 | \n",
      "Epoch: 1323 | train_loss: 118.2208328247 | test_loss: 6.2172589302 | \n",
      "Epoch: 1324 | train_loss: 118.2204818726 | test_loss: 6.2172365189 | \n",
      "Epoch: 1325 | train_loss: 118.2200851440 | test_loss: 6.2172031403 | \n",
      "Epoch: 1326 | train_loss: 118.2197265625 | test_loss: 6.2171759605 | \n",
      "Epoch: 1327 | train_loss: 118.2193527222 | test_loss: 6.2171454430 | \n",
      "Epoch: 1328 | train_loss: 118.2190170288 | test_loss: 6.2171168327 | \n",
      "Epoch: 1329 | train_loss: 118.2186203003 | test_loss: 6.2170906067 | \n",
      "Epoch: 1330 | train_loss: 118.2182693481 | test_loss: 6.2170605659 | \n",
      "Epoch: 1331 | train_loss: 118.2179183960 | test_loss: 6.2170338631 | \n",
      "Epoch: 1332 | train_loss: 118.2175674438 | test_loss: 6.2170042992 | \n",
      "Epoch: 1333 | train_loss: 118.2171859741 | test_loss: 6.2169785500 | \n",
      "Epoch: 1334 | train_loss: 118.2168655396 | test_loss: 6.2169466019 | \n",
      "Epoch: 1335 | train_loss: 118.2164840698 | test_loss: 6.2169170380 | \n",
      "Epoch: 1336 | train_loss: 118.2161712646 | test_loss: 6.2168908119 | \n",
      "Epoch: 1337 | train_loss: 118.2157974243 | test_loss: 6.2168560028 | \n",
      "Epoch: 1338 | train_loss: 118.2154312134 | test_loss: 6.2168335915 | \n",
      "Epoch: 1339 | train_loss: 118.2150878906 | test_loss: 6.2167963982 | \n",
      "Epoch: 1340 | train_loss: 118.2147369385 | test_loss: 6.2167658806 | \n",
      "Epoch: 1341 | train_loss: 118.2143936157 | test_loss: 6.2167401314 | \n",
      "Epoch: 1342 | train_loss: 118.2140502930 | test_loss: 6.2167119980 | \n",
      "Epoch: 1343 | train_loss: 118.2136993408 | test_loss: 6.2166829109 | \n",
      "Epoch: 1344 | train_loss: 118.2133102417 | test_loss: 6.2166533470 | \n",
      "Epoch: 1345 | train_loss: 118.2130203247 | test_loss: 6.2166228294 | \n",
      "Epoch: 1346 | train_loss: 118.2126235962 | test_loss: 6.2165946960 | \n",
      "Epoch: 1347 | train_loss: 118.2123031616 | test_loss: 6.2165660858 | \n",
      "Epoch: 1348 | train_loss: 118.2119598389 | test_loss: 6.2165374756 | \n",
      "Epoch: 1349 | train_loss: 118.2116241455 | test_loss: 6.2165088654 | \n",
      "Epoch: 1350 | train_loss: 118.2112808228 | test_loss: 6.2164783478 | \n",
      "Epoch: 1351 | train_loss: 118.2109680176 | test_loss: 6.2164459229 | \n",
      "Epoch: 1352 | train_loss: 118.2105484009 | test_loss: 6.2164196968 | \n",
      "Epoch: 1353 | train_loss: 118.2102355957 | test_loss: 6.2163877487 | \n",
      "Epoch: 1354 | train_loss: 118.2098999023 | test_loss: 6.2163562775 | \n",
      "Epoch: 1355 | train_loss: 118.2095489502 | test_loss: 6.2163276672 | \n",
      "Epoch: 1356 | train_loss: 118.2092208862 | test_loss: 6.2162995338 | \n",
      "Epoch: 1357 | train_loss: 118.2088928223 | test_loss: 6.2162656784 | \n",
      "Epoch: 1358 | train_loss: 118.2085342407 | test_loss: 6.2162380219 | \n",
      "Epoch: 1359 | train_loss: 118.2082443237 | test_loss: 6.2162075043 | \n",
      "Epoch: 1360 | train_loss: 118.2079315186 | test_loss: 6.2161784172 | \n",
      "Epoch: 1361 | train_loss: 118.2075424194 | test_loss: 6.2161512375 | \n",
      "Epoch: 1362 | train_loss: 118.2072753906 | test_loss: 6.2161207199 | \n",
      "Epoch: 1363 | train_loss: 118.2068939209 | test_loss: 6.2160902023 | \n",
      "Epoch: 1364 | train_loss: 118.2065811157 | test_loss: 6.2160620689 | \n",
      "Epoch: 1365 | train_loss: 118.2062377930 | test_loss: 6.2160253525 | \n",
      "Epoch: 1366 | train_loss: 118.2059020996 | test_loss: 6.2159991264 | \n",
      "Epoch: 1367 | train_loss: 118.2055740356 | test_loss: 6.2159714699 | \n",
      "Epoch: 1368 | train_loss: 118.2052612305 | test_loss: 6.2159361839 | \n",
      "Epoch: 1369 | train_loss: 118.2049102783 | test_loss: 6.2159032822 | \n",
      "Epoch: 1370 | train_loss: 118.2046127319 | test_loss: 6.2158732414 | \n",
      "Epoch: 1371 | train_loss: 118.2042541504 | test_loss: 6.2158446312 | \n",
      "Epoch: 1372 | train_loss: 118.2039566040 | test_loss: 6.2158122063 | \n",
      "Epoch: 1373 | train_loss: 118.2036056519 | test_loss: 6.2157793045 | \n",
      "Epoch: 1374 | train_loss: 118.2033004761 | test_loss: 6.2157506943 | \n",
      "Epoch: 1375 | train_loss: 118.2029571533 | test_loss: 6.2157149315 | \n",
      "Epoch: 1376 | train_loss: 118.2026367188 | test_loss: 6.2156901360 | \n",
      "Epoch: 1377 | train_loss: 118.2023315430 | test_loss: 6.2156610489 | \n",
      "Epoch: 1378 | train_loss: 118.2020187378 | test_loss: 6.2156310081 | \n",
      "Epoch: 1379 | train_loss: 118.2016906738 | test_loss: 6.2155900002 | \n",
      "Epoch: 1380 | train_loss: 118.2013549805 | test_loss: 6.2155623436 | \n",
      "Epoch: 1381 | train_loss: 118.2010421753 | test_loss: 6.2155308723 | \n",
      "Epoch: 1382 | train_loss: 118.2007446289 | test_loss: 6.2154994011 | \n",
      "Epoch: 1383 | train_loss: 118.2004165649 | test_loss: 6.2154693604 | \n",
      "Epoch: 1384 | train_loss: 118.2001190186 | test_loss: 6.2154369354 | \n",
      "Epoch: 1385 | train_loss: 118.1997451782 | test_loss: 6.2154045105 | \n",
      "Epoch: 1386 | train_loss: 118.1994323730 | test_loss: 6.2153754234 | \n",
      "Epoch: 1387 | train_loss: 118.1991577148 | test_loss: 6.2153449059 | \n",
      "Epoch: 1388 | train_loss: 118.1988449097 | test_loss: 6.2153100967 | \n",
      "Epoch: 1389 | train_loss: 118.1985473633 | test_loss: 6.2152776718 | \n",
      "Epoch: 1390 | train_loss: 118.1982345581 | test_loss: 6.2152471542 | \n",
      "Epoch: 1391 | train_loss: 118.1978988647 | test_loss: 6.2152113914 | \n",
      "Epoch: 1392 | train_loss: 118.1975784302 | test_loss: 6.2151794434 | \n",
      "Epoch: 1393 | train_loss: 118.1973037720 | test_loss: 6.2151474953 | \n",
      "Epoch: 1394 | train_loss: 118.1969833374 | test_loss: 6.2151179314 | \n",
      "Epoch: 1395 | train_loss: 118.1966781616 | test_loss: 6.2150812149 | \n",
      "Epoch: 1396 | train_loss: 118.1963653564 | test_loss: 6.2150545120 | \n",
      "Epoch: 1397 | train_loss: 118.1960754395 | test_loss: 6.2150173187 | \n",
      "Epoch: 1398 | train_loss: 118.1957702637 | test_loss: 6.2149829865 | \n",
      "Epoch: 1399 | train_loss: 118.1954269409 | test_loss: 6.2149543762 | \n",
      "Epoch: 1400 | train_loss: 118.1951370239 | test_loss: 6.2149229050 | \n",
      "Epoch: 1401 | train_loss: 118.1948699951 | test_loss: 6.2148919106 | \n",
      "Epoch: 1402 | train_loss: 118.1945419312 | test_loss: 6.2148637772 | \n",
      "Epoch: 1403 | train_loss: 118.1942443848 | test_loss: 6.2148218155 | \n",
      "Epoch: 1404 | train_loss: 118.1939392090 | test_loss: 6.2147932053 | \n",
      "Epoch: 1405 | train_loss: 118.1936111450 | test_loss: 6.2147560120 | \n",
      "Epoch: 1406 | train_loss: 118.1933288574 | test_loss: 6.2147283554 | \n",
      "Epoch: 1407 | train_loss: 118.1930084229 | test_loss: 6.2146973610 | \n",
      "Epoch: 1408 | train_loss: 118.1927337646 | test_loss: 6.2146601677 | \n",
      "Epoch: 1409 | train_loss: 118.1924057007 | test_loss: 6.2146263123 | \n",
      "Epoch: 1410 | train_loss: 118.1921005249 | test_loss: 6.2145905495 | \n",
      "Epoch: 1411 | train_loss: 118.1918258667 | test_loss: 6.2145576477 | \n",
      "Epoch: 1412 | train_loss: 118.1914825439 | test_loss: 6.2145271301 | \n",
      "Epoch: 1413 | train_loss: 118.1912078857 | test_loss: 6.2144865990 | \n",
      "Epoch: 1414 | train_loss: 118.1908950806 | test_loss: 6.2144594193 | \n",
      "Epoch: 1415 | train_loss: 118.1906127930 | test_loss: 6.2144289017 | \n",
      "Epoch: 1416 | train_loss: 118.1902999878 | test_loss: 6.2143859863 | \n",
      "Epoch: 1417 | train_loss: 118.1900177002 | test_loss: 6.2143530846 | \n",
      "Epoch: 1418 | train_loss: 118.1897048950 | test_loss: 6.2143168449 | \n",
      "Epoch: 1419 | train_loss: 118.1894302368 | test_loss: 6.2142853737 | \n",
      "Epoch: 1420 | train_loss: 118.1891174316 | test_loss: 6.2142481804 | \n",
      "Epoch: 1421 | train_loss: 118.1888351440 | test_loss: 6.2142176628 | \n",
      "Epoch: 1422 | train_loss: 118.1885070801 | test_loss: 6.2141861916 | \n",
      "Epoch: 1423 | train_loss: 118.1882629395 | test_loss: 6.2141489983 | \n",
      "Epoch: 1424 | train_loss: 118.1879577637 | test_loss: 6.2141108513 | \n",
      "Epoch: 1425 | train_loss: 118.1876754761 | test_loss: 6.2140812874 | \n",
      "Epoch: 1426 | train_loss: 118.1873703003 | test_loss: 6.2140522003 | \n",
      "Epoch: 1427 | train_loss: 118.1870727539 | test_loss: 6.2140102386 | \n",
      "Epoch: 1428 | train_loss: 118.1867752075 | test_loss: 6.2139763832 | \n",
      "Epoch: 1429 | train_loss: 118.1864929199 | test_loss: 6.2139449120 | \n",
      "Epoch: 1430 | train_loss: 118.1862182617 | test_loss: 6.2139101028 | \n",
      "Epoch: 1431 | train_loss: 118.1859359741 | test_loss: 6.2138752937 | \n",
      "Epoch: 1432 | train_loss: 118.1856155396 | test_loss: 6.2138433456 | \n",
      "Epoch: 1433 | train_loss: 118.1853713989 | test_loss: 6.2138056755 | \n",
      "Epoch: 1434 | train_loss: 118.1850814819 | test_loss: 6.2137651443 | \n",
      "Epoch: 1435 | train_loss: 118.1847534180 | test_loss: 6.2137355804 | \n",
      "Epoch: 1436 | train_loss: 118.1845245361 | test_loss: 6.2136969566 | \n",
      "Epoch: 1437 | train_loss: 118.1841964722 | test_loss: 6.2136597633 | \n",
      "Epoch: 1438 | train_loss: 118.1838989258 | test_loss: 6.2136216164 | \n",
      "Epoch: 1439 | train_loss: 118.1836013794 | test_loss: 6.2135901451 | \n",
      "Epoch: 1440 | train_loss: 118.1833419800 | test_loss: 6.2135534286 | \n",
      "Epoch: 1441 | train_loss: 118.1830444336 | test_loss: 6.2135186195 | \n",
      "Epoch: 1442 | train_loss: 118.1827697754 | test_loss: 6.2134842873 | \n",
      "Epoch: 1443 | train_loss: 118.1824722290 | test_loss: 6.2134480476 | \n",
      "Epoch: 1444 | train_loss: 118.1821899414 | test_loss: 6.2134113312 | \n",
      "Epoch: 1445 | train_loss: 118.1819229126 | test_loss: 6.2133727074 | \n",
      "Epoch: 1446 | train_loss: 118.1816177368 | test_loss: 6.2133398056 | \n",
      "Epoch: 1447 | train_loss: 118.1813583374 | test_loss: 6.2133064270 | \n",
      "Epoch: 1448 | train_loss: 118.1810836792 | test_loss: 6.2132725716 | \n",
      "Epoch: 1449 | train_loss: 118.1808013916 | test_loss: 6.2132291794 | \n",
      "Epoch: 1450 | train_loss: 118.1804962158 | test_loss: 6.2131943703 | \n",
      "Epoch: 1451 | train_loss: 118.1802368164 | test_loss: 6.2131586075 | \n",
      "Epoch: 1452 | train_loss: 118.1799545288 | test_loss: 6.2131199837 | \n",
      "Epoch: 1453 | train_loss: 118.1796417236 | test_loss: 6.2130832672 | \n",
      "Epoch: 1454 | train_loss: 118.1793975830 | test_loss: 6.2130503654 | \n",
      "Epoch: 1455 | train_loss: 118.1790924072 | test_loss: 6.2130112648 | \n",
      "Epoch: 1456 | train_loss: 118.1788177490 | test_loss: 6.2129793167 | \n",
      "Epoch: 1457 | train_loss: 118.1785583496 | test_loss: 6.2129378319 | \n",
      "Epoch: 1458 | train_loss: 118.1782531738 | test_loss: 6.2128996849 | \n",
      "Epoch: 1459 | train_loss: 118.1779785156 | test_loss: 6.2128615379 | \n",
      "Epoch: 1460 | train_loss: 118.1776962280 | test_loss: 6.2128291130 | \n",
      "Epoch: 1461 | train_loss: 118.1774291992 | test_loss: 6.2127952576 | \n",
      "Epoch: 1462 | train_loss: 118.1771545410 | test_loss: 6.2127537727 | \n",
      "Epoch: 1463 | train_loss: 118.1769104004 | test_loss: 6.2127227783 | \n",
      "Epoch: 1464 | train_loss: 118.1766510010 | test_loss: 6.2126832008 | \n",
      "Epoch: 1465 | train_loss: 118.1763000488 | test_loss: 6.2126512527 | \n",
      "Epoch: 1466 | train_loss: 118.1760559082 | test_loss: 6.2126069069 | \n",
      "Epoch: 1467 | train_loss: 118.1757812500 | test_loss: 6.2125706673 | \n",
      "Epoch: 1468 | train_loss: 118.1755065918 | test_loss: 6.2125329971 | \n",
      "Epoch: 1469 | train_loss: 118.1752471924 | test_loss: 6.2124986649 | \n",
      "Epoch: 1470 | train_loss: 118.1749801636 | test_loss: 6.2124519348 | \n",
      "Epoch: 1471 | train_loss: 118.1747055054 | test_loss: 6.2124161720 | \n",
      "Epoch: 1472 | train_loss: 118.1744232178 | test_loss: 6.2123789787 | \n",
      "Epoch: 1473 | train_loss: 118.1741180420 | test_loss: 6.2123436928 | \n",
      "Epoch: 1474 | train_loss: 118.1738510132 | test_loss: 6.2123112679 | \n",
      "Epoch: 1475 | train_loss: 118.1736068726 | test_loss: 6.2122707367 | \n",
      "Epoch: 1476 | train_loss: 118.1733398438 | test_loss: 6.2122344971 | \n",
      "Epoch: 1477 | train_loss: 118.1730957031 | test_loss: 6.2121887207 | \n",
      "Epoch: 1478 | train_loss: 118.1727905273 | test_loss: 6.2121510506 | \n",
      "Epoch: 1479 | train_loss: 118.1725006104 | test_loss: 6.2121143341 | \n",
      "Epoch: 1480 | train_loss: 118.1722564697 | test_loss: 6.2120780945 | \n",
      "Epoch: 1481 | train_loss: 118.1719894409 | test_loss: 6.2120389938 | \n",
      "Epoch: 1482 | train_loss: 118.1717224121 | test_loss: 6.2119989395 | \n",
      "Epoch: 1483 | train_loss: 118.1714096069 | test_loss: 6.2119617462 | \n",
      "Epoch: 1484 | train_loss: 118.1711883545 | test_loss: 6.2119202614 | \n",
      "Epoch: 1485 | train_loss: 118.1709213257 | test_loss: 6.2118840218 | \n",
      "Epoch: 1486 | train_loss: 118.1706619263 | test_loss: 6.2118482590 | \n",
      "Epoch: 1487 | train_loss: 118.1704025269 | test_loss: 6.2118091583 | \n",
      "Epoch: 1488 | train_loss: 118.1701049805 | test_loss: 6.2117676735 | \n",
      "Epoch: 1489 | train_loss: 118.1698532104 | test_loss: 6.2117271423 | \n",
      "Epoch: 1490 | train_loss: 118.1696166992 | test_loss: 6.2116942406 | \n",
      "Epoch: 1491 | train_loss: 118.1693038940 | test_loss: 6.2116498947 | \n",
      "Epoch: 1492 | train_loss: 118.1690444946 | test_loss: 6.2116088867 | \n",
      "Epoch: 1493 | train_loss: 118.1687927246 | test_loss: 6.2115654945 | \n",
      "Epoch: 1494 | train_loss: 118.1685180664 | test_loss: 6.2115254402 | \n",
      "Epoch: 1495 | train_loss: 118.1682586670 | test_loss: 6.2114934921 | \n",
      "Epoch: 1496 | train_loss: 118.1679916382 | test_loss: 6.2114481926 | \n",
      "Epoch: 1497 | train_loss: 118.1676940918 | test_loss: 6.2114109993 | \n",
      "Epoch: 1498 | train_loss: 118.1674423218 | test_loss: 6.2113652229 | \n",
      "Epoch: 1499 | train_loss: 118.1671905518 | test_loss: 6.2113304138 | \n",
      "Epoch: 1500 | train_loss: 118.1669616699 | test_loss: 6.2112903595 | \n",
      "Epoch: 1501 | train_loss: 118.1666793823 | test_loss: 6.2112555504 | \n",
      "Epoch: 1502 | train_loss: 118.1664047241 | test_loss: 6.2112121582 | \n",
      "Epoch: 1503 | train_loss: 118.1661682129 | test_loss: 6.2111692429 | \n",
      "Epoch: 1504 | train_loss: 118.1658782959 | test_loss: 6.2111310959 | \n",
      "Epoch: 1505 | train_loss: 118.1656417847 | test_loss: 6.2110919952 | \n",
      "Epoch: 1506 | train_loss: 118.1653900146 | test_loss: 6.2110514641 | \n",
      "Epoch: 1507 | train_loss: 118.1651306152 | test_loss: 6.2110095024 | \n",
      "Epoch: 1508 | train_loss: 118.1648559570 | test_loss: 6.2109708786 | \n",
      "Epoch: 1509 | train_loss: 118.1645889282 | test_loss: 6.2109317780 | \n",
      "Epoch: 1510 | train_loss: 118.1643447876 | test_loss: 6.2108902931 | \n",
      "Epoch: 1511 | train_loss: 118.1640701294 | test_loss: 6.2108545303 | \n",
      "Epoch: 1512 | train_loss: 118.1638259888 | test_loss: 6.2108082771 | \n",
      "Epoch: 1513 | train_loss: 118.1635284424 | test_loss: 6.2107677460 | \n",
      "Epoch: 1514 | train_loss: 118.1632843018 | test_loss: 6.2107305527 | \n",
      "Epoch: 1515 | train_loss: 118.1630401611 | test_loss: 6.2106904984 | \n",
      "Epoch: 1516 | train_loss: 118.1627731323 | test_loss: 6.2106451988 | \n",
      "Epoch: 1517 | train_loss: 118.1625213623 | test_loss: 6.2106056213 | \n",
      "Epoch: 1518 | train_loss: 118.1622238159 | test_loss: 6.2105617523 | \n",
      "Epoch: 1519 | train_loss: 118.1620025635 | test_loss: 6.2105231285 | \n",
      "Epoch: 1520 | train_loss: 118.1617355347 | test_loss: 6.2104802132 | \n",
      "Epoch: 1521 | train_loss: 118.1614685059 | test_loss: 6.2104430199 | \n",
      "Epoch: 1522 | train_loss: 118.1612243652 | test_loss: 6.2103967667 | \n",
      "Epoch: 1523 | train_loss: 118.1609573364 | test_loss: 6.2103552818 | \n",
      "Epoch: 1524 | train_loss: 118.1607208252 | test_loss: 6.2103137970 | \n",
      "Epoch: 1525 | train_loss: 118.1604614258 | test_loss: 6.2102775574 | \n",
      "Epoch: 1526 | train_loss: 118.1601867676 | test_loss: 6.2102355957 | \n",
      "Epoch: 1527 | train_loss: 118.1599349976 | test_loss: 6.2101912498 | \n",
      "Epoch: 1528 | train_loss: 118.1596908569 | test_loss: 6.2101488113 | \n",
      "Epoch: 1529 | train_loss: 118.1594238281 | test_loss: 6.2101087570 | \n",
      "Epoch: 1530 | train_loss: 118.1591567993 | test_loss: 6.2100691795 | \n",
      "Epoch: 1531 | train_loss: 118.1589431763 | test_loss: 6.2100200653 | \n",
      "Epoch: 1532 | train_loss: 118.1586608887 | test_loss: 6.2099804878 | \n",
      "Epoch: 1533 | train_loss: 118.1583862305 | test_loss: 6.2099361420 | \n",
      "Epoch: 1534 | train_loss: 118.1581420898 | test_loss: 6.2098975182 | \n",
      "Epoch: 1535 | train_loss: 118.1578674316 | test_loss: 6.2098541260 | \n",
      "Epoch: 1536 | train_loss: 118.1576156616 | test_loss: 6.2098112106 | \n",
      "Epoch: 1537 | train_loss: 118.1573562622 | test_loss: 6.2097697258 | \n",
      "Epoch: 1538 | train_loss: 118.1571197510 | test_loss: 6.2097234726 | \n",
      "Epoch: 1539 | train_loss: 118.1568527222 | test_loss: 6.2096838951 | \n",
      "Epoch: 1540 | train_loss: 118.1565856934 | test_loss: 6.2096376419 | \n",
      "Epoch: 1541 | train_loss: 118.1563186646 | test_loss: 6.2095994949 | \n",
      "Epoch: 1542 | train_loss: 118.1560745239 | test_loss: 6.2095561028 | \n",
      "Epoch: 1543 | train_loss: 118.1558227539 | test_loss: 6.2095141411 | \n",
      "Epoch: 1544 | train_loss: 118.1555786133 | test_loss: 6.2094740868 | \n",
      "Epoch: 1545 | train_loss: 118.1553039551 | test_loss: 6.2094292641 | \n",
      "Epoch: 1546 | train_loss: 118.1550598145 | test_loss: 6.2093877792 | \n",
      "Epoch: 1547 | train_loss: 118.1548080444 | test_loss: 6.2093467712 | \n",
      "Epoch: 1548 | train_loss: 118.1545867920 | test_loss: 6.2092981339 | \n",
      "Epoch: 1549 | train_loss: 118.1542968750 | test_loss: 6.2092547417 | \n",
      "Epoch: 1550 | train_loss: 118.1540756226 | test_loss: 6.2092089653 | \n",
      "Epoch: 1551 | train_loss: 118.1538314819 | test_loss: 6.2091693878 | \n",
      "Epoch: 1552 | train_loss: 118.1535644531 | test_loss: 6.2091255188 | \n",
      "Epoch: 1553 | train_loss: 118.1533203125 | test_loss: 6.2090873718 | \n",
      "Epoch: 1554 | train_loss: 118.1530761719 | test_loss: 6.2090439796 | \n",
      "Epoch: 1555 | train_loss: 118.1527938843 | test_loss: 6.2089967728 | \n",
      "Epoch: 1556 | train_loss: 118.1525650024 | test_loss: 6.2089552879 | \n",
      "Epoch: 1557 | train_loss: 118.1523056030 | test_loss: 6.2089076042 | \n",
      "Epoch: 1558 | train_loss: 118.1520309448 | test_loss: 6.2088689804 | \n",
      "Epoch: 1559 | train_loss: 118.1517868042 | test_loss: 6.2088212967 | \n",
      "Epoch: 1560 | train_loss: 118.1515197754 | test_loss: 6.2087755203 | \n",
      "Epoch: 1561 | train_loss: 118.1512756348 | test_loss: 6.2087335587 | \n",
      "Epoch: 1562 | train_loss: 118.1510467529 | test_loss: 6.2086939812 | \n",
      "Epoch: 1563 | train_loss: 118.1508102417 | test_loss: 6.2086472511 | \n",
      "Epoch: 1564 | train_loss: 118.1505279541 | test_loss: 6.2086048126 | \n",
      "Epoch: 1565 | train_loss: 118.1503219604 | test_loss: 6.2085618973 | \n",
      "Epoch: 1566 | train_loss: 118.1500473022 | test_loss: 6.2085199356 | \n",
      "Epoch: 1567 | train_loss: 118.1497726440 | test_loss: 6.2084760666 | \n",
      "Epoch: 1568 | train_loss: 118.1495590210 | test_loss: 6.2084321976 | \n",
      "Epoch: 1569 | train_loss: 118.1492767334 | test_loss: 6.2083873749 | \n",
      "Epoch: 1570 | train_loss: 118.1490249634 | test_loss: 6.2083415985 | \n",
      "Epoch: 1571 | train_loss: 118.1488113403 | test_loss: 6.2082962990 | \n",
      "Epoch: 1572 | train_loss: 118.1485519409 | test_loss: 6.2082505226 | \n",
      "Epoch: 1573 | train_loss: 118.1482925415 | test_loss: 6.2081990242 | \n",
      "Epoch: 1574 | train_loss: 118.1480484009 | test_loss: 6.2081556320 | \n",
      "Epoch: 1575 | train_loss: 118.1478042603 | test_loss: 6.2081151009 | \n",
      "Epoch: 1576 | train_loss: 118.1475372314 | test_loss: 6.2080669403 | \n",
      "Epoch: 1577 | train_loss: 118.1473159790 | test_loss: 6.2080254555 | \n",
      "Epoch: 1578 | train_loss: 118.1470718384 | test_loss: 6.2079820633 | \n",
      "Epoch: 1579 | train_loss: 118.1468276978 | test_loss: 6.2079353333 | \n",
      "Epoch: 1580 | train_loss: 118.1465835571 | test_loss: 6.2078905106 | \n",
      "Epoch: 1581 | train_loss: 118.1463165283 | test_loss: 6.2078428268 | \n",
      "Epoch: 1582 | train_loss: 118.1460571289 | test_loss: 6.2077984810 | \n",
      "Epoch: 1583 | train_loss: 118.1458282471 | test_loss: 6.2077512741 | \n",
      "Epoch: 1584 | train_loss: 118.1455459595 | test_loss: 6.2077064514 | \n",
      "Epoch: 1585 | train_loss: 118.1452865601 | test_loss: 6.2076625824 | \n",
      "Epoch: 1586 | train_loss: 118.1450881958 | test_loss: 6.2076125145 | \n",
      "Epoch: 1587 | train_loss: 118.1448440552 | test_loss: 6.2075700760 | \n",
      "Epoch: 1588 | train_loss: 118.1445999146 | test_loss: 6.2075219154 | \n",
      "Epoch: 1589 | train_loss: 118.1443252563 | test_loss: 6.2074818611 | \n",
      "Epoch: 1590 | train_loss: 118.1441040039 | test_loss: 6.2074317932 | \n",
      "Epoch: 1591 | train_loss: 118.1438903809 | test_loss: 6.2073822021 | \n",
      "Epoch: 1592 | train_loss: 118.1436157227 | test_loss: 6.2073445320 | \n",
      "Epoch: 1593 | train_loss: 118.1434097290 | test_loss: 6.2072887421 | \n",
      "Epoch: 1594 | train_loss: 118.1431579590 | test_loss: 6.2072463036 | \n",
      "Epoch: 1595 | train_loss: 118.1428833008 | test_loss: 6.2072048187 | \n",
      "Epoch: 1596 | train_loss: 118.1426239014 | test_loss: 6.2071528435 | \n",
      "Epoch: 1597 | train_loss: 118.1423950195 | test_loss: 6.2071061134 | \n",
      "Epoch: 1598 | train_loss: 118.1421585083 | test_loss: 6.2070550919 | \n",
      "Epoch: 1599 | train_loss: 118.1418838501 | test_loss: 6.2070145607 | \n",
      "Epoch: 1600 | train_loss: 118.1416320801 | test_loss: 6.2069702148 | \n",
      "Epoch: 1601 | train_loss: 118.1414031982 | test_loss: 6.2069220543 | \n",
      "Epoch: 1602 | train_loss: 118.1411590576 | test_loss: 6.2068758011 | \n",
      "Epoch: 1603 | train_loss: 118.1408920288 | test_loss: 6.2068285942 | \n",
      "Epoch: 1604 | train_loss: 118.1406326294 | test_loss: 6.2067804337 | \n",
      "Epoch: 1605 | train_loss: 118.1404113770 | test_loss: 6.2067351341 | \n",
      "Epoch: 1606 | train_loss: 118.1401977539 | test_loss: 6.2066907883 | \n",
      "Epoch: 1607 | train_loss: 118.1399307251 | test_loss: 6.2066459656 | \n",
      "Epoch: 1608 | train_loss: 118.1396560669 | test_loss: 6.2066020966 | \n",
      "Epoch: 1609 | train_loss: 118.1394500732 | test_loss: 6.2065429688 | \n",
      "Epoch: 1610 | train_loss: 118.1391601562 | test_loss: 6.2065014839 | \n",
      "Epoch: 1611 | train_loss: 118.1389389038 | test_loss: 6.2064585686 | \n",
      "Epoch: 1612 | train_loss: 118.1386947632 | test_loss: 6.2064056396 | \n",
      "Epoch: 1613 | train_loss: 118.1384887695 | test_loss: 6.2063617706 | \n",
      "Epoch: 1614 | train_loss: 118.1382141113 | test_loss: 6.2063117027 | \n",
      "Epoch: 1615 | train_loss: 118.1379623413 | test_loss: 6.2062659264 | \n",
      "Epoch: 1616 | train_loss: 118.1377258301 | test_loss: 6.2062187195 | \n",
      "Epoch: 1617 | train_loss: 118.1374740601 | test_loss: 6.2061729431 | \n",
      "Epoch: 1618 | train_loss: 118.1372528076 | test_loss: 6.2061223984 | \n",
      "Epoch: 1619 | train_loss: 118.1369934082 | test_loss: 6.2060728073 | \n",
      "Epoch: 1620 | train_loss: 118.1367263794 | test_loss: 6.2060256004 | \n",
      "Epoch: 1621 | train_loss: 118.1365432739 | test_loss: 6.2059807777 | \n",
      "Epoch: 1622 | train_loss: 118.1362915039 | test_loss: 6.2059311867 | \n",
      "Epoch: 1623 | train_loss: 118.1360626221 | test_loss: 6.2058863640 | \n",
      "Epoch: 1624 | train_loss: 118.1357879639 | test_loss: 6.2058362961 | \n",
      "Epoch: 1625 | train_loss: 118.1355438232 | test_loss: 6.2057967186 | \n",
      "Epoch: 1626 | train_loss: 118.1353302002 | test_loss: 6.2057495117 | \n",
      "Epoch: 1627 | train_loss: 118.1350860596 | test_loss: 6.2056970596 | \n",
      "Epoch: 1628 | train_loss: 118.1348419189 | test_loss: 6.2056493759 | \n",
      "Epoch: 1629 | train_loss: 118.1345901489 | test_loss: 6.2056007385 | \n",
      "Epoch: 1630 | train_loss: 118.1343460083 | test_loss: 6.2055516243 | \n",
      "Epoch: 1631 | train_loss: 118.1341476440 | test_loss: 6.2055048943 | \n",
      "Epoch: 1632 | train_loss: 118.1338958740 | test_loss: 6.2054557800 | \n",
      "Epoch: 1633 | train_loss: 118.1336364746 | test_loss: 6.2054023743 | \n",
      "Epoch: 1634 | train_loss: 118.1333770752 | test_loss: 6.2053608894 | \n",
      "Epoch: 1635 | train_loss: 118.1331634521 | test_loss: 6.2053070068 | \n",
      "Epoch: 1636 | train_loss: 118.1329421997 | test_loss: 6.2052640915 | \n",
      "Epoch: 1637 | train_loss: 118.1326751709 | test_loss: 6.2052116394 | \n",
      "Epoch: 1638 | train_loss: 118.1324234009 | test_loss: 6.2051582336 | \n",
      "Epoch: 1639 | train_loss: 118.1321563721 | test_loss: 6.2051148415 | \n",
      "Epoch: 1640 | train_loss: 118.1319732666 | test_loss: 6.2050652504 | \n",
      "Epoch: 1641 | train_loss: 118.1316986084 | test_loss: 6.2050189972 | \n",
      "Epoch: 1642 | train_loss: 118.1314544678 | test_loss: 6.2049674988 | \n",
      "Epoch: 1643 | train_loss: 118.1312408447 | test_loss: 6.2049193382 | \n",
      "Epoch: 1644 | train_loss: 118.1309585571 | test_loss: 6.2048726082 | \n",
      "Epoch: 1645 | train_loss: 118.1307601929 | test_loss: 6.2048273087 | \n",
      "Epoch: 1646 | train_loss: 118.1305389404 | test_loss: 6.2047729492 | \n",
      "Epoch: 1647 | train_loss: 118.1302719116 | test_loss: 6.2047266960 | \n",
      "Epoch: 1648 | train_loss: 118.1300201416 | test_loss: 6.2046756744 | \n",
      "Epoch: 1649 | train_loss: 118.1298217773 | test_loss: 6.2046246529 | \n",
      "Epoch: 1650 | train_loss: 118.1295700073 | test_loss: 6.2045764923 | \n",
      "Epoch: 1651 | train_loss: 118.1293182373 | test_loss: 6.2045192719 | \n",
      "Epoch: 1652 | train_loss: 118.1290588379 | test_loss: 6.2044749260 | \n",
      "Epoch: 1653 | train_loss: 118.1288299561 | test_loss: 6.2044200897 | \n",
      "Epoch: 1654 | train_loss: 118.1285552979 | test_loss: 6.2043695450 | \n",
      "Epoch: 1655 | train_loss: 118.1283493042 | test_loss: 6.2043275833 | \n",
      "Epoch: 1656 | train_loss: 118.1281433105 | test_loss: 6.2042779922 | \n",
      "Epoch: 1657 | train_loss: 118.1278839111 | test_loss: 6.2042250633 | \n",
      "Epoch: 1658 | train_loss: 118.1276245117 | test_loss: 6.2041759491 | \n",
      "Epoch: 1659 | train_loss: 118.1274185181 | test_loss: 6.2041316032 | \n",
      "Epoch: 1660 | train_loss: 118.1271591187 | test_loss: 6.2040753365 | \n",
      "Epoch: 1661 | train_loss: 118.1269302368 | test_loss: 6.2040266991 | \n",
      "Epoch: 1662 | train_loss: 118.1267166138 | test_loss: 6.2039747238 | \n",
      "Epoch: 1663 | train_loss: 118.1264419556 | test_loss: 6.2039222717 | \n",
      "Epoch: 1664 | train_loss: 118.1262130737 | test_loss: 6.2038731575 | \n",
      "Epoch: 1665 | train_loss: 118.1259460449 | test_loss: 6.2038254738 | \n",
      "Epoch: 1666 | train_loss: 118.1257324219 | test_loss: 6.2037763596 | \n",
      "Epoch: 1667 | train_loss: 118.1254882812 | test_loss: 6.2037248611 | \n",
      "Epoch: 1668 | train_loss: 118.1252670288 | test_loss: 6.2036724091 | \n",
      "Epoch: 1669 | train_loss: 118.1250000000 | test_loss: 6.2036247253 | \n",
      "Epoch: 1670 | train_loss: 118.1248245239 | test_loss: 6.2035760880 | \n",
      "Epoch: 1671 | train_loss: 118.1245193481 | test_loss: 6.2035231590 | \n",
      "Epoch: 1672 | train_loss: 118.1243133545 | test_loss: 6.2034735680 | \n",
      "Epoch: 1673 | train_loss: 118.1240615845 | test_loss: 6.2034215927 | \n",
      "Epoch: 1674 | train_loss: 118.1238403320 | test_loss: 6.2033705711 | \n",
      "Epoch: 1675 | train_loss: 118.1235733032 | test_loss: 6.2033214569 | \n",
      "Epoch: 1676 | train_loss: 118.1233825684 | test_loss: 6.2032666206 | \n",
      "Epoch: 1677 | train_loss: 118.1231231689 | test_loss: 6.2032170296 | \n",
      "Epoch: 1678 | train_loss: 118.1228713989 | test_loss: 6.2031636238 | \n",
      "Epoch: 1679 | train_loss: 118.1226272583 | test_loss: 6.2031135559 | \n",
      "Epoch: 1680 | train_loss: 118.1223754883 | test_loss: 6.2030653954 | \n",
      "Epoch: 1681 | train_loss: 118.1221618652 | test_loss: 6.2030162811 | \n",
      "Epoch: 1682 | train_loss: 118.1219329834 | test_loss: 6.2029652596 | \n",
      "Epoch: 1683 | train_loss: 118.1217117310 | test_loss: 6.2029118538 | \n",
      "Epoch: 1684 | train_loss: 118.1214523315 | test_loss: 6.2028651237 | \n",
      "Epoch: 1685 | train_loss: 118.1212005615 | test_loss: 6.2028136253 | \n",
      "Epoch: 1686 | train_loss: 118.1209564209 | test_loss: 6.2027583122 | \n",
      "Epoch: 1687 | train_loss: 118.1207351685 | test_loss: 6.2027125359 | \n",
      "Epoch: 1688 | train_loss: 118.1205291748 | test_loss: 6.2026586533 | \n",
      "Epoch: 1689 | train_loss: 118.1202850342 | test_loss: 6.2026100159 | \n",
      "Epoch: 1690 | train_loss: 118.1200256348 | test_loss: 6.2025537491 | \n",
      "Epoch: 1691 | train_loss: 118.1197814941 | test_loss: 6.2025065422 | \n",
      "Epoch: 1692 | train_loss: 118.1195220947 | test_loss: 6.2024531364 | \n",
      "Epoch: 1693 | train_loss: 118.1193084717 | test_loss: 6.2023983002 | \n",
      "Epoch: 1694 | train_loss: 118.1190795898 | test_loss: 6.2023515701 | \n",
      "Epoch: 1695 | train_loss: 118.1188049316 | test_loss: 6.2022981644 | \n",
      "Epoch: 1696 | train_loss: 118.1185760498 | test_loss: 6.2022395134 | \n",
      "Epoch: 1697 | train_loss: 118.1183700562 | test_loss: 6.2021884918 | \n",
      "Epoch: 1698 | train_loss: 118.1181335449 | test_loss: 6.2021317482 | \n",
      "Epoch: 1699 | train_loss: 118.1178741455 | test_loss: 6.2020831108 | \n",
      "Epoch: 1700 | train_loss: 118.1176681519 | test_loss: 6.2020263672 | \n",
      "Epoch: 1701 | train_loss: 118.1174087524 | test_loss: 6.2019767761 | \n",
      "Epoch: 1702 | train_loss: 118.1171417236 | test_loss: 6.2019243240 | \n",
      "Epoch: 1703 | train_loss: 118.1169052124 | test_loss: 6.2018647194 | \n",
      "Epoch: 1704 | train_loss: 118.1166915894 | test_loss: 6.2018189430 | \n",
      "Epoch: 1705 | train_loss: 118.1164627075 | test_loss: 6.2017693520 | \n",
      "Epoch: 1706 | train_loss: 118.1162490845 | test_loss: 6.2017130852 | \n",
      "Epoch: 1707 | train_loss: 118.1160049438 | test_loss: 6.2016582489 | \n",
      "Epoch: 1708 | train_loss: 118.1157531738 | test_loss: 6.2016057968 | \n",
      "Epoch: 1709 | train_loss: 118.1155090332 | test_loss: 6.2015538216 | \n",
      "Epoch: 1710 | train_loss: 118.1152572632 | test_loss: 6.2015013695 | \n",
      "Epoch: 1711 | train_loss: 118.1150588989 | test_loss: 6.2014465332 | \n",
      "Epoch: 1712 | train_loss: 118.1147918701 | test_loss: 6.2013936043 | \n",
      "Epoch: 1713 | train_loss: 118.1145324707 | test_loss: 6.2013401985 | \n",
      "Epoch: 1714 | train_loss: 118.1143035889 | test_loss: 6.2012882233 | \n",
      "Epoch: 1715 | train_loss: 118.1141128540 | test_loss: 6.2012372017 | \n",
      "Epoch: 1716 | train_loss: 118.1138458252 | test_loss: 6.2011842728 | \n",
      "Epoch: 1717 | train_loss: 118.1136169434 | test_loss: 6.2011265755 | \n",
      "Epoch: 1718 | train_loss: 118.1133956909 | test_loss: 6.2010741234 | \n",
      "Epoch: 1719 | train_loss: 118.1131820679 | test_loss: 6.2010207176 | \n",
      "Epoch: 1720 | train_loss: 118.1129379272 | test_loss: 6.2009720802 | \n",
      "Epoch: 1721 | train_loss: 118.1127014160 | test_loss: 6.2009258270 | \n",
      "Epoch: 1722 | train_loss: 118.1124496460 | test_loss: 6.2008666992 | \n",
      "Epoch: 1723 | train_loss: 118.1122131348 | test_loss: 6.2008113861 | \n",
      "Epoch: 1724 | train_loss: 118.1119689941 | test_loss: 6.2007560730 | \n",
      "Epoch: 1725 | train_loss: 118.1117477417 | test_loss: 6.2007055283 | \n",
      "Epoch: 1726 | train_loss: 118.1115036011 | test_loss: 6.2006530762 | \n",
      "Epoch: 1727 | train_loss: 118.1112518311 | test_loss: 6.2006068230 | \n",
      "Epoch: 1728 | train_loss: 118.1110076904 | test_loss: 6.2005510330 | \n",
      "Epoch: 1729 | train_loss: 118.1108169556 | test_loss: 6.2004976273 | \n",
      "Epoch: 1730 | train_loss: 118.1105880737 | test_loss: 6.2004456520 | \n",
      "Epoch: 1731 | train_loss: 118.1103057861 | test_loss: 6.2003955841 | \n",
      "Epoch: 1732 | train_loss: 118.1101150513 | test_loss: 6.2003369331 | \n",
      "Epoch: 1733 | train_loss: 118.1098709106 | test_loss: 6.2002868652 | \n",
      "Epoch: 1734 | train_loss: 118.1096267700 | test_loss: 6.2002296448 | \n",
      "Epoch: 1735 | train_loss: 118.1093673706 | test_loss: 6.2001733780 | \n",
      "Epoch: 1736 | train_loss: 118.1091613770 | test_loss: 6.2001175880 | \n",
      "Epoch: 1737 | train_loss: 118.1089096069 | test_loss: 6.2000622749 | \n",
      "Epoch: 1738 | train_loss: 118.1087036133 | test_loss: 6.2000107765 | \n",
      "Epoch: 1739 | train_loss: 118.1084671021 | test_loss: 6.1999616623 | \n",
      "Epoch: 1740 | train_loss: 118.1082000732 | test_loss: 6.1999077797 | \n",
      "Epoch: 1741 | train_loss: 118.1079559326 | test_loss: 6.1998500824 | \n",
      "Epoch: 1742 | train_loss: 118.1077575684 | test_loss: 6.1997914314 | \n",
      "Epoch: 1743 | train_loss: 118.1074829102 | test_loss: 6.1997370720 | \n",
      "Epoch: 1744 | train_loss: 118.1072769165 | test_loss: 6.1996827126 | \n",
      "Epoch: 1745 | train_loss: 118.1070098877 | test_loss: 6.1996278763 | \n",
      "Epoch: 1746 | train_loss: 118.1067810059 | test_loss: 6.1995711327 | \n",
      "Epoch: 1747 | train_loss: 118.1065979004 | test_loss: 6.1995244026 | \n",
      "Epoch: 1748 | train_loss: 118.1063385010 | test_loss: 6.1994624138 | \n",
      "Epoch: 1749 | train_loss: 118.1060867310 | test_loss: 6.1994090080 | \n",
      "Epoch: 1750 | train_loss: 118.1058654785 | test_loss: 6.1993641853 | \n",
      "Epoch: 1751 | train_loss: 118.1056671143 | test_loss: 6.1993031502 | \n",
      "Epoch: 1752 | train_loss: 118.1054229736 | test_loss: 6.1992492676 | \n",
      "Epoch: 1753 | train_loss: 118.1052017212 | test_loss: 6.1991968155 | \n",
      "Epoch: 1754 | train_loss: 118.1049728394 | test_loss: 6.1991381645 | \n",
      "Epoch: 1755 | train_loss: 118.1047286987 | test_loss: 6.1990818977 | \n",
      "Epoch: 1756 | train_loss: 118.1044616699 | test_loss: 6.1990361214 | \n",
      "Epoch: 1757 | train_loss: 118.1042480469 | test_loss: 6.1989789009 | \n",
      "Epoch: 1758 | train_loss: 118.1040344238 | test_loss: 6.1989226341 | \n",
      "Epoch: 1759 | train_loss: 118.1037673950 | test_loss: 6.1988692284 | \n",
      "Epoch: 1760 | train_loss: 118.1035537720 | test_loss: 6.1988191605 | \n",
      "Epoch: 1761 | train_loss: 118.1033020020 | test_loss: 6.1987566948 | \n",
      "Epoch: 1762 | train_loss: 118.1030960083 | test_loss: 6.1987061501 | \n",
      "Epoch: 1763 | train_loss: 118.1028213501 | test_loss: 6.1986513138 | \n",
      "Epoch: 1764 | train_loss: 118.1025848389 | test_loss: 6.1985955238 | \n",
      "Epoch: 1765 | train_loss: 118.1023559570 | test_loss: 6.1985411644 | \n",
      "Epoch: 1766 | train_loss: 118.1021423340 | test_loss: 6.1984863281 | \n",
      "Epoch: 1767 | train_loss: 118.1018829346 | test_loss: 6.1984338760 | \n",
      "Epoch: 1768 | train_loss: 118.1016540527 | test_loss: 6.1983780861 | \n",
      "Epoch: 1769 | train_loss: 118.1014480591 | test_loss: 6.1983251572 | \n",
      "Epoch: 1770 | train_loss: 118.1012268066 | test_loss: 6.1982722282 | \n",
      "Epoch: 1771 | train_loss: 118.1009597778 | test_loss: 6.1982145309 | \n",
      "Epoch: 1772 | train_loss: 118.1007156372 | test_loss: 6.1981520653 | \n",
      "Epoch: 1773 | train_loss: 118.1004867554 | test_loss: 6.1980967522 | \n",
      "Epoch: 1774 | train_loss: 118.1003036499 | test_loss: 6.1980390549 | \n",
      "Epoch: 1775 | train_loss: 118.1000518799 | test_loss: 6.1979856491 | \n",
      "Epoch: 1776 | train_loss: 118.0998153687 | test_loss: 6.1979298592 | \n",
      "Epoch: 1777 | train_loss: 118.0995635986 | test_loss: 6.1978721619 | \n",
      "Epoch: 1778 | train_loss: 118.0993270874 | test_loss: 6.1978168488 | \n",
      "Epoch: 1779 | train_loss: 118.0990829468 | test_loss: 6.1977620125 | \n",
      "Epoch: 1780 | train_loss: 118.0988693237 | test_loss: 6.1977086067 | \n",
      "Epoch: 1781 | train_loss: 118.0986099243 | test_loss: 6.1976528168 | \n",
      "Epoch: 1782 | train_loss: 118.0983886719 | test_loss: 6.1975975037 | \n",
      "Epoch: 1783 | train_loss: 118.0981826782 | test_loss: 6.1975402832 | \n",
      "Epoch: 1784 | train_loss: 118.0979309082 | test_loss: 6.1974821091 | \n",
      "Epoch: 1785 | train_loss: 118.0976943970 | test_loss: 6.1974253654 | \n",
      "Epoch: 1786 | train_loss: 118.0974731445 | test_loss: 6.1973738670 | \n",
      "Epoch: 1787 | train_loss: 118.0972442627 | test_loss: 6.1973161697 | \n",
      "Epoch: 1788 | train_loss: 118.0970535278 | test_loss: 6.1972584724 | \n",
      "Epoch: 1789 | train_loss: 118.0968093872 | test_loss: 6.1972026825 | \n",
      "Epoch: 1790 | train_loss: 118.0965728760 | test_loss: 6.1971478462 | \n",
      "Epoch: 1791 | train_loss: 118.0963211060 | test_loss: 6.1970872879 | \n",
      "Epoch: 1792 | train_loss: 118.0960617065 | test_loss: 6.1970372200 | \n",
      "Epoch: 1793 | train_loss: 118.0958404541 | test_loss: 6.1969742775 | \n",
      "Epoch: 1794 | train_loss: 118.0956344604 | test_loss: 6.1969208717 | \n",
      "Epoch: 1795 | train_loss: 118.0953598022 | test_loss: 6.1968626976 | \n",
      "Epoch: 1796 | train_loss: 118.0951461792 | test_loss: 6.1968116760 | \n",
      "Epoch: 1797 | train_loss: 118.0949249268 | test_loss: 6.1967601776 | \n",
      "Epoch: 1798 | train_loss: 118.0946807861 | test_loss: 6.1967053413 | \n",
      "Epoch: 1799 | train_loss: 118.0944824219 | test_loss: 6.1966404915 | \n",
      "Epoch: 1800 | train_loss: 118.0942611694 | test_loss: 6.1965827942 | \n",
      "Epoch: 1801 | train_loss: 118.0940322876 | test_loss: 6.1965317726 | \n",
      "Epoch: 1802 | train_loss: 118.0937728882 | test_loss: 6.1964735985 | \n",
      "Epoch: 1803 | train_loss: 118.0935745239 | test_loss: 6.1964221001 | \n",
      "Epoch: 1804 | train_loss: 118.0933303833 | test_loss: 6.1963629723 | \n",
      "Epoch: 1805 | train_loss: 118.0930862427 | test_loss: 6.1963086128 | \n",
      "Epoch: 1806 | train_loss: 118.0928115845 | test_loss: 6.1962504387 | \n",
      "Epoch: 1807 | train_loss: 118.0926589966 | test_loss: 6.1961960793 | \n",
      "Epoch: 1808 | train_loss: 118.0923995972 | test_loss: 6.1961369514 | \n",
      "Epoch: 1809 | train_loss: 118.0921707153 | test_loss: 6.1960778236 | \n",
      "Epoch: 1810 | train_loss: 118.0919342041 | test_loss: 6.1960206032 | \n",
      "Epoch: 1811 | train_loss: 118.0917053223 | test_loss: 6.1959600449 | \n",
      "Epoch: 1812 | train_loss: 118.0914535522 | test_loss: 6.1959075928 | \n",
      "Epoch: 1813 | train_loss: 118.0912170410 | test_loss: 6.1958489418 | \n",
      "Epoch: 1814 | train_loss: 118.0909576416 | test_loss: 6.1957945824 | \n",
      "Epoch: 1815 | train_loss: 118.0907745361 | test_loss: 6.1957292557 | \n",
      "Epoch: 1816 | train_loss: 118.0905151367 | test_loss: 6.1956739426 | \n",
      "Epoch: 1817 | train_loss: 118.0902862549 | test_loss: 6.1956205368 | \n",
      "Epoch: 1818 | train_loss: 118.0900802612 | test_loss: 6.1955604553 | \n",
      "Epoch: 1819 | train_loss: 118.0898361206 | test_loss: 6.1955046654 | \n",
      "Epoch: 1820 | train_loss: 118.0896224976 | test_loss: 6.1954455376 | \n",
      "Epoch: 1821 | train_loss: 118.0893783569 | test_loss: 6.1953926086 | \n",
      "Epoch: 1822 | train_loss: 118.0891723633 | test_loss: 6.1953363419 | \n",
      "Epoch: 1823 | train_loss: 118.0889129639 | test_loss: 6.1952762604 | \n",
      "Epoch: 1824 | train_loss: 118.0886840820 | test_loss: 6.1952204704 | \n",
      "Epoch: 1825 | train_loss: 118.0884552002 | test_loss: 6.1951665878 | \n",
      "Epoch: 1826 | train_loss: 118.0882263184 | test_loss: 6.1951112747 | \n",
      "Epoch: 1827 | train_loss: 118.0880279541 | test_loss: 6.1950554848 | \n",
      "Epoch: 1828 | train_loss: 118.0877990723 | test_loss: 6.1949939728 | \n",
      "Epoch: 1829 | train_loss: 118.0875549316 | test_loss: 6.1949334145 | \n",
      "Epoch: 1830 | train_loss: 118.0873413086 | test_loss: 6.1948804855 | \n",
      "Epoch: 1831 | train_loss: 118.0871124268 | test_loss: 6.1948204041 | \n",
      "Epoch: 1832 | train_loss: 118.0868759155 | test_loss: 6.1947646141 | \n",
      "Epoch: 1833 | train_loss: 118.0866394043 | test_loss: 6.1947002411 | \n",
      "Epoch: 1834 | train_loss: 118.0863952637 | test_loss: 6.1946434975 | \n",
      "Epoch: 1835 | train_loss: 118.0861816406 | test_loss: 6.1945905685 | \n",
      "Epoch: 1836 | train_loss: 118.0859603882 | test_loss: 6.1945338249 | \n",
      "Epoch: 1837 | train_loss: 118.0857315063 | test_loss: 6.1944789886 | \n",
      "Epoch: 1838 | train_loss: 118.0854492188 | test_loss: 6.1944160461 | \n",
      "Epoch: 1839 | train_loss: 118.0852432251 | test_loss: 6.1943597794 | \n",
      "Epoch: 1840 | train_loss: 118.0850067139 | test_loss: 6.1943016052 | \n",
      "Epoch: 1841 | train_loss: 118.0847854614 | test_loss: 6.1942439079 | \n",
      "Epoch: 1842 | train_loss: 118.0845489502 | test_loss: 6.1941857338 | \n",
      "Epoch: 1843 | train_loss: 118.0843200684 | test_loss: 6.1941251755 | \n",
      "Epoch: 1844 | train_loss: 118.0840530396 | test_loss: 6.1940684319 | \n",
      "Epoch: 1845 | train_loss: 118.0838470459 | test_loss: 6.1940088272 | \n",
      "Epoch: 1846 | train_loss: 118.0835952759 | test_loss: 6.1939573288 | \n",
      "Epoch: 1847 | train_loss: 118.0833587646 | test_loss: 6.1939015388 | \n",
      "Epoch: 1848 | train_loss: 118.0831527710 | test_loss: 6.1938414574 | \n",
      "Epoch: 1849 | train_loss: 118.0829162598 | test_loss: 6.1937842369 | \n",
      "Epoch: 1850 | train_loss: 118.0826950073 | test_loss: 6.1937217712 | \n",
      "Epoch: 1851 | train_loss: 118.0824508667 | test_loss: 6.1936602592 | \n",
      "Epoch: 1852 | train_loss: 118.0822067261 | test_loss: 6.1936016083 | \n",
      "Epoch: 1853 | train_loss: 118.0820083618 | test_loss: 6.1935443878 | \n",
      "Epoch: 1854 | train_loss: 118.0817565918 | test_loss: 6.1934862137 | \n",
      "Epoch: 1855 | train_loss: 118.0815124512 | test_loss: 6.1934266090 | \n",
      "Epoch: 1856 | train_loss: 118.0813369751 | test_loss: 6.1933708191 | \n",
      "Epoch: 1857 | train_loss: 118.0810852051 | test_loss: 6.1933150291 | \n",
      "Epoch: 1858 | train_loss: 118.0808792114 | test_loss: 6.1932592392 | \n",
      "Epoch: 1859 | train_loss: 118.0805969238 | test_loss: 6.1931939125 | \n",
      "Epoch: 1860 | train_loss: 118.0803833008 | test_loss: 6.1931390762 | \n",
      "Epoch: 1861 | train_loss: 118.0801773071 | test_loss: 6.1930785179 | \n",
      "Epoch: 1862 | train_loss: 118.0799255371 | test_loss: 6.1930208206 | \n",
      "Epoch: 1863 | train_loss: 118.0797195435 | test_loss: 6.1929621696 | \n",
      "Epoch: 1864 | train_loss: 118.0794754028 | test_loss: 6.1928954124 | \n",
      "Epoch: 1865 | train_loss: 118.0792694092 | test_loss: 6.1928420067 | \n",
      "Epoch: 1866 | train_loss: 118.0790328979 | test_loss: 6.1927866936 | \n",
      "Epoch: 1867 | train_loss: 118.0788192749 | test_loss: 6.1927247047 | \n",
      "Epoch: 1868 | train_loss: 118.0785751343 | test_loss: 6.1926617622 | \n",
      "Epoch: 1869 | train_loss: 118.0783386230 | test_loss: 6.1926054955 | \n",
      "Epoch: 1870 | train_loss: 118.0780944824 | test_loss: 6.1925544739 | \n",
      "Epoch: 1871 | train_loss: 118.0778884888 | test_loss: 6.1924915314 | \n",
      "Epoch: 1872 | train_loss: 118.0776290894 | test_loss: 6.1924343109 | \n",
      "Epoch: 1873 | train_loss: 118.0774536133 | test_loss: 6.1923766136 | \n",
      "Epoch: 1874 | train_loss: 118.0771560669 | test_loss: 6.1923103333 | \n",
      "Epoch: 1875 | train_loss: 118.0769729614 | test_loss: 6.1922512054 | \n",
      "Epoch: 1876 | train_loss: 118.0767364502 | test_loss: 6.1922011375 | \n",
      "Epoch: 1877 | train_loss: 118.0765228271 | test_loss: 6.1921415329 | \n",
      "Epoch: 1878 | train_loss: 118.0763015747 | test_loss: 6.1920833588 | \n",
      "Epoch: 1879 | train_loss: 118.0761032104 | test_loss: 6.1920166016 | \n",
      "Epoch: 1880 | train_loss: 118.0758285522 | test_loss: 6.1919574738 | \n",
      "Epoch: 1881 | train_loss: 118.0756378174 | test_loss: 6.1918964386 | \n",
      "Epoch: 1882 | train_loss: 118.0753784180 | test_loss: 6.1918444633 | \n",
      "Epoch: 1883 | train_loss: 118.0751953125 | test_loss: 6.1917881966 | \n",
      "Epoch: 1884 | train_loss: 118.0749206543 | test_loss: 6.1917266846 | \n",
      "Epoch: 1885 | train_loss: 118.0747070312 | test_loss: 6.1916632652 | \n",
      "Epoch: 1886 | train_loss: 118.0745086670 | test_loss: 6.1916074753 | \n",
      "Epoch: 1887 | train_loss: 118.0742645264 | test_loss: 6.1915516853 | \n",
      "Epoch: 1888 | train_loss: 118.0740432739 | test_loss: 6.1914892197 | \n",
      "Epoch: 1889 | train_loss: 118.0738220215 | test_loss: 6.1914277077 | \n",
      "Epoch: 1890 | train_loss: 118.0736007690 | test_loss: 6.1913671494 | \n",
      "Epoch: 1891 | train_loss: 118.0733566284 | test_loss: 6.1913118362 | \n",
      "Epoch: 1892 | train_loss: 118.0730972290 | test_loss: 6.1912488937 | \n",
      "Epoch: 1893 | train_loss: 118.0728912354 | test_loss: 6.1911835670 | \n",
      "Epoch: 1894 | train_loss: 118.0726547241 | test_loss: 6.1911249161 | \n",
      "Epoch: 1895 | train_loss: 118.0724487305 | test_loss: 6.1910648346 | \n",
      "Epoch: 1896 | train_loss: 118.0721969604 | test_loss: 6.1910076141 | \n",
      "Epoch: 1897 | train_loss: 118.0719909668 | test_loss: 6.1909513474 | \n",
      "Epoch: 1898 | train_loss: 118.0717468262 | test_loss: 6.1908879280 | \n",
      "Epoch: 1899 | train_loss: 118.0715408325 | test_loss: 6.1908249855 | \n",
      "Epoch: 1900 | train_loss: 118.0712966919 | test_loss: 6.1907649040 | \n",
      "Epoch: 1901 | train_loss: 118.0710906982 | test_loss: 6.1907048225 | \n",
      "Epoch: 1902 | train_loss: 118.0708618164 | test_loss: 6.1906495094 | \n",
      "Epoch: 1903 | train_loss: 118.0706329346 | test_loss: 6.1905970573 | \n",
      "Epoch: 1904 | train_loss: 118.0704116821 | test_loss: 6.1905307770 | \n",
      "Epoch: 1905 | train_loss: 118.0701828003 | test_loss: 6.1904673576 | \n",
      "Epoch: 1906 | train_loss: 118.0699920654 | test_loss: 6.1904106140 | \n",
      "Epoch: 1907 | train_loss: 118.0697326660 | test_loss: 6.1903491020 | \n",
      "Epoch: 1908 | train_loss: 118.0695190430 | test_loss: 6.1902880669 | \n",
      "Epoch: 1909 | train_loss: 118.0692977905 | test_loss: 6.1902298927 | \n",
      "Epoch: 1910 | train_loss: 118.0690841675 | test_loss: 6.1901726723 | \n",
      "Epoch: 1911 | train_loss: 118.0688476562 | test_loss: 6.1901073456 | \n",
      "Epoch: 1912 | train_loss: 118.0686035156 | test_loss: 6.1900477409 | \n",
      "Epoch: 1913 | train_loss: 118.0683822632 | test_loss: 6.1899914742 | \n",
      "Epoch: 1914 | train_loss: 118.0681533813 | test_loss: 6.1899290085 | \n",
      "Epoch: 1915 | train_loss: 118.0679321289 | test_loss: 6.1898775101 | \n",
      "Epoch: 1916 | train_loss: 118.0677261353 | test_loss: 6.1898126602 | \n",
      "Epoch: 1917 | train_loss: 118.0674819946 | test_loss: 6.1897492409 | \n",
      "Epoch: 1918 | train_loss: 118.0672760010 | test_loss: 6.1896839142 | \n",
      "Epoch: 1919 | train_loss: 118.0670471191 | test_loss: 6.1896224022 | \n",
      "Epoch: 1920 | train_loss: 118.0667877197 | test_loss: 6.1895723343 | \n",
      "Epoch: 1921 | train_loss: 118.0665817261 | test_loss: 6.1895122528 | \n",
      "Epoch: 1922 | train_loss: 118.0663604736 | test_loss: 6.1894516945 | \n",
      "Epoch: 1923 | train_loss: 118.0661392212 | test_loss: 6.1893906593 | \n",
      "Epoch: 1924 | train_loss: 118.0659484863 | test_loss: 6.1893286705 | \n",
      "Epoch: 1925 | train_loss: 118.0656967163 | test_loss: 6.1892676353 | \n",
      "Epoch: 1926 | train_loss: 118.0654373169 | test_loss: 6.1892108917 | \n",
      "Epoch: 1927 | train_loss: 118.0652542114 | test_loss: 6.1891479492 | \n",
      "Epoch: 1928 | train_loss: 118.0650329590 | test_loss: 6.1890873909 | \n",
      "Epoch: 1929 | train_loss: 118.0647735596 | test_loss: 6.1890316010 | \n",
      "Epoch: 1930 | train_loss: 118.0645294189 | test_loss: 6.1889691353 | \n",
      "Epoch: 1931 | train_loss: 118.0643081665 | test_loss: 6.1889076233 | \n",
      "Epoch: 1932 | train_loss: 118.0641021729 | test_loss: 6.1888408661 | \n",
      "Epoch: 1933 | train_loss: 118.0638885498 | test_loss: 6.1887841225 | \n",
      "Epoch: 1934 | train_loss: 118.0636825562 | test_loss: 6.1887264252 | \n",
      "Epoch: 1935 | train_loss: 118.0634307861 | test_loss: 6.1886644363 | \n",
      "Epoch: 1936 | train_loss: 118.0632019043 | test_loss: 6.1886014938 | \n",
      "Epoch: 1937 | train_loss: 118.0629425049 | test_loss: 6.1885423660 | \n",
      "Epoch: 1938 | train_loss: 118.0627670288 | test_loss: 6.1884875298 | \n",
      "Epoch: 1939 | train_loss: 118.0625381470 | test_loss: 6.1884307861 | \n",
      "Epoch: 1940 | train_loss: 118.0623245239 | test_loss: 6.1883659363 | \n",
      "Epoch: 1941 | train_loss: 118.0621032715 | test_loss: 6.1883068085 | \n",
      "Epoch: 1942 | train_loss: 118.0618972778 | test_loss: 6.1882448196 | \n",
      "Epoch: 1943 | train_loss: 118.0616531372 | test_loss: 6.1881799698 | \n",
      "Epoch: 1944 | train_loss: 118.0614471436 | test_loss: 6.1881160736 | \n",
      "Epoch: 1945 | train_loss: 118.0612030029 | test_loss: 6.1880521774 | \n",
      "Epoch: 1946 | train_loss: 118.0609970093 | test_loss: 6.1879968643 | \n",
      "Epoch: 1947 | train_loss: 118.0608139038 | test_loss: 6.1879367828 | \n",
      "Epoch: 1948 | train_loss: 118.0605239868 | test_loss: 6.1878771782 | \n",
      "Epoch: 1949 | train_loss: 118.0603256226 | test_loss: 6.1878142357 | \n",
      "Epoch: 1950 | train_loss: 118.0601196289 | test_loss: 6.1877512932 | \n",
      "Epoch: 1951 | train_loss: 118.0598831177 | test_loss: 6.1876945496 | \n",
      "Epoch: 1952 | train_loss: 118.0596771240 | test_loss: 6.1876330376 | \n",
      "Epoch: 1953 | train_loss: 118.0593872070 | test_loss: 6.1875734329 | \n",
      "Epoch: 1954 | train_loss: 118.0591735840 | test_loss: 6.1875133514 | \n",
      "Epoch: 1955 | train_loss: 118.0589904785 | test_loss: 6.1874518394 | \n",
      "Epoch: 1956 | train_loss: 118.0587615967 | test_loss: 6.1873874664 | \n",
      "Epoch: 1957 | train_loss: 118.0585174561 | test_loss: 6.1873345375 | \n",
      "Epoch: 1958 | train_loss: 118.0583114624 | test_loss: 6.1872706413 | \n",
      "Epoch: 1959 | train_loss: 118.0580825806 | test_loss: 6.1872100830 | \n",
      "Epoch: 1960 | train_loss: 118.0578384399 | test_loss: 6.1871466637 | \n",
      "Epoch: 1961 | train_loss: 118.0576400757 | test_loss: 6.1870856285 | \n",
      "Epoch: 1962 | train_loss: 118.0574188232 | test_loss: 6.1870312691 | \n",
      "Epoch: 1963 | train_loss: 118.0572204590 | test_loss: 6.1869678497 | \n",
      "Epoch: 1964 | train_loss: 118.0569610596 | test_loss: 6.1869049072 | \n",
      "Epoch: 1965 | train_loss: 118.0567703247 | test_loss: 6.1868453026 | \n",
      "Epoch: 1966 | train_loss: 118.0565109253 | test_loss: 6.1867828369 | \n",
      "Epoch: 1967 | train_loss: 118.0562820435 | test_loss: 6.1867179871 | \n",
      "Epoch: 1968 | train_loss: 118.0560531616 | test_loss: 6.1866583824 | \n",
      "Epoch: 1969 | train_loss: 118.0558395386 | test_loss: 6.1866011620 | \n",
      "Epoch: 1970 | train_loss: 118.0556259155 | test_loss: 6.1865386963 | \n",
      "Epoch: 1971 | train_loss: 118.0554199219 | test_loss: 6.1864738464 | \n",
      "Epoch: 1972 | train_loss: 118.0551986694 | test_loss: 6.1864123344 | \n",
      "Epoch: 1973 | train_loss: 118.0549545288 | test_loss: 6.1863512993 | \n",
      "Epoch: 1974 | train_loss: 118.0547256470 | test_loss: 6.1862931252 | \n",
      "Epoch: 1975 | train_loss: 118.0545120239 | test_loss: 6.1862287521 | \n",
      "Epoch: 1976 | train_loss: 118.0543060303 | test_loss: 6.1861662865 | \n",
      "Epoch: 1977 | train_loss: 118.0540924072 | test_loss: 6.1861023903 | \n",
      "Epoch: 1978 | train_loss: 118.0538253784 | test_loss: 6.1860418320 | \n",
      "Epoch: 1979 | train_loss: 118.0536117554 | test_loss: 6.1859803200 | \n",
      "Epoch: 1980 | train_loss: 118.0534210205 | test_loss: 6.1859235764 | \n",
      "Epoch: 1981 | train_loss: 118.0532226562 | test_loss: 6.1858563423 | \n",
      "Epoch: 1982 | train_loss: 118.0529785156 | test_loss: 6.1857972145 | \n",
      "Epoch: 1983 | train_loss: 118.0527267456 | test_loss: 6.1857299805 | \n",
      "Epoch: 1984 | train_loss: 118.0524902344 | test_loss: 6.1856718063 | \n",
      "Epoch: 1985 | train_loss: 118.0523147583 | test_loss: 6.1856164932 | \n",
      "Epoch: 1986 | train_loss: 118.0520706177 | test_loss: 6.1855592728 | \n",
      "Epoch: 1987 | train_loss: 118.0518722534 | test_loss: 6.1854939461 | \n",
      "Epoch: 1988 | train_loss: 118.0516052246 | test_loss: 6.1854314804 | \n",
      "Epoch: 1989 | train_loss: 118.0514144897 | test_loss: 6.1853675842 | \n",
      "Epoch: 1990 | train_loss: 118.0511932373 | test_loss: 6.1853055954 | \n",
      "Epoch: 1991 | train_loss: 118.0509567261 | test_loss: 6.1852416992 | \n",
      "Epoch: 1992 | train_loss: 118.0507431030 | test_loss: 6.1851830482 | \n",
      "Epoch: 1993 | train_loss: 118.0505142212 | test_loss: 6.1851263046 | \n",
      "Epoch: 1994 | train_loss: 118.0503311157 | test_loss: 6.1850667000 | \n",
      "Epoch: 1995 | train_loss: 118.0500946045 | test_loss: 6.1850061417 | \n",
      "Epoch: 1996 | train_loss: 118.0498733521 | test_loss: 6.1849470139 | \n",
      "Epoch: 1997 | train_loss: 118.0496368408 | test_loss: 6.1848778725 | \n",
      "Epoch: 1998 | train_loss: 118.0494232178 | test_loss: 6.1848173141 | \n",
      "Epoch: 1999 | train_loss: 118.0492019653 | test_loss: 6.1847548485 | \n",
      "Epoch: 2000 | train_loss: 118.0489654541 | test_loss: 6.1846899986 | \n",
      "Epoch: 2001 | train_loss: 118.0487899780 | test_loss: 6.1846356392 | \n",
      "Epoch: 2002 | train_loss: 118.0485534668 | test_loss: 6.1845717430 | \n",
      "Epoch: 2003 | train_loss: 118.0483398438 | test_loss: 6.1845159531 | \n",
      "Epoch: 2004 | train_loss: 118.0480651855 | test_loss: 6.1844506264 | \n",
      "Epoch: 2005 | train_loss: 118.0478286743 | test_loss: 6.1843872070 | \n",
      "Epoch: 2006 | train_loss: 118.0476379395 | test_loss: 6.1843247414 | \n",
      "Epoch: 2007 | train_loss: 118.0474014282 | test_loss: 6.1842603683 | \n",
      "Epoch: 2008 | train_loss: 118.0472183228 | test_loss: 6.1841993332 | \n",
      "Epoch: 2009 | train_loss: 118.0469512939 | test_loss: 6.1841382980 | \n",
      "Epoch: 2010 | train_loss: 118.0467453003 | test_loss: 6.1840763092 | \n",
      "Epoch: 2011 | train_loss: 118.0465240479 | test_loss: 6.1840181351 | \n",
      "Epoch: 2012 | train_loss: 118.0462875366 | test_loss: 6.1839594841 | \n",
      "Epoch: 2013 | train_loss: 118.0460968018 | test_loss: 6.1838994026 | \n",
      "Epoch: 2014 | train_loss: 118.0459060669 | test_loss: 6.1838378906 | \n",
      "Epoch: 2015 | train_loss: 118.0456924438 | test_loss: 6.1837739944 | \n",
      "Epoch: 2016 | train_loss: 118.0454559326 | test_loss: 6.1837201118 | \n",
      "Epoch: 2017 | train_loss: 118.0452270508 | test_loss: 6.1836476326 | \n",
      "Epoch: 2018 | train_loss: 118.0450210571 | test_loss: 6.1835913658 | \n",
      "Epoch: 2019 | train_loss: 118.0447921753 | test_loss: 6.1835312843 | \n",
      "Epoch: 2020 | train_loss: 118.0445785522 | test_loss: 6.1834630966 | \n",
      "Epoch: 2021 | train_loss: 118.0443572998 | test_loss: 6.1834001541 | \n",
      "Epoch: 2022 | train_loss: 118.0441513062 | test_loss: 6.1833438873 | \n",
      "Epoch: 2023 | train_loss: 118.0439453125 | test_loss: 6.1832733154 | \n",
      "Epoch: 2024 | train_loss: 118.0437011719 | test_loss: 6.1832160950 | \n",
      "Epoch: 2025 | train_loss: 118.0434646606 | test_loss: 6.1831536293 | \n",
      "Epoch: 2026 | train_loss: 118.0432357788 | test_loss: 6.1830925941 | \n",
      "Epoch: 2027 | train_loss: 118.0430068970 | test_loss: 6.1830234528 | \n",
      "Epoch: 2028 | train_loss: 118.0427932739 | test_loss: 6.1829633713 | \n",
      "Epoch: 2029 | train_loss: 118.0425720215 | test_loss: 6.1829075813 | \n",
      "Epoch: 2030 | train_loss: 118.0423507690 | test_loss: 6.1828417778 | \n",
      "Epoch: 2031 | train_loss: 118.0421447754 | test_loss: 6.1827831268 | \n",
      "Epoch: 2032 | train_loss: 118.0419311523 | test_loss: 6.1827254295 | \n",
      "Epoch: 2033 | train_loss: 118.0417098999 | test_loss: 6.1826634407 | \n",
      "Epoch: 2034 | train_loss: 118.0414428711 | test_loss: 6.1826000214 | \n",
      "Epoch: 2035 | train_loss: 118.0412902832 | test_loss: 6.1825432777 | \n",
      "Epoch: 2036 | train_loss: 118.0410842896 | test_loss: 6.1824822426 | \n",
      "Epoch: 2037 | train_loss: 118.0408630371 | test_loss: 6.1824212074 | \n",
      "Epoch: 2038 | train_loss: 118.0406036377 | test_loss: 6.1823582649 | \n",
      "Epoch: 2039 | train_loss: 118.0403747559 | test_loss: 6.1823010445 | \n",
      "Epoch: 2040 | train_loss: 118.0402069092 | test_loss: 6.1822381020 | \n",
      "Epoch: 2041 | train_loss: 118.0399703979 | test_loss: 6.1821732521 | \n",
      "Epoch: 2042 | train_loss: 118.0397262573 | test_loss: 6.1821141243 | \n",
      "Epoch: 2043 | train_loss: 118.0395431519 | test_loss: 6.1820507050 | \n",
      "Epoch: 2044 | train_loss: 118.0392837524 | test_loss: 6.1819872856 | \n",
      "Epoch: 2045 | train_loss: 118.0390701294 | test_loss: 6.1819286346 | \n",
      "Epoch: 2046 | train_loss: 118.0388488770 | test_loss: 6.1818675995 | \n",
      "Epoch: 2047 | train_loss: 118.0386657715 | test_loss: 6.1818079948 | \n",
      "Epoch: 2048 | train_loss: 118.0384292603 | test_loss: 6.1817474365 | \n",
      "Epoch: 2049 | train_loss: 118.0381851196 | test_loss: 6.1816773415 | \n",
      "Epoch: 2050 | train_loss: 118.0379714966 | test_loss: 6.1816167831 | \n",
      "Epoch: 2051 | train_loss: 118.0377655029 | test_loss: 6.1815581322 | \n",
      "Epoch: 2052 | train_loss: 118.0375061035 | test_loss: 6.1814942360 | \n",
      "Epoch: 2053 | train_loss: 118.0372924805 | test_loss: 6.1814374924 | \n",
      "Epoch: 2054 | train_loss: 118.0370788574 | test_loss: 6.1813683510 | \n",
      "Epoch: 2055 | train_loss: 118.0368957520 | test_loss: 6.1813139915 | \n",
      "Epoch: 2056 | train_loss: 118.0366516113 | test_loss: 6.1812534332 | \n",
      "Epoch: 2057 | train_loss: 118.0364761353 | test_loss: 6.1811885834 | \n",
      "Epoch: 2058 | train_loss: 118.0362319946 | test_loss: 6.1811323166 | \n",
      "Epoch: 2059 | train_loss: 118.0360336304 | test_loss: 6.1810712814 | \n",
      "Epoch: 2060 | train_loss: 118.0358123779 | test_loss: 6.1810064316 | \n",
      "Epoch: 2061 | train_loss: 118.0355987549 | test_loss: 6.1809434891 | \n",
      "Epoch: 2062 | train_loss: 118.0353317261 | test_loss: 6.1808815002 | \n",
      "Epoch: 2063 | train_loss: 118.0351486206 | test_loss: 6.1808166504 | \n",
      "Epoch: 2064 | train_loss: 118.0348892212 | test_loss: 6.1807579994 | \n",
      "Epoch: 2065 | train_loss: 118.0347061157 | test_loss: 6.1806921959 | \n",
      "Epoch: 2066 | train_loss: 118.0344314575 | test_loss: 6.1806373596 | \n",
      "Epoch: 2067 | train_loss: 118.0342712402 | test_loss: 6.1805772781 | \n",
      "Epoch: 2068 | train_loss: 118.0340270996 | test_loss: 6.1805138588 | \n",
      "Epoch: 2069 | train_loss: 118.0338287354 | test_loss: 6.1804499626 | \n",
      "Epoch: 2070 | train_loss: 118.0335845947 | test_loss: 6.1803870201 | \n",
      "Epoch: 2071 | train_loss: 118.0333862305 | test_loss: 6.1803283691 | \n",
      "Epoch: 2072 | train_loss: 118.0331954956 | test_loss: 6.1802721024 | \n",
      "Epoch: 2073 | train_loss: 118.0329513550 | test_loss: 6.1802077293 | \n",
      "Epoch: 2074 | train_loss: 118.0327072144 | test_loss: 6.1801395416 | \n",
      "Epoch: 2075 | train_loss: 118.0324707031 | test_loss: 6.1800799370 | \n",
      "Epoch: 2076 | train_loss: 118.0322952271 | test_loss: 6.1800146103 | \n",
      "Epoch: 2077 | train_loss: 118.0320968628 | test_loss: 6.1799550056 | \n",
      "Epoch: 2078 | train_loss: 118.0318374634 | test_loss: 6.1798939705 | \n",
      "Epoch: 2079 | train_loss: 118.0316085815 | test_loss: 6.1798319817 | \n",
      "Epoch: 2080 | train_loss: 118.0314178467 | test_loss: 6.1797685623 | \n",
      "Epoch: 2081 | train_loss: 118.0311813354 | test_loss: 6.1797022820 | \n",
      "Epoch: 2082 | train_loss: 118.0309448242 | test_loss: 6.1796488762 | \n",
      "Epoch: 2083 | train_loss: 118.0307846069 | test_loss: 6.1795935631 | \n",
      "Epoch: 2084 | train_loss: 118.0305786133 | test_loss: 6.1795310974 | \n",
      "Epoch: 2085 | train_loss: 118.0303421021 | test_loss: 6.1794691086 | \n",
      "Epoch: 2086 | train_loss: 118.0300750732 | test_loss: 6.1794004440 | \n",
      "Epoch: 2087 | train_loss: 118.0299148560 | test_loss: 6.1793479919 | \n",
      "Epoch: 2088 | train_loss: 118.0296859741 | test_loss: 6.1792817116 | \n",
      "Epoch: 2089 | train_loss: 118.0294494629 | test_loss: 6.1792268753 | \n",
      "Epoch: 2090 | train_loss: 118.0292587280 | test_loss: 6.1791615486 | \n",
      "Epoch: 2091 | train_loss: 118.0290145874 | test_loss: 6.1791076660 | \n",
      "Epoch: 2092 | train_loss: 118.0287780762 | test_loss: 6.1790437698 | \n",
      "Epoch: 2093 | train_loss: 118.0285720825 | test_loss: 6.1789793968 | \n",
      "Epoch: 2094 | train_loss: 118.0283584595 | test_loss: 6.1789178848 | \n",
      "Epoch: 2095 | train_loss: 118.0281448364 | test_loss: 6.1788549423 | \n",
      "Epoch: 2096 | train_loss: 118.0279006958 | test_loss: 6.1787934303 | \n",
      "Epoch: 2097 | train_loss: 118.0276870728 | test_loss: 6.1787319183 | \n",
      "Epoch: 2098 | train_loss: 118.0274505615 | test_loss: 6.1786670685 | \n",
      "Epoch: 2099 | train_loss: 118.0272521973 | test_loss: 6.1786060333 | \n",
      "Epoch: 2100 | train_loss: 118.0270309448 | test_loss: 6.1785454750 | \n",
      "Epoch: 2101 | train_loss: 118.0268478394 | test_loss: 6.1784892082 | \n",
      "Epoch: 2102 | train_loss: 118.0265884399 | test_loss: 6.1784257889 | \n",
      "Epoch: 2103 | train_loss: 118.0263748169 | test_loss: 6.1783618927 | \n",
      "Epoch: 2104 | train_loss: 118.0261230469 | test_loss: 6.1783046722 | \n",
      "Epoch: 2105 | train_loss: 118.0259246826 | test_loss: 6.1782445908 | \n",
      "Epoch: 2106 | train_loss: 118.0257034302 | test_loss: 6.1781816483 | \n",
      "Epoch: 2107 | train_loss: 118.0254669189 | test_loss: 6.1781229973 | \n",
      "Epoch: 2108 | train_loss: 118.0252761841 | test_loss: 6.1780576706 | \n",
      "Epoch: 2109 | train_loss: 118.0250854492 | test_loss: 6.1779956818 | \n",
      "Epoch: 2110 | train_loss: 118.0248336792 | test_loss: 6.1779365540 | \n",
      "Epoch: 2111 | train_loss: 118.0246124268 | test_loss: 6.1778755188 | \n",
      "Epoch: 2112 | train_loss: 118.0243377686 | test_loss: 6.1778111458 | \n",
      "Epoch: 2113 | train_loss: 118.0241775513 | test_loss: 6.1777496338 | \n",
      "Epoch: 2114 | train_loss: 118.0239486694 | test_loss: 6.1776885986 | \n",
      "Epoch: 2115 | train_loss: 118.0237045288 | test_loss: 6.1776280403 | \n",
      "Epoch: 2116 | train_loss: 118.0235137939 | test_loss: 6.1775684357 | \n",
      "Epoch: 2117 | train_loss: 118.0233001709 | test_loss: 6.1775078773 | \n",
      "Epoch: 2118 | train_loss: 118.0230484009 | test_loss: 6.1774482727 | \n",
      "Epoch: 2119 | train_loss: 118.0228576660 | test_loss: 6.1773877144 | \n",
      "Epoch: 2120 | train_loss: 118.0226593018 | test_loss: 6.1773304939 | \n",
      "Epoch: 2121 | train_loss: 118.0224609375 | test_loss: 6.1772685051 | \n",
      "Epoch: 2122 | train_loss: 118.0221862793 | test_loss: 6.1772089005 | \n",
      "Epoch: 2123 | train_loss: 118.0219802856 | test_loss: 6.1771464348 | \n",
      "Epoch: 2124 | train_loss: 118.0217895508 | test_loss: 6.1770853996 | \n",
      "Epoch: 2125 | train_loss: 118.0215530396 | test_loss: 6.1770243645 | \n",
      "Epoch: 2126 | train_loss: 118.0213546753 | test_loss: 6.1769628525 | \n",
      "Epoch: 2127 | train_loss: 118.0211105347 | test_loss: 6.1769008636 | \n",
      "Epoch: 2128 | train_loss: 118.0208892822 | test_loss: 6.1768465042 | \n",
      "Epoch: 2129 | train_loss: 118.0206604004 | test_loss: 6.1767787933 | \n",
      "Epoch: 2130 | train_loss: 118.0204162598 | test_loss: 6.1767215729 | \n",
      "Epoch: 2131 | train_loss: 118.0202255249 | test_loss: 6.1766533852 | \n",
      "Epoch: 2132 | train_loss: 118.0199966431 | test_loss: 6.1765942574 | \n",
      "Epoch: 2133 | train_loss: 118.0197677612 | test_loss: 6.1765327454 | \n",
      "Epoch: 2134 | train_loss: 118.0195770264 | test_loss: 6.1764745712 | \n",
      "Epoch: 2135 | train_loss: 118.0193176270 | test_loss: 6.1764130592 | \n",
      "Epoch: 2136 | train_loss: 118.0191421509 | test_loss: 6.1763615608 | \n",
      "Epoch: 2137 | train_loss: 118.0189132690 | test_loss: 6.1763005257 | \n",
      "Epoch: 2138 | train_loss: 118.0187301636 | test_loss: 6.1762318611 | \n",
      "Epoch: 2139 | train_loss: 118.0184860229 | test_loss: 6.1761751175 | \n",
      "Epoch: 2140 | train_loss: 118.0182723999 | test_loss: 6.1761126518 | \n",
      "Epoch: 2141 | train_loss: 118.0180358887 | test_loss: 6.1760544777 | \n",
      "Epoch: 2142 | train_loss: 118.0178451538 | test_loss: 6.1759977341 | \n",
      "Epoch: 2143 | train_loss: 118.0176391602 | test_loss: 6.1759395599 | \n",
      "Epoch: 2144 | train_loss: 118.0174026489 | test_loss: 6.1758785248 | \n",
      "Epoch: 2145 | train_loss: 118.0171661377 | test_loss: 6.1758179665 | \n",
      "Epoch: 2146 | train_loss: 118.0169754028 | test_loss: 6.1757588387 | \n",
      "Epoch: 2147 | train_loss: 118.0167846680 | test_loss: 6.1756968498 | \n",
      "Epoch: 2148 | train_loss: 118.0165100098 | test_loss: 6.1756353378 | \n",
      "Epoch: 2149 | train_loss: 118.0163192749 | test_loss: 6.1755771637 | \n",
      "Epoch: 2150 | train_loss: 118.0161056519 | test_loss: 6.1755137444 | \n",
      "Epoch: 2151 | train_loss: 118.0158691406 | test_loss: 6.1754550934 | \n",
      "Epoch: 2152 | train_loss: 118.0156326294 | test_loss: 6.1754002571 | \n",
      "Epoch: 2153 | train_loss: 118.0154266357 | test_loss: 6.1753411293 | \n",
      "Epoch: 2154 | train_loss: 118.0152435303 | test_loss: 6.1752796173 | \n",
      "Epoch: 2155 | train_loss: 118.0150070190 | test_loss: 6.1752181053 | \n",
      "Epoch: 2156 | train_loss: 118.0147933960 | test_loss: 6.1751542091 | \n",
      "Epoch: 2157 | train_loss: 118.0146102905 | test_loss: 6.1750926971 | \n",
      "Epoch: 2158 | train_loss: 118.0143661499 | test_loss: 6.1750364304 | \n",
      "Epoch: 2159 | train_loss: 118.0141830444 | test_loss: 6.1749768257 | \n",
      "Epoch: 2160 | train_loss: 118.0139465332 | test_loss: 6.1749157906 | \n",
      "Epoch: 2161 | train_loss: 118.0137329102 | test_loss: 6.1748561859 | \n",
      "Epoch: 2162 | train_loss: 118.0135269165 | test_loss: 6.1747980118 | \n",
      "Epoch: 2163 | train_loss: 118.0132904053 | test_loss: 6.1747412682 | \n",
      "Epoch: 2164 | train_loss: 118.0130844116 | test_loss: 6.1746778488 | \n",
      "Epoch: 2165 | train_loss: 118.0128631592 | test_loss: 6.1746149063 | \n",
      "Epoch: 2166 | train_loss: 118.0126342773 | test_loss: 6.1745600700 | \n",
      "Epoch: 2167 | train_loss: 118.0124206543 | test_loss: 6.1745033264 | \n",
      "Epoch: 2168 | train_loss: 118.0121841431 | test_loss: 6.1744413376 | \n",
      "Epoch: 2169 | train_loss: 118.0119934082 | test_loss: 6.1743793488 | \n",
      "Epoch: 2170 | train_loss: 118.0117568970 | test_loss: 6.1743206978 | \n",
      "Epoch: 2171 | train_loss: 118.0115661621 | test_loss: 6.1742572784 | \n",
      "Epoch: 2172 | train_loss: 118.0113449097 | test_loss: 6.1741986275 | \n",
      "Epoch: 2173 | train_loss: 118.0111007690 | test_loss: 6.1741328239 | \n",
      "Epoch: 2174 | train_loss: 118.0108489990 | test_loss: 6.1740727425 | \n",
      "Epoch: 2175 | train_loss: 118.0106430054 | test_loss: 6.1740107536 | \n",
      "Epoch: 2176 | train_loss: 118.0103988647 | test_loss: 6.1739525795 | \n",
      "Epoch: 2177 | train_loss: 118.0101623535 | test_loss: 6.1738948822 | \n",
      "Epoch: 2178 | train_loss: 118.0099411011 | test_loss: 6.1738352776 | \n",
      "Epoch: 2179 | train_loss: 118.0097351074 | test_loss: 6.1737775803 | \n",
      "Epoch: 2180 | train_loss: 118.0095214844 | test_loss: 6.1737174988 | \n",
      "Epoch: 2181 | train_loss: 118.0093002319 | test_loss: 6.1736564636 | \n",
      "Epoch: 2182 | train_loss: 118.0091018677 | test_loss: 6.1735935211 | \n",
      "Epoch: 2183 | train_loss: 118.0088653564 | test_loss: 6.1735396385 | \n",
      "Epoch: 2184 | train_loss: 118.0086746216 | test_loss: 6.1734819412 | \n",
      "Epoch: 2185 | train_loss: 118.0084381104 | test_loss: 6.1734199524 | \n",
      "Epoch: 2186 | train_loss: 118.0082244873 | test_loss: 6.1733593941 | \n",
      "Epoch: 2187 | train_loss: 118.0080108643 | test_loss: 6.1733021736 | \n",
      "Epoch: 2188 | train_loss: 118.0077896118 | test_loss: 6.1732459068 | \n",
      "Epoch: 2189 | train_loss: 118.0075988770 | test_loss: 6.1731896400 | \n",
      "Epoch: 2190 | train_loss: 118.0073699951 | test_loss: 6.1731266975 | \n",
      "Epoch: 2191 | train_loss: 118.0071258545 | test_loss: 6.1730709076 | \n",
      "Epoch: 2192 | train_loss: 118.0069503784 | test_loss: 6.1730108261 | \n",
      "Epoch: 2193 | train_loss: 118.0067596436 | test_loss: 6.1729516983 | \n",
      "Epoch: 2194 | train_loss: 118.0065460205 | test_loss: 6.1728901863 | \n",
      "Epoch: 2195 | train_loss: 118.0063018799 | test_loss: 6.1728334427 | \n",
      "Epoch: 2196 | train_loss: 118.0060806274 | test_loss: 6.1727647781 | \n",
      "Epoch: 2197 | train_loss: 118.0058517456 | test_loss: 6.1727099419 | \n",
      "Epoch: 2198 | train_loss: 118.0056457520 | test_loss: 6.1726546288 | \n",
      "Epoch: 2199 | train_loss: 118.0054473877 | test_loss: 6.1726016998 | \n",
      "Epoch: 2200 | train_loss: 118.0052185059 | test_loss: 6.1725401878 | \n",
      "Epoch: 2201 | train_loss: 118.0049896240 | test_loss: 6.1724805832 | \n",
      "Epoch: 2202 | train_loss: 118.0047912598 | test_loss: 6.1724247932 | \n",
      "Epoch: 2203 | train_loss: 118.0045242310 | test_loss: 6.1723570824 | \n",
      "Epoch: 2204 | train_loss: 118.0043334961 | test_loss: 6.1722998619 | \n",
      "Epoch: 2205 | train_loss: 118.0041580200 | test_loss: 6.1722435951 | \n",
      "Epoch: 2206 | train_loss: 118.0039291382 | test_loss: 6.1721849442 | \n",
      "Epoch: 2207 | train_loss: 118.0037231445 | test_loss: 6.1721177101 | \n",
      "Epoch: 2208 | train_loss: 118.0034790039 | test_loss: 6.1720619202 | \n",
      "Epoch: 2209 | train_loss: 118.0032730103 | test_loss: 6.1720080376 | \n",
      "Epoch: 2210 | train_loss: 118.0030593872 | test_loss: 6.1719551086 | \n",
      "Epoch: 2211 | train_loss: 118.0028381348 | test_loss: 6.1718983650 | \n",
      "Epoch: 2212 | train_loss: 118.0026092529 | test_loss: 6.1718397141 | \n",
      "Epoch: 2213 | train_loss: 118.0024032593 | test_loss: 6.1717796326 | \n",
      "Epoch: 2214 | train_loss: 118.0021667480 | test_loss: 6.1717162132 | \n",
      "Epoch: 2215 | train_loss: 118.0019149780 | test_loss: 6.1716632843 | \n",
      "Epoch: 2216 | train_loss: 118.0017318726 | test_loss: 6.1715998650 | \n",
      "Epoch: 2217 | train_loss: 118.0015106201 | test_loss: 6.1715474129 | \n",
      "Epoch: 2218 | train_loss: 118.0012969971 | test_loss: 6.1714935303 | \n",
      "Epoch: 2219 | train_loss: 118.0010757446 | test_loss: 6.1714353561 | \n",
      "Epoch: 2220 | train_loss: 118.0008697510 | test_loss: 6.1713733673 | \n",
      "Epoch: 2221 | train_loss: 118.0006561279 | test_loss: 6.1713175774 | \n",
      "Epoch: 2222 | train_loss: 118.0004577637 | test_loss: 6.1712493896 | \n",
      "Epoch: 2223 | train_loss: 118.0002212524 | test_loss: 6.1711931229 | \n",
      "Epoch: 2224 | train_loss: 118.0000228882 | test_loss: 6.1711301804 | \n",
      "Epoch: 2225 | train_loss: 117.9998016357 | test_loss: 6.1710700989 | \n",
      "Epoch: 2226 | train_loss: 117.9995803833 | test_loss: 6.1710119247 | \n",
      "Epoch: 2227 | train_loss: 117.9993591309 | test_loss: 6.1709570885 | \n",
      "Epoch: 2228 | train_loss: 117.9991607666 | test_loss: 6.1708989143 | \n",
      "Epoch: 2229 | train_loss: 117.9989242554 | test_loss: 6.1708350182 | \n",
      "Epoch: 2230 | train_loss: 117.9987106323 | test_loss: 6.1707801819 | \n",
      "Epoch: 2231 | train_loss: 117.9985122681 | test_loss: 6.1707220078 | \n",
      "Epoch: 2232 | train_loss: 117.9982833862 | test_loss: 6.1706628799 | \n",
      "Epoch: 2233 | train_loss: 117.9980392456 | test_loss: 6.1706018448 | \n",
      "Epoch: 2234 | train_loss: 117.9978332520 | test_loss: 6.1705431938 | \n",
      "Epoch: 2235 | train_loss: 117.9975662231 | test_loss: 6.1704864502 | \n",
      "Epoch: 2236 | train_loss: 117.9973831177 | test_loss: 6.1704335213 | \n",
      "Epoch: 2237 | train_loss: 117.9971618652 | test_loss: 6.1703681946 | \n",
      "Epoch: 2238 | train_loss: 117.9969329834 | test_loss: 6.1703171730 | \n",
      "Epoch: 2239 | train_loss: 117.9966812134 | test_loss: 6.1702599525 | \n",
      "Epoch: 2240 | train_loss: 117.9965133667 | test_loss: 6.1701936722 | \n",
      "Epoch: 2241 | train_loss: 117.9962921143 | test_loss: 6.1701359749 | \n",
      "Epoch: 2242 | train_loss: 117.9960479736 | test_loss: 6.1700768471 | \n",
      "Epoch: 2243 | train_loss: 117.9958419800 | test_loss: 6.1700191498 | \n",
      "Epoch: 2244 | train_loss: 117.9956359863 | test_loss: 6.1699585915 | \n",
      "Epoch: 2245 | train_loss: 117.9953994751 | test_loss: 6.1699008942 | \n",
      "Epoch: 2246 | train_loss: 117.9951629639 | test_loss: 6.1698484421 | \n",
      "Epoch: 2247 | train_loss: 117.9949493408 | test_loss: 6.1697931290 | \n",
      "Epoch: 2248 | train_loss: 117.9947280884 | test_loss: 6.1697368622 | \n",
      "Epoch: 2249 | train_loss: 117.9945220947 | test_loss: 6.1696729660 | \n",
      "Epoch: 2250 | train_loss: 117.9943466187 | test_loss: 6.1696128845 | \n",
      "Epoch: 2251 | train_loss: 117.9941406250 | test_loss: 6.1695542336 | \n",
      "Epoch: 2252 | train_loss: 117.9939193726 | test_loss: 6.1695055962 | \n",
      "Epoch: 2253 | train_loss: 117.9936752319 | test_loss: 6.1694431305 | \n",
      "Epoch: 2254 | train_loss: 117.9934463501 | test_loss: 6.1693882942 | \n",
      "Epoch: 2255 | train_loss: 117.9932098389 | test_loss: 6.1693282127 | \n",
      "Epoch: 2256 | train_loss: 117.9930191040 | test_loss: 6.1692652702 | \n",
      "Epoch: 2257 | train_loss: 117.9928054810 | test_loss: 6.1692066193 | \n",
      "Epoch: 2258 | train_loss: 117.9925765991 | test_loss: 6.1691508293 | \n",
      "Epoch: 2259 | train_loss: 117.9923706055 | test_loss: 6.1690959930 | \n",
      "Epoch: 2260 | train_loss: 117.9921188354 | test_loss: 6.1690349579 | \n",
      "Epoch: 2261 | train_loss: 117.9919204712 | test_loss: 6.1689796448 | \n",
      "Epoch: 2262 | train_loss: 117.9917297363 | test_loss: 6.1689257622 | \n",
      "Epoch: 2263 | train_loss: 117.9915008545 | test_loss: 6.1688675880 | \n",
      "Epoch: 2264 | train_loss: 117.9913177490 | test_loss: 6.1688103676 | \n",
      "Epoch: 2265 | train_loss: 117.9910736084 | test_loss: 6.1687469482 | \n",
      "Epoch: 2266 | train_loss: 117.9908599854 | test_loss: 6.1686902046 | \n",
      "Epoch: 2267 | train_loss: 117.9906768799 | test_loss: 6.1686344147 | \n",
      "Epoch: 2268 | train_loss: 117.9904174805 | test_loss: 6.1685752869 | \n",
      "Epoch: 2269 | train_loss: 117.9902267456 | test_loss: 6.1685180664 | \n",
      "Epoch: 2270 | train_loss: 117.9900588989 | test_loss: 6.1684613228 | \n",
      "Epoch: 2271 | train_loss: 117.9898223877 | test_loss: 6.1683988571 | \n",
      "Epoch: 2272 | train_loss: 117.9895782471 | test_loss: 6.1683497429 | \n",
      "Epoch: 2273 | train_loss: 117.9893569946 | test_loss: 6.1682868004 | \n",
      "Epoch: 2274 | train_loss: 117.9891204834 | test_loss: 6.1682338715 | \n",
      "Epoch: 2275 | train_loss: 117.9889221191 | test_loss: 6.1681728363 | \n",
      "Epoch: 2276 | train_loss: 117.9887237549 | test_loss: 6.1681151390 | \n",
      "Epoch: 2277 | train_loss: 117.9885025024 | test_loss: 6.1680588722 | \n",
      "Epoch: 2278 | train_loss: 117.9882507324 | test_loss: 6.1679978371 | \n",
      "Epoch: 2279 | train_loss: 117.9880447388 | test_loss: 6.1679387093 | \n",
      "Epoch: 2280 | train_loss: 117.9878311157 | test_loss: 6.1678853035 | \n",
      "Epoch: 2281 | train_loss: 117.9876174927 | test_loss: 6.1678357124 | \n",
      "Epoch: 2282 | train_loss: 117.9874191284 | test_loss: 6.1677808762 | \n",
      "Epoch: 2283 | train_loss: 117.9872360229 | test_loss: 6.1677203178 | \n",
      "Epoch: 2284 | train_loss: 117.9869995117 | test_loss: 6.1676611900 | \n",
      "Epoch: 2285 | train_loss: 117.9867782593 | test_loss: 6.1676082611 | \n",
      "Epoch: 2286 | train_loss: 117.9865646362 | test_loss: 6.1675496101 | \n",
      "Epoch: 2287 | train_loss: 117.9863510132 | test_loss: 6.1674914360 | \n",
      "Epoch: 2288 | train_loss: 117.9861450195 | test_loss: 6.1674342155 | \n",
      "Epoch: 2289 | train_loss: 117.9859390259 | test_loss: 6.1673779488 | \n",
      "Epoch: 2290 | train_loss: 117.9856948853 | test_loss: 6.1673264503 | \n",
      "Epoch: 2291 | train_loss: 117.9854965210 | test_loss: 6.1672654152 | \n",
      "Epoch: 2292 | train_loss: 117.9852523804 | test_loss: 6.1672072411 | \n",
      "Epoch: 2293 | train_loss: 117.9850311279 | test_loss: 6.1671485901 | \n",
      "Epoch: 2294 | train_loss: 117.9848251343 | test_loss: 6.1670894623 | \n",
      "Epoch: 2295 | train_loss: 117.9846191406 | test_loss: 6.1670312881 | \n",
      "Epoch: 2296 | train_loss: 117.9843826294 | test_loss: 6.1669683456 | \n",
      "Epoch: 2297 | train_loss: 117.9841384888 | test_loss: 6.1669139862 | \n",
      "Epoch: 2298 | train_loss: 117.9839172363 | test_loss: 6.1668629646 | \n",
      "Epoch: 2299 | train_loss: 117.9837112427 | test_loss: 6.1668076515 | \n",
      "Epoch: 2300 | train_loss: 117.9834976196 | test_loss: 6.1667509079 | \n",
      "Epoch: 2301 | train_loss: 117.9832916260 | test_loss: 6.1666898727 | \n",
      "Epoch: 2302 | train_loss: 117.9831237793 | test_loss: 6.1666345596 | \n",
      "Epoch: 2303 | train_loss: 117.9828720093 | test_loss: 6.1665763855 | \n",
      "Epoch: 2304 | train_loss: 117.9827041626 | test_loss: 6.1665196419 | \n",
      "Epoch: 2305 | train_loss: 117.9824523926 | test_loss: 6.1664566994 | \n",
      "Epoch: 2306 | train_loss: 117.9822540283 | test_loss: 6.1664037704 | \n",
      "Epoch: 2307 | train_loss: 117.9820480347 | test_loss: 6.1663475037 | \n",
      "Epoch: 2308 | train_loss: 117.9818344116 | test_loss: 6.1662974358 | \n",
      "Epoch: 2309 | train_loss: 117.9815979004 | test_loss: 6.1662406921 | \n",
      "Epoch: 2310 | train_loss: 117.9813766479 | test_loss: 6.1661820412 | \n",
      "Epoch: 2311 | train_loss: 117.9811706543 | test_loss: 6.1661243439 | \n",
      "Epoch: 2312 | train_loss: 117.9809494019 | test_loss: 6.1660656929 | \n",
      "Epoch: 2313 | train_loss: 117.9807586670 | test_loss: 6.1660075188 | \n",
      "Epoch: 2314 | train_loss: 117.9805679321 | test_loss: 6.1659517288 | \n",
      "Epoch: 2315 | train_loss: 117.9803009033 | test_loss: 6.1658964157 | \n",
      "Epoch: 2316 | train_loss: 117.9800949097 | test_loss: 6.1658396721 | \n",
      "Epoch: 2317 | train_loss: 117.9798812866 | test_loss: 6.1657886505 | \n",
      "Epoch: 2318 | train_loss: 117.9796752930 | test_loss: 6.1657276154 | \n",
      "Epoch: 2319 | train_loss: 117.9794311523 | test_loss: 6.1656718254 | \n",
      "Epoch: 2320 | train_loss: 117.9792404175 | test_loss: 6.1656103134 | \n",
      "Epoch: 2321 | train_loss: 117.9790115356 | test_loss: 6.1655507088 | \n",
      "Epoch: 2322 | train_loss: 117.9788208008 | test_loss: 6.1654944420 | \n",
      "Epoch: 2323 | train_loss: 117.9785842896 | test_loss: 6.1654386520 | \n",
      "Epoch: 2324 | train_loss: 117.9783782959 | test_loss: 6.1653842926 | \n",
      "Epoch: 2325 | train_loss: 117.9781723022 | test_loss: 6.1653318405 | \n",
      "Epoch: 2326 | train_loss: 117.9779510498 | test_loss: 6.1652765274 | \n",
      "Epoch: 2327 | train_loss: 117.9777374268 | test_loss: 6.1652159691 | \n",
      "Epoch: 2328 | train_loss: 117.9775085449 | test_loss: 6.1651573181 | \n",
      "Epoch: 2329 | train_loss: 117.9773559570 | test_loss: 6.1650977135 | \n",
      "Epoch: 2330 | train_loss: 117.9770889282 | test_loss: 6.1650447845 | \n",
      "Epoch: 2331 | train_loss: 117.9769058228 | test_loss: 6.1649889946 | \n",
      "Epoch: 2332 | train_loss: 117.9766540527 | test_loss: 6.1649317741 | \n",
      "Epoch: 2333 | train_loss: 117.9764556885 | test_loss: 6.1648736000 | \n",
      "Epoch: 2334 | train_loss: 117.9762268066 | test_loss: 6.1648139954 | \n",
      "Epoch: 2335 | train_loss: 117.9759826660 | test_loss: 6.1647577286 | \n",
      "Epoch: 2336 | train_loss: 117.9757995605 | test_loss: 6.1647038460 | \n",
      "Epoch: 2337 | train_loss: 117.9756088257 | test_loss: 6.1646485329 | \n",
      "Epoch: 2338 | train_loss: 117.9753646851 | test_loss: 6.1645922661 | \n",
      "Epoch: 2339 | train_loss: 117.9751586914 | test_loss: 6.1645355225 | \n",
      "Epoch: 2340 | train_loss: 117.9749603271 | test_loss: 6.1644840240 | \n",
      "Epoch: 2341 | train_loss: 117.9747543335 | test_loss: 6.1644268036 | \n",
      "Epoch: 2342 | train_loss: 117.9745025635 | test_loss: 6.1643671989 | \n",
      "Epoch: 2343 | train_loss: 117.9743270874 | test_loss: 6.1643071175 | \n",
      "Epoch: 2344 | train_loss: 117.9740753174 | test_loss: 6.1642479897 | \n",
      "Epoch: 2345 | train_loss: 117.9739151001 | test_loss: 6.1642012596 | \n",
      "Epoch: 2346 | train_loss: 117.9737091064 | test_loss: 6.1641387939 | \n",
      "Epoch: 2347 | train_loss: 117.9734802246 | test_loss: 6.1640782356 | \n",
      "Epoch: 2348 | train_loss: 117.9732513428 | test_loss: 6.1640238762 | \n",
      "Epoch: 2349 | train_loss: 117.9730148315 | test_loss: 6.1639671326 | \n",
      "Epoch: 2350 | train_loss: 117.9727935791 | test_loss: 6.1639165878 | \n",
      "Epoch: 2351 | train_loss: 117.9725646973 | test_loss: 6.1638574600 | \n",
      "Epoch: 2352 | train_loss: 117.9723434448 | test_loss: 6.1638026237 | \n",
      "Epoch: 2353 | train_loss: 117.9721755981 | test_loss: 6.1637382507 | \n",
      "Epoch: 2354 | train_loss: 117.9719238281 | test_loss: 6.1636838913 | \n",
      "Epoch: 2355 | train_loss: 117.9717178345 | test_loss: 6.1636257172 | \n",
      "Epoch: 2356 | train_loss: 117.9715042114 | test_loss: 6.1635723114 | \n",
      "Epoch: 2357 | train_loss: 117.9712524414 | test_loss: 6.1635198593 | \n",
      "Epoch: 2358 | train_loss: 117.9710540771 | test_loss: 6.1634635925 | \n",
      "Epoch: 2359 | train_loss: 117.9708404541 | test_loss: 6.1634039879 | \n",
      "Epoch: 2360 | train_loss: 117.9706573486 | test_loss: 6.1633491516 | \n",
      "Epoch: 2361 | train_loss: 117.9703979492 | test_loss: 6.1632933617 | \n",
      "Epoch: 2362 | train_loss: 117.9702377319 | test_loss: 6.1632361412 | \n",
      "Epoch: 2363 | train_loss: 117.9699935913 | test_loss: 6.1631770134 | \n",
      "Epoch: 2364 | train_loss: 117.9697875977 | test_loss: 6.1631226540 | \n",
      "Epoch: 2365 | train_loss: 117.9695892334 | test_loss: 6.1630687714 | \n",
      "Epoch: 2366 | train_loss: 117.9693450928 | test_loss: 6.1630158424 | \n",
      "Epoch: 2367 | train_loss: 117.9691772461 | test_loss: 6.1629581451 | \n",
      "Epoch: 2368 | train_loss: 117.9689178467 | test_loss: 6.1628966331 | \n",
      "Epoch: 2369 | train_loss: 117.9686889648 | test_loss: 6.1628489494 | \n",
      "Epoch: 2370 | train_loss: 117.9685287476 | test_loss: 6.1627922058 | \n",
      "Epoch: 2371 | train_loss: 117.9682693481 | test_loss: 6.1627378464 | \n",
      "Epoch: 2372 | train_loss: 117.9680786133 | test_loss: 6.1626787186 | \n",
      "Epoch: 2373 | train_loss: 117.9678726196 | test_loss: 6.1626219749 | \n",
      "Epoch: 2374 | train_loss: 117.9676284790 | test_loss: 6.1625628471 | \n",
      "Epoch: 2375 | train_loss: 117.9673767090 | test_loss: 6.1625061035 | \n",
      "Epoch: 2376 | train_loss: 117.9672088623 | test_loss: 6.1624546051 | \n",
      "Epoch: 2377 | train_loss: 117.9669952393 | test_loss: 6.1623926163 | \n",
      "Epoch: 2378 | train_loss: 117.9667358398 | test_loss: 6.1623401642 | \n",
      "Epoch: 2379 | train_loss: 117.9665527344 | test_loss: 6.1622800827 | \n",
      "Epoch: 2380 | train_loss: 117.9663085938 | test_loss: 6.1622290611 | \n",
      "Epoch: 2381 | train_loss: 117.9660949707 | test_loss: 6.1621708870 | \n",
      "Epoch: 2382 | train_loss: 117.9658966064 | test_loss: 6.1621141434 | \n",
      "Epoch: 2383 | train_loss: 117.9656677246 | test_loss: 6.1620631218 | \n",
      "Epoch: 2384 | train_loss: 117.9654464722 | test_loss: 6.1620011330 | \n",
      "Epoch: 2385 | train_loss: 117.9652175903 | test_loss: 6.1619462967 | \n",
      "Epoch: 2386 | train_loss: 117.9650421143 | test_loss: 6.1618933678 | \n",
      "Epoch: 2387 | train_loss: 117.9648056030 | test_loss: 6.1618428230 | \n",
      "Epoch: 2388 | train_loss: 117.9645996094 | test_loss: 6.1617865562 | \n",
      "Epoch: 2389 | train_loss: 117.9644165039 | test_loss: 6.1617264748 | \n",
      "Epoch: 2390 | train_loss: 117.9641723633 | test_loss: 6.1616640091 | \n",
      "Epoch: 2391 | train_loss: 117.9639434814 | test_loss: 6.1616106033 | \n",
      "Epoch: 2392 | train_loss: 117.9637374878 | test_loss: 6.1615529060 | \n",
      "Epoch: 2393 | train_loss: 117.9634933472 | test_loss: 6.1615009308 | \n",
      "Epoch: 2394 | train_loss: 117.9633102417 | test_loss: 6.1614470482 | \n",
      "Epoch: 2395 | train_loss: 117.9630889893 | test_loss: 6.1613879204 | \n",
      "Epoch: 2396 | train_loss: 117.9628524780 | test_loss: 6.1613354683 | \n",
      "Epoch: 2397 | train_loss: 117.9626388550 | test_loss: 6.1612811089 | \n",
      "Epoch: 2398 | train_loss: 117.9624404907 | test_loss: 6.1612224579 | \n",
      "Epoch: 2399 | train_loss: 117.9622192383 | test_loss: 6.1611738205 | \n",
      "Epoch: 2400 | train_loss: 117.9619903564 | test_loss: 6.1611113548 | \n",
      "Epoch: 2401 | train_loss: 117.9617843628 | test_loss: 6.1610517502 | \n",
      "Epoch: 2402 | train_loss: 117.9615402222 | test_loss: 6.1609992981 | \n",
      "Epoch: 2403 | train_loss: 117.9613571167 | test_loss: 6.1609406471 | \n",
      "Epoch: 2404 | train_loss: 117.9611206055 | test_loss: 6.1608872414 | \n",
      "Epoch: 2405 | train_loss: 117.9609375000 | test_loss: 6.1608319283 | \n",
      "Epoch: 2406 | train_loss: 117.9607238770 | test_loss: 6.1607775688 | \n",
      "Epoch: 2407 | train_loss: 117.9604873657 | test_loss: 6.1607208252 | \n",
      "Epoch: 2408 | train_loss: 117.9602432251 | test_loss: 6.1606655121 | \n",
      "Epoch: 2409 | train_loss: 117.9600524902 | test_loss: 6.1606078148 | \n",
      "Epoch: 2410 | train_loss: 117.9598846436 | test_loss: 6.1605482101 | \n",
      "Epoch: 2411 | train_loss: 117.9596252441 | test_loss: 6.1604948044 | \n",
      "Epoch: 2412 | train_loss: 117.9594116211 | test_loss: 6.1604366302 | \n",
      "Epoch: 2413 | train_loss: 117.9591674805 | test_loss: 6.1603837013 | \n",
      "Epoch: 2414 | train_loss: 117.9589538574 | test_loss: 6.1603288651 | \n",
      "Epoch: 2415 | train_loss: 117.9587478638 | test_loss: 6.1602735519 | \n",
      "Epoch: 2416 | train_loss: 117.9585266113 | test_loss: 6.1602158546 | \n",
      "Epoch: 2417 | train_loss: 117.9583206177 | test_loss: 6.1601648331 | \n",
      "Epoch: 2418 | train_loss: 117.9580841064 | test_loss: 6.1601018906 | \n",
      "Epoch: 2419 | train_loss: 117.9578247070 | test_loss: 6.1600480080 | \n",
      "Epoch: 2420 | train_loss: 117.9576110840 | test_loss: 6.1599907875 | \n",
      "Epoch: 2421 | train_loss: 117.9574279785 | test_loss: 6.1599397659 | \n",
      "Epoch: 2422 | train_loss: 117.9572067261 | test_loss: 6.1598844528 | \n",
      "Epoch: 2423 | train_loss: 117.9570007324 | test_loss: 6.1598234177 | \n",
      "Epoch: 2424 | train_loss: 117.9567871094 | test_loss: 6.1597723961 | \n",
      "Epoch: 2425 | train_loss: 117.9565353394 | test_loss: 6.1597142220 | \n",
      "Epoch: 2426 | train_loss: 117.9563217163 | test_loss: 6.1596598625 | \n",
      "Epoch: 2427 | train_loss: 117.9561157227 | test_loss: 6.1596007347 | \n",
      "Epoch: 2428 | train_loss: 117.9559249878 | test_loss: 6.1595473289 | \n",
      "Epoch: 2429 | train_loss: 117.9557037354 | test_loss: 6.1594963074 | \n",
      "Epoch: 2430 | train_loss: 117.9554519653 | test_loss: 6.1594347954 | \n",
      "Epoch: 2431 | train_loss: 117.9552536011 | test_loss: 6.1593818665 | \n",
      "Epoch: 2432 | train_loss: 117.9550399780 | test_loss: 6.1593217850 | \n",
      "Epoch: 2433 | train_loss: 117.9547958374 | test_loss: 6.1592617035 | \n",
      "Epoch: 2434 | train_loss: 117.9546279907 | test_loss: 6.1592121124 | \n",
      "Epoch: 2435 | train_loss: 117.9543914795 | test_loss: 6.1591620445 | \n",
      "Epoch: 2436 | train_loss: 117.9541702271 | test_loss: 6.1591010094 | \n",
      "Epoch: 2437 | train_loss: 117.9539642334 | test_loss: 6.1590437889 | \n",
      "Epoch: 2438 | train_loss: 117.9537582397 | test_loss: 6.1589875221 | \n",
      "Epoch: 2439 | train_loss: 117.9535446167 | test_loss: 6.1589336395 | \n",
      "Epoch: 2440 | train_loss: 117.9533309937 | test_loss: 6.1588745117 | \n",
      "Epoch: 2441 | train_loss: 117.9531021118 | test_loss: 6.1588230133 | \n",
      "Epoch: 2442 | train_loss: 117.9529190063 | test_loss: 6.1587653160 | \n",
      "Epoch: 2443 | train_loss: 117.9526748657 | test_loss: 6.1587123871 | \n",
      "Epoch: 2444 | train_loss: 117.9524536133 | test_loss: 6.1586551666 | \n",
      "Epoch: 2445 | train_loss: 117.9522247314 | test_loss: 6.1585969925 | \n",
      "Epoch: 2446 | train_loss: 117.9519958496 | test_loss: 6.1585450172 | \n",
      "Epoch: 2447 | train_loss: 117.9517745972 | test_loss: 6.1584959030 | \n",
      "Epoch: 2448 | train_loss: 117.9515914917 | test_loss: 6.1584367752 | \n",
      "Epoch: 2449 | train_loss: 117.9513778687 | test_loss: 6.1583776474 | \n",
      "Epoch: 2450 | train_loss: 117.9511489868 | test_loss: 6.1583251953 | \n",
      "Epoch: 2451 | train_loss: 117.9509277344 | test_loss: 6.1582708359 | \n",
      "Epoch: 2452 | train_loss: 117.9507141113 | test_loss: 6.1582188606 | \n",
      "Epoch: 2453 | train_loss: 117.9505004883 | test_loss: 6.1581587791 | \n",
      "Epoch: 2454 | train_loss: 117.9502563477 | test_loss: 6.1581025124 | \n",
      "Epoch: 2455 | train_loss: 117.9500656128 | test_loss: 6.1580438614 | \n",
      "Epoch: 2456 | train_loss: 117.9498443604 | test_loss: 6.1579952240 | \n",
      "Epoch: 2457 | train_loss: 117.9496459961 | test_loss: 6.1579341888 | \n",
      "Epoch: 2458 | train_loss: 117.9493942261 | test_loss: 6.1578841209 | \n",
      "Epoch: 2459 | train_loss: 117.9491806030 | test_loss: 6.1578383446 | \n",
      "Epoch: 2460 | train_loss: 117.9489746094 | test_loss: 6.1577787399 | \n",
      "Epoch: 2461 | train_loss: 117.9487609863 | test_loss: 6.1577229500 | \n",
      "Epoch: 2462 | train_loss: 117.9485778809 | test_loss: 6.1576647758 | \n",
      "Epoch: 2463 | train_loss: 117.9483871460 | test_loss: 6.1576066017 | \n",
      "Epoch: 2464 | train_loss: 117.9481353760 | test_loss: 6.1575489044 | \n",
      "Epoch: 2465 | train_loss: 117.9479217529 | test_loss: 6.1574993134 | \n",
      "Epoch: 2466 | train_loss: 117.9477157593 | test_loss: 6.1574430466 | \n",
      "Epoch: 2467 | train_loss: 117.9475097656 | test_loss: 6.1573886871 | \n",
      "Epoch: 2468 | train_loss: 117.9472656250 | test_loss: 6.1573286057 | \n",
      "Epoch: 2469 | train_loss: 117.9470672607 | test_loss: 6.1572771072 | \n",
      "Epoch: 2470 | train_loss: 117.9468231201 | test_loss: 6.1572217941 | \n",
      "Epoch: 2471 | train_loss: 117.9466094971 | test_loss: 6.1571717262 | \n",
      "Epoch: 2472 | train_loss: 117.9463882446 | test_loss: 6.1571168900 | \n",
      "Epoch: 2473 | train_loss: 117.9461517334 | test_loss: 6.1570634842 | \n",
      "Epoch: 2474 | train_loss: 117.9459838867 | test_loss: 6.1570081711 | \n",
      "Epoch: 2475 | train_loss: 117.9457397461 | test_loss: 6.1569514275 | \n",
      "Epoch: 2476 | train_loss: 117.9455490112 | test_loss: 6.1568932533 | \n",
      "Epoch: 2477 | train_loss: 117.9453201294 | test_loss: 6.1568412781 | \n",
      "Epoch: 2478 | train_loss: 117.9450988770 | test_loss: 6.1567878723 | \n",
      "Epoch: 2479 | train_loss: 117.9448699951 | test_loss: 6.1567301750 | \n",
      "Epoch: 2480 | train_loss: 117.9446792603 | test_loss: 6.1566772461 | \n",
      "Epoch: 2481 | train_loss: 117.9444427490 | test_loss: 6.1566190720 | \n",
      "Epoch: 2482 | train_loss: 117.9442672729 | test_loss: 6.1565599442 | \n",
      "Epoch: 2483 | train_loss: 117.9440155029 | test_loss: 6.1565036774 | \n",
      "Epoch: 2484 | train_loss: 117.9437866211 | test_loss: 6.1564502716 | \n",
      "Epoch: 2485 | train_loss: 117.9435653687 | test_loss: 6.1563911438 | \n",
      "Epoch: 2486 | train_loss: 117.9433517456 | test_loss: 6.1563348770 | \n",
      "Epoch: 2487 | train_loss: 117.9431457520 | test_loss: 6.1562781334 | \n",
      "Epoch: 2488 | train_loss: 117.9429473877 | test_loss: 6.1562280655 | \n",
      "Epoch: 2489 | train_loss: 117.9427566528 | test_loss: 6.1561741829 | \n",
      "Epoch: 2490 | train_loss: 117.9425582886 | test_loss: 6.1561183929 | \n",
      "Epoch: 2491 | train_loss: 117.9423217773 | test_loss: 6.1560630798 | \n",
      "Epoch: 2492 | train_loss: 117.9420928955 | test_loss: 6.1560087204 | \n",
      "Epoch: 2493 | train_loss: 117.9418334961 | test_loss: 6.1559534073 | \n",
      "Epoch: 2494 | train_loss: 117.9416732788 | test_loss: 6.1558961868 | \n",
      "Epoch: 2495 | train_loss: 117.9414291382 | test_loss: 6.1558408737 | \n",
      "Epoch: 2496 | train_loss: 117.9412536621 | test_loss: 6.1557893753 | \n",
      "Epoch: 2497 | train_loss: 117.9409942627 | test_loss: 6.1557307243 | \n",
      "Epoch: 2498 | train_loss: 117.9407806396 | test_loss: 6.1556739807 | \n",
      "Epoch: 2499 | train_loss: 117.9405593872 | test_loss: 6.1556181908 | \n",
      "Epoch: 2500 | train_loss: 117.9403915405 | test_loss: 6.1555657387 | \n",
      "Epoch: 2501 | train_loss: 117.9401245117 | test_loss: 6.1555070877 | \n",
      "Epoch: 2502 | train_loss: 117.9398956299 | test_loss: 6.1554503441 | \n",
      "Epoch: 2503 | train_loss: 117.9396972656 | test_loss: 6.1553983688 | \n",
      "Epoch: 2504 | train_loss: 117.9394454956 | test_loss: 6.1553425789 | \n",
      "Epoch: 2505 | train_loss: 117.9392395020 | test_loss: 6.1552844048 | \n",
      "Epoch: 2506 | train_loss: 117.9390029907 | test_loss: 6.1552295685 | \n",
      "Epoch: 2507 | train_loss: 117.9387817383 | test_loss: 6.1551713943 | \n",
      "Epoch: 2508 | train_loss: 117.9385910034 | test_loss: 6.1551170349 | \n",
      "Epoch: 2509 | train_loss: 117.9383697510 | test_loss: 6.1550593376 | \n",
      "Epoch: 2510 | train_loss: 117.9381942749 | test_loss: 6.1550097466 | \n",
      "Epoch: 2511 | train_loss: 117.9379272461 | test_loss: 6.1549572945 | \n",
      "Epoch: 2512 | train_loss: 117.9377212524 | test_loss: 6.1548991203 | \n",
      "Epoch: 2513 | train_loss: 117.9375228882 | test_loss: 6.1548447609 | \n",
      "Epoch: 2514 | train_loss: 117.9372863770 | test_loss: 6.1547899246 | \n",
      "Epoch: 2515 | train_loss: 117.9370574951 | test_loss: 6.1547369957 | \n",
      "Epoch: 2516 | train_loss: 117.9368743896 | test_loss: 6.1546769142 | \n",
      "Epoch: 2517 | train_loss: 117.9366302490 | test_loss: 6.1546216011 | \n",
      "Epoch: 2518 | train_loss: 117.9364089966 | test_loss: 6.1545648575 | \n",
      "Epoch: 2519 | train_loss: 117.9361724854 | test_loss: 6.1545109749 | \n",
      "Epoch: 2520 | train_loss: 117.9360046387 | test_loss: 6.1544585228 | \n",
      "Epoch: 2521 | train_loss: 117.9357833862 | test_loss: 6.1544017792 | \n",
      "Epoch: 2522 | train_loss: 117.9355163574 | test_loss: 6.1543455124 | \n",
      "Epoch: 2523 | train_loss: 117.9352722168 | test_loss: 6.1542944908 | \n",
      "Epoch: 2524 | train_loss: 117.9350814819 | test_loss: 6.1542382240 | \n",
      "Epoch: 2525 | train_loss: 117.9348754883 | test_loss: 6.1541876793 | \n",
      "Epoch: 2526 | train_loss: 117.9346771240 | test_loss: 6.1541333199 | \n",
      "Epoch: 2527 | train_loss: 117.9344329834 | test_loss: 6.1540751457 | \n",
      "Epoch: 2528 | train_loss: 117.9341812134 | test_loss: 6.1540203094 | \n",
      "Epoch: 2529 | train_loss: 117.9340133667 | test_loss: 6.1539673805 | \n",
      "Epoch: 2530 | train_loss: 117.9337692261 | test_loss: 6.1539139748 | \n",
      "Epoch: 2531 | train_loss: 117.9335556030 | test_loss: 6.1538591385 | \n",
      "Epoch: 2532 | train_loss: 117.9333724976 | test_loss: 6.1538057327 | \n",
      "Epoch: 2533 | train_loss: 117.9331130981 | test_loss: 6.1537532806 | \n",
      "Epoch: 2534 | train_loss: 117.9328918457 | test_loss: 6.1536970139 | \n",
      "Epoch: 2535 | train_loss: 117.9326782227 | test_loss: 6.1536450386 | \n",
      "Epoch: 2536 | train_loss: 117.9324493408 | test_loss: 6.1535859108 | \n",
      "Epoch: 2537 | train_loss: 117.9322280884 | test_loss: 6.1535301208 | \n",
      "Epoch: 2538 | train_loss: 117.9319763184 | test_loss: 6.1534714699 | \n",
      "Epoch: 2539 | train_loss: 117.9317855835 | test_loss: 6.1534161568 | \n",
      "Epoch: 2540 | train_loss: 117.9315795898 | test_loss: 6.1533622742 | \n",
      "Epoch: 2541 | train_loss: 117.9313583374 | test_loss: 6.1533160210 | \n",
      "Epoch: 2542 | train_loss: 117.9311447144 | test_loss: 6.1532673836 | \n",
      "Epoch: 2543 | train_loss: 117.9309005737 | test_loss: 6.1532073021 | \n",
      "Epoch: 2544 | train_loss: 117.9306945801 | test_loss: 6.1531486511 | \n",
      "Epoch: 2545 | train_loss: 117.9304428101 | test_loss: 6.1530952454 | \n",
      "Epoch: 2546 | train_loss: 117.9302444458 | test_loss: 6.1530399323 | \n",
      "Epoch: 2547 | train_loss: 117.9300308228 | test_loss: 6.1529860497 | \n",
      "Epoch: 2548 | train_loss: 117.9298171997 | test_loss: 6.1529359818 | \n",
      "Epoch: 2549 | train_loss: 117.9296112061 | test_loss: 6.1528763771 | \n",
      "Epoch: 2550 | train_loss: 117.9293518066 | test_loss: 6.1528215408 | \n",
      "Epoch: 2551 | train_loss: 117.9291381836 | test_loss: 6.1527695656 | \n",
      "Epoch: 2552 | train_loss: 117.9289321899 | test_loss: 6.1527123451 | \n",
      "Epoch: 2553 | train_loss: 117.9287033081 | test_loss: 6.1526618004 | \n",
      "Epoch: 2554 | train_loss: 117.9284362793 | test_loss: 6.1526026726 | \n",
      "Epoch: 2555 | train_loss: 117.9282608032 | test_loss: 6.1525468826 | \n",
      "Epoch: 2556 | train_loss: 117.9280166626 | test_loss: 6.1524925232 | \n",
      "Epoch: 2557 | train_loss: 117.9278030396 | test_loss: 6.1524353027 | \n",
      "Epoch: 2558 | train_loss: 117.9275817871 | test_loss: 6.1523723602 | \n",
      "Epoch: 2559 | train_loss: 117.9273223877 | test_loss: 6.1523151398 | \n",
      "Epoch: 2560 | train_loss: 117.9270935059 | test_loss: 6.1522669792 | \n",
      "Epoch: 2561 | train_loss: 117.9269104004 | test_loss: 6.1522145271 | \n",
      "Epoch: 2562 | train_loss: 117.9266815186 | test_loss: 6.1521611214 | \n",
      "Epoch: 2563 | train_loss: 117.9264907837 | test_loss: 6.1521110535 | \n",
      "Epoch: 2564 | train_loss: 117.9262771606 | test_loss: 6.1520552635 | \n",
      "Epoch: 2565 | train_loss: 117.9260330200 | test_loss: 6.1519975662 | \n",
      "Epoch: 2566 | train_loss: 117.9258117676 | test_loss: 6.1519427299 | \n",
      "Epoch: 2567 | train_loss: 117.9255752563 | test_loss: 6.1518845558 | \n",
      "Epoch: 2568 | train_loss: 117.9252929688 | test_loss: 6.1518373489 | \n",
      "Epoch: 2569 | train_loss: 117.9251403809 | test_loss: 6.1517820358 | \n",
      "Epoch: 2570 | train_loss: 117.9249114990 | test_loss: 6.1517267227 | \n",
      "Epoch: 2571 | train_loss: 117.9246902466 | test_loss: 6.1516761780 | \n",
      "Epoch: 2572 | train_loss: 117.9244613647 | test_loss: 6.1516199112 | \n",
      "Epoch: 2573 | train_loss: 117.9242477417 | test_loss: 6.1515650749 | \n",
      "Epoch: 2574 | train_loss: 117.9239807129 | test_loss: 6.1515126228 | \n",
      "Epoch: 2575 | train_loss: 117.9237670898 | test_loss: 6.1514568329 | \n",
      "Epoch: 2576 | train_loss: 117.9235763550 | test_loss: 6.1513996124 | \n",
      "Epoch: 2577 | train_loss: 117.9233474731 | test_loss: 6.1513481140 | \n",
      "Epoch: 2578 | train_loss: 117.9230880737 | test_loss: 6.1512951851 | \n",
      "Epoch: 2579 | train_loss: 117.9228897095 | test_loss: 6.1512355804 | \n",
      "Epoch: 2580 | train_loss: 117.9226837158 | test_loss: 6.1511812210 | \n",
      "Epoch: 2581 | train_loss: 117.9224395752 | test_loss: 6.1511273384 | \n",
      "Epoch: 2582 | train_loss: 117.9221954346 | test_loss: 6.1510701180 | \n",
      "Epoch: 2583 | train_loss: 117.9219512939 | test_loss: 6.1510233879 | \n",
      "Epoch: 2584 | train_loss: 117.9217376709 | test_loss: 6.1509709358 | \n",
      "Epoch: 2585 | train_loss: 117.9215316772 | test_loss: 6.1509122849 | \n",
      "Epoch: 2586 | train_loss: 117.9213409424 | test_loss: 6.1508574486 | \n",
      "Epoch: 2587 | train_loss: 117.9211196899 | test_loss: 6.1508021355 | \n",
      "Epoch: 2588 | train_loss: 117.9208602905 | test_loss: 6.1507463455 | \n",
      "Epoch: 2589 | train_loss: 117.9206237793 | test_loss: 6.1506943703 | \n",
      "Epoch: 2590 | train_loss: 117.9204483032 | test_loss: 6.1506376266 | \n",
      "Epoch: 2591 | train_loss: 117.9201736450 | test_loss: 6.1505804062 | \n",
      "Epoch: 2592 | train_loss: 117.9199676514 | test_loss: 6.1505236626 | \n",
      "Epoch: 2593 | train_loss: 117.9197463989 | test_loss: 6.1504640579 | \n",
      "Epoch: 2594 | train_loss: 117.9195251465 | test_loss: 6.1504125595 | \n",
      "Epoch: 2595 | train_loss: 117.9192962646 | test_loss: 6.1503615379 | \n",
      "Epoch: 2596 | train_loss: 117.9190597534 | test_loss: 6.1503095627 | \n",
      "Epoch: 2597 | train_loss: 117.9188308716 | test_loss: 6.1502575874 | \n",
      "Epoch: 2598 | train_loss: 117.9185867310 | test_loss: 6.1501984596 | \n",
      "Epoch: 2599 | train_loss: 117.9184112549 | test_loss: 6.1501398087 | \n",
      "Epoch: 2600 | train_loss: 117.9181823730 | test_loss: 6.1500825882 | \n",
      "Epoch: 2601 | train_loss: 117.9179382324 | test_loss: 6.1500291824 | \n",
      "Epoch: 2602 | train_loss: 117.9177322388 | test_loss: 6.1499762535 | \n",
      "Epoch: 2603 | train_loss: 117.9175109863 | test_loss: 6.1499233246 | \n",
      "Epoch: 2604 | train_loss: 117.9172973633 | test_loss: 6.1498661041 | \n",
      "Epoch: 2605 | train_loss: 117.9170608521 | test_loss: 6.1498103142 | \n",
      "Epoch: 2606 | train_loss: 117.9168777466 | test_loss: 6.1497535706 | \n",
      "Epoch: 2607 | train_loss: 117.9166336060 | test_loss: 6.1497058868 | \n",
      "Epoch: 2608 | train_loss: 117.9164047241 | test_loss: 6.1496496201 | \n",
      "Epoch: 2609 | train_loss: 117.9161834717 | test_loss: 6.1495971680 | \n",
      "Epoch: 2610 | train_loss: 117.9159851074 | test_loss: 6.1495432854 | \n",
      "Epoch: 2611 | train_loss: 117.9157333374 | test_loss: 6.1494812965 | \n",
      "Epoch: 2612 | train_loss: 117.9155197144 | test_loss: 6.1494293213 | \n",
      "Epoch: 2613 | train_loss: 117.9152603149 | test_loss: 6.1493763924 | \n",
      "Epoch: 2614 | train_loss: 117.9150390625 | test_loss: 6.1493244171 | \n",
      "Epoch: 2615 | train_loss: 117.9148178101 | test_loss: 6.1492729187 | \n",
      "Epoch: 2616 | train_loss: 117.9145965576 | test_loss: 6.1492195129 | \n",
      "Epoch: 2617 | train_loss: 117.9143524170 | test_loss: 6.1491641998 | \n",
      "Epoch: 2618 | train_loss: 117.9141311646 | test_loss: 6.1491074562 | \n",
      "Epoch: 2619 | train_loss: 117.9139480591 | test_loss: 6.1490526199 | \n",
      "Epoch: 2620 | train_loss: 117.9137115479 | test_loss: 6.1490015984 | \n",
      "Epoch: 2621 | train_loss: 117.9134979248 | test_loss: 6.1489481926 | \n",
      "Epoch: 2622 | train_loss: 117.9132385254 | test_loss: 6.1488933563 | \n",
      "Epoch: 2623 | train_loss: 117.9129943848 | test_loss: 6.1488337517 | \n",
      "Epoch: 2624 | train_loss: 117.9127426147 | test_loss: 6.1487812996 | \n",
      "Epoch: 2625 | train_loss: 117.9125366211 | test_loss: 6.1487255096 | \n",
      "Epoch: 2626 | train_loss: 117.9123153687 | test_loss: 6.1486711502 | \n",
      "Epoch: 2627 | train_loss: 117.9121093750 | test_loss: 6.1486206055 | \n",
      "Epoch: 2628 | train_loss: 117.9118881226 | test_loss: 6.1485638618 | \n",
      "Epoch: 2629 | train_loss: 117.9116439819 | test_loss: 6.1485028267 | \n",
      "Epoch: 2630 | train_loss: 117.9114456177 | test_loss: 6.1484513283 | \n",
      "Epoch: 2631 | train_loss: 117.9111785889 | test_loss: 6.1483983994 | \n",
      "Epoch: 2632 | train_loss: 117.9109573364 | test_loss: 6.1483421326 | \n",
      "Epoch: 2633 | train_loss: 117.9107513428 | test_loss: 6.1482906342 | \n",
      "Epoch: 2634 | train_loss: 117.9105072021 | test_loss: 6.1482367516 | \n",
      "Epoch: 2635 | train_loss: 117.9102706909 | test_loss: 6.1481819153 | \n",
      "Epoch: 2636 | train_loss: 117.9100646973 | test_loss: 6.1481275558 | \n",
      "Epoch: 2637 | train_loss: 117.9098129272 | test_loss: 6.1480770111 | \n",
      "Epoch: 2638 | train_loss: 117.9095916748 | test_loss: 6.1480250359 | \n",
      "Epoch: 2639 | train_loss: 117.9093551636 | test_loss: 6.1479668617 | \n",
      "Epoch: 2640 | train_loss: 117.9091339111 | test_loss: 6.1479096413 | \n",
      "Epoch: 2641 | train_loss: 117.9088745117 | test_loss: 6.1478619576 | \n",
      "Epoch: 2642 | train_loss: 117.9086608887 | test_loss: 6.1478052139 | \n",
      "Epoch: 2643 | train_loss: 117.9084396362 | test_loss: 6.1477503777 | \n",
      "Epoch: 2644 | train_loss: 117.9082336426 | test_loss: 6.1476955414 | \n",
      "Epoch: 2645 | train_loss: 117.9080352783 | test_loss: 6.1476402283 | \n",
      "Epoch: 2646 | train_loss: 117.9077758789 | test_loss: 6.1475849152 | \n",
      "Epoch: 2647 | train_loss: 117.9075164795 | test_loss: 6.1475281715 | \n",
      "Epoch: 2648 | train_loss: 117.9072875977 | test_loss: 6.1474690437 | \n",
      "Epoch: 2649 | train_loss: 117.9070434570 | test_loss: 6.1474137306 | \n",
      "Epoch: 2650 | train_loss: 117.9068450928 | test_loss: 6.1473565102 | \n",
      "Epoch: 2651 | train_loss: 117.9065704346 | test_loss: 6.1473083496 | \n",
      "Epoch: 2652 | train_loss: 117.9063186646 | test_loss: 6.1472496986 | \n",
      "Epoch: 2653 | train_loss: 117.9061050415 | test_loss: 6.1472001076 | \n",
      "Epoch: 2654 | train_loss: 117.9058914185 | test_loss: 6.1471405029 | \n",
      "Epoch: 2655 | train_loss: 117.9056549072 | test_loss: 6.1470818520 | \n",
      "Epoch: 2656 | train_loss: 117.9054107666 | test_loss: 6.1470313072 | \n",
      "Epoch: 2657 | train_loss: 117.9051818848 | test_loss: 6.1469764709 | \n",
      "Epoch: 2658 | train_loss: 117.9049224854 | test_loss: 6.1469230652 | \n",
      "Epoch: 2659 | train_loss: 117.9046783447 | test_loss: 6.1468715668 | \n",
      "Epoch: 2660 | train_loss: 117.9045028687 | test_loss: 6.1468153000 | \n",
      "Epoch: 2661 | train_loss: 117.9042282104 | test_loss: 6.1467614174 | \n",
      "Epoch: 2662 | train_loss: 117.9040069580 | test_loss: 6.1467037201 | \n",
      "Epoch: 2663 | train_loss: 117.9037628174 | test_loss: 6.1466474533 | \n",
      "Epoch: 2664 | train_loss: 117.9035415649 | test_loss: 6.1465969086 | \n",
      "Epoch: 2665 | train_loss: 117.9033508301 | test_loss: 6.1465392113 | \n",
      "Epoch: 2666 | train_loss: 117.9030990601 | test_loss: 6.1464800835 | \n",
      "Epoch: 2667 | train_loss: 117.9028549194 | test_loss: 6.1464300156 | \n",
      "Epoch: 2668 | train_loss: 117.9026260376 | test_loss: 6.1463775635 | \n",
      "Epoch: 2669 | train_loss: 117.9023818970 | test_loss: 6.1463260651 | \n",
      "Epoch: 2670 | train_loss: 117.9021606445 | test_loss: 6.1462736130 | \n",
      "Epoch: 2671 | train_loss: 117.9019470215 | test_loss: 6.1462140083 | \n",
      "Epoch: 2672 | train_loss: 117.9017105103 | test_loss: 6.1461634636 | \n",
      "Epoch: 2673 | train_loss: 117.9014587402 | test_loss: 6.1461100578 | \n",
      "Epoch: 2674 | train_loss: 117.9012374878 | test_loss: 6.1460547447 | \n",
      "Epoch: 2675 | train_loss: 117.9009780884 | test_loss: 6.1459980011 | \n",
      "Epoch: 2676 | train_loss: 117.9007339478 | test_loss: 6.1459445953 | \n",
      "Epoch: 2677 | train_loss: 117.9005279541 | test_loss: 6.1458930969 | \n",
      "Epoch: 2678 | train_loss: 117.9002914429 | test_loss: 6.1458396912 | \n",
      "Epoch: 2679 | train_loss: 117.9000549316 | test_loss: 6.1457853317 | \n",
      "Epoch: 2680 | train_loss: 117.8998336792 | test_loss: 6.1457324028 | \n",
      "Epoch: 2681 | train_loss: 117.8995819092 | test_loss: 6.1456756592 | \n",
      "Epoch: 2682 | train_loss: 117.8993759155 | test_loss: 6.1456232071 | \n",
      "Epoch: 2683 | train_loss: 117.8991317749 | test_loss: 6.1455659866 | \n",
      "Epoch: 2684 | train_loss: 117.8988800049 | test_loss: 6.1455097198 | \n",
      "Epoch: 2685 | train_loss: 117.8986206055 | test_loss: 6.1454601288 | \n",
      "Epoch: 2686 | train_loss: 117.8983993530 | test_loss: 6.1454081535 | \n",
      "Epoch: 2687 | train_loss: 117.8981475830 | test_loss: 6.1453528404 | \n",
      "Epoch: 2688 | train_loss: 117.8979263306 | test_loss: 6.1453003883 | \n",
      "Epoch: 2689 | train_loss: 117.8977127075 | test_loss: 6.1452465057 | \n",
      "Epoch: 2690 | train_loss: 117.8974609375 | test_loss: 6.1451940536 | \n",
      "Epoch: 2691 | train_loss: 117.8972167969 | test_loss: 6.1451396942 | \n",
      "Epoch: 2692 | train_loss: 117.8970031738 | test_loss: 6.1450824738 | \n",
      "Epoch: 2693 | train_loss: 117.8967742920 | test_loss: 6.1450276375 | \n",
      "Epoch: 2694 | train_loss: 117.8965606689 | test_loss: 6.1449766159 | \n",
      "Epoch: 2695 | train_loss: 117.8963165283 | test_loss: 6.1449232101 | \n",
      "Epoch: 2696 | train_loss: 117.8960723877 | test_loss: 6.1448674202 | \n",
      "Epoch: 2697 | train_loss: 117.8958663940 | test_loss: 6.1448154449 | \n",
      "Epoch: 2698 | train_loss: 117.8956069946 | test_loss: 6.1447563171 | \n",
      "Epoch: 2699 | train_loss: 117.8953781128 | test_loss: 6.1447038651 | \n",
      "Epoch: 2700 | train_loss: 117.8951492310 | test_loss: 6.1446518898 | \n",
      "Epoch: 2701 | train_loss: 117.8949127197 | test_loss: 6.1446027756 | \n",
      "Epoch: 2702 | train_loss: 117.8946990967 | test_loss: 6.1445484161 | \n",
      "Epoch: 2703 | train_loss: 117.8944320679 | test_loss: 6.1444926262 | \n",
      "Epoch: 2704 | train_loss: 117.8941955566 | test_loss: 6.1444373131 | \n",
      "Epoch: 2705 | train_loss: 117.8939743042 | test_loss: 6.1443858147 | \n",
      "Epoch: 2706 | train_loss: 117.8937377930 | test_loss: 6.1443309784 | \n",
      "Epoch: 2707 | train_loss: 117.8935089111 | test_loss: 6.1442737579 | \n",
      "Epoch: 2708 | train_loss: 117.8932723999 | test_loss: 6.1442165375 | \n",
      "Epoch: 2709 | train_loss: 117.8930206299 | test_loss: 6.1441612244 | \n",
      "Epoch: 2710 | train_loss: 117.8927764893 | test_loss: 6.1441054344 | \n",
      "Epoch: 2711 | train_loss: 117.8925704956 | test_loss: 6.1440510750 | \n",
      "Epoch: 2712 | train_loss: 117.8923034668 | test_loss: 6.1439976692 | \n",
      "Epoch: 2713 | train_loss: 117.8920669556 | test_loss: 6.1439456940 | \n",
      "Epoch: 2714 | train_loss: 117.8918685913 | test_loss: 6.1438884735 | \n",
      "Epoch: 2715 | train_loss: 117.8916320801 | test_loss: 6.1438341141 | \n",
      "Epoch: 2716 | train_loss: 117.8913879395 | test_loss: 6.1437811852 | \n",
      "Epoch: 2717 | train_loss: 117.8911437988 | test_loss: 6.1437315941 | \n",
      "Epoch: 2718 | train_loss: 117.8909149170 | test_loss: 6.1436767578 | \n",
      "Epoch: 2719 | train_loss: 117.8906707764 | test_loss: 6.1436266899 | \n",
      "Epoch: 2720 | train_loss: 117.8904495239 | test_loss: 6.1435699463 | \n",
      "Epoch: 2721 | train_loss: 117.8901824951 | test_loss: 6.1435174942 | \n",
      "Epoch: 2722 | train_loss: 117.8899841309 | test_loss: 6.1434688568 | \n",
      "Epoch: 2723 | train_loss: 117.8897552490 | test_loss: 6.1434144974 | \n",
      "Epoch: 2724 | train_loss: 117.8895034790 | test_loss: 6.1433610916 | \n",
      "Epoch: 2725 | train_loss: 117.8892517090 | test_loss: 6.1433062553 | \n",
      "Epoch: 2726 | train_loss: 117.8890151978 | test_loss: 6.1432476044 | \n",
      "Epoch: 2727 | train_loss: 117.8887786865 | test_loss: 6.1431980133 | \n",
      "Epoch: 2728 | train_loss: 117.8885574341 | test_loss: 6.1431469917 | \n",
      "Epoch: 2729 | train_loss: 117.8883285522 | test_loss: 6.1430907249 | \n",
      "Epoch: 2730 | train_loss: 117.8880844116 | test_loss: 6.1430335045 | \n",
      "Epoch: 2731 | train_loss: 117.8878402710 | test_loss: 6.1429777145 | \n",
      "Epoch: 2732 | train_loss: 117.8875961304 | test_loss: 6.1429195404 | \n",
      "Epoch: 2733 | train_loss: 117.8873748779 | test_loss: 6.1428642273 | \n",
      "Epoch: 2734 | train_loss: 117.8871383667 | test_loss: 6.1428074837 | \n",
      "Epoch: 2735 | train_loss: 117.8868942261 | test_loss: 6.1427545547 | \n",
      "Epoch: 2736 | train_loss: 117.8866729736 | test_loss: 6.1427044868 | \n",
      "Epoch: 2737 | train_loss: 117.8864440918 | test_loss: 6.1426453590 | \n",
      "Epoch: 2738 | train_loss: 117.8861618042 | test_loss: 6.1425938606 | \n",
      "Epoch: 2739 | train_loss: 117.8859405518 | test_loss: 6.1425380707 | \n",
      "Epoch: 2740 | train_loss: 117.8856658936 | test_loss: 6.1424827576 | \n",
      "Epoch: 2741 | train_loss: 117.8854064941 | test_loss: 6.1424283981 | \n",
      "Epoch: 2742 | train_loss: 117.8851776123 | test_loss: 6.1423730850 | \n",
      "Epoch: 2743 | train_loss: 117.8849334717 | test_loss: 6.1423215866 | \n",
      "Epoch: 2744 | train_loss: 117.8846893311 | test_loss: 6.1422667503 | \n",
      "Epoch: 2745 | train_loss: 117.8844604492 | test_loss: 6.1422100067 | \n",
      "Epoch: 2746 | train_loss: 117.8842010498 | test_loss: 6.1421599388 | \n",
      "Epoch: 2747 | train_loss: 117.8839950562 | test_loss: 6.1421027184 | \n",
      "Epoch: 2748 | train_loss: 117.8837280273 | test_loss: 6.1420531273 | \n",
      "Epoch: 2749 | train_loss: 117.8834838867 | test_loss: 6.1419954300 | \n",
      "Epoch: 2750 | train_loss: 117.8832321167 | test_loss: 6.1419429779 | \n",
      "Epoch: 2751 | train_loss: 117.8829956055 | test_loss: 6.1418867111 | \n",
      "Epoch: 2752 | train_loss: 117.8827209473 | test_loss: 6.1418333054 | \n",
      "Epoch: 2753 | train_loss: 117.8824996948 | test_loss: 6.1417841911 | \n",
      "Epoch: 2754 | train_loss: 117.8822784424 | test_loss: 6.1417307854 | \n",
      "Epoch: 2755 | train_loss: 117.8820114136 | test_loss: 6.1416749954 | \n",
      "Epoch: 2756 | train_loss: 117.8817901611 | test_loss: 6.1416206360 | \n",
      "Epoch: 2757 | train_loss: 117.8815612793 | test_loss: 6.1415667534 | \n",
      "Epoch: 2758 | train_loss: 117.8813095093 | test_loss: 6.1415114403 | \n",
      "Epoch: 2759 | train_loss: 117.8810577393 | test_loss: 6.1414594650 | \n",
      "Epoch: 2760 | train_loss: 117.8808135986 | test_loss: 6.1414093971 | \n",
      "Epoch: 2761 | train_loss: 117.8805541992 | test_loss: 6.1413531303 | \n",
      "Epoch: 2762 | train_loss: 117.8802719116 | test_loss: 6.1412987709 | \n",
      "Epoch: 2763 | train_loss: 117.8800582886 | test_loss: 6.1412439346 | \n",
      "Epoch: 2764 | train_loss: 117.8798141479 | test_loss: 6.1411890984 | \n",
      "Epoch: 2765 | train_loss: 117.8795928955 | test_loss: 6.1411323547 | \n",
      "Epoch: 2766 | train_loss: 117.8793258667 | test_loss: 6.1410813332 | \n",
      "Epoch: 2767 | train_loss: 117.8790893555 | test_loss: 6.1410255432 | \n",
      "Epoch: 2768 | train_loss: 117.8788604736 | test_loss: 6.1409759521 | \n",
      "Epoch: 2769 | train_loss: 117.8786163330 | test_loss: 6.1409196854 | \n",
      "Epoch: 2770 | train_loss: 117.8784179688 | test_loss: 6.1408648491 | \n",
      "Epoch: 2771 | train_loss: 117.8781280518 | test_loss: 6.1408109665 | \n",
      "Epoch: 2772 | train_loss: 117.8779373169 | test_loss: 6.1407585144 | \n",
      "Epoch: 2773 | train_loss: 117.8777084351 | test_loss: 6.1407036781 | \n",
      "Epoch: 2774 | train_loss: 117.8774337769 | test_loss: 6.1406497955 | \n",
      "Epoch: 2775 | train_loss: 117.8772048950 | test_loss: 6.1405968666 | \n",
      "Epoch: 2776 | train_loss: 117.8769912720 | test_loss: 6.1405425072 | \n",
      "Epoch: 2777 | train_loss: 117.8767318726 | test_loss: 6.1404871941 | \n",
      "Epoch: 2778 | train_loss: 117.8764724731 | test_loss: 6.1404376030 | \n",
      "Epoch: 2779 | train_loss: 117.8762817383 | test_loss: 6.1403846741 | \n",
      "Epoch: 2780 | train_loss: 117.8759994507 | test_loss: 6.1403269768 | \n",
      "Epoch: 2781 | train_loss: 117.8757553101 | test_loss: 6.1402745247 | \n",
      "Epoch: 2782 | train_loss: 117.8755111694 | test_loss: 6.1402173042 | \n",
      "Epoch: 2783 | train_loss: 117.8752517700 | test_loss: 6.1401624680 | \n",
      "Epoch: 2784 | train_loss: 117.8749923706 | test_loss: 6.1401081085 | \n",
      "Epoch: 2785 | train_loss: 117.8747482300 | test_loss: 6.1400542259 | \n",
      "Epoch: 2786 | train_loss: 117.8745498657 | test_loss: 6.1399974823 | \n",
      "Epoch: 2787 | train_loss: 117.8742904663 | test_loss: 6.1399450302 | \n",
      "Epoch: 2788 | train_loss: 117.8740310669 | test_loss: 6.1398997307 | \n",
      "Epoch: 2789 | train_loss: 117.8737869263 | test_loss: 6.1398429871 | \n",
      "Epoch: 2790 | train_loss: 117.8735275269 | test_loss: 6.1397900581 | \n",
      "Epoch: 2791 | train_loss: 117.8732604980 | test_loss: 6.1397399902 | \n",
      "Epoch: 2792 | train_loss: 117.8730239868 | test_loss: 6.1396851540 | \n",
      "Epoch: 2793 | train_loss: 117.8727645874 | test_loss: 6.1396350861 | \n",
      "Epoch: 2794 | train_loss: 117.8725357056 | test_loss: 6.1395802498 | \n",
      "Epoch: 2795 | train_loss: 117.8722686768 | test_loss: 6.1395277977 | \n",
      "Epoch: 2796 | train_loss: 117.8720092773 | test_loss: 6.1394681931 | \n",
      "Epoch: 2797 | train_loss: 117.8717803955 | test_loss: 6.1394176483 | \n",
      "Epoch: 2798 | train_loss: 117.8715438843 | test_loss: 6.1393589973 | \n",
      "Epoch: 2799 | train_loss: 117.8712615967 | test_loss: 6.1393103600 | \n",
      "Epoch: 2800 | train_loss: 117.8710250854 | test_loss: 6.1392536163 | \n",
      "Epoch: 2801 | train_loss: 117.8707733154 | test_loss: 6.1391992569 | \n",
      "Epoch: 2802 | train_loss: 117.8704986572 | test_loss: 6.1391406059 | \n",
      "Epoch: 2803 | train_loss: 117.8702545166 | test_loss: 6.1390886307 | \n",
      "Epoch: 2804 | train_loss: 117.8700103760 | test_loss: 6.1390318871 | \n",
      "Epoch: 2805 | train_loss: 117.8697738647 | test_loss: 6.1389732361 | \n",
      "Epoch: 2806 | train_loss: 117.8695220947 | test_loss: 6.1389222145 | \n",
      "Epoch: 2807 | train_loss: 117.8692779541 | test_loss: 6.1388678551 | \n",
      "Epoch: 2808 | train_loss: 117.8690338135 | test_loss: 6.1388163567 | \n",
      "Epoch: 2809 | train_loss: 117.8687515259 | test_loss: 6.1387596130 | \n",
      "Epoch: 2810 | train_loss: 117.8684844971 | test_loss: 6.1387071609 | \n",
      "Epoch: 2811 | train_loss: 117.8682708740 | test_loss: 6.1386580467 | \n",
      "Epoch: 2812 | train_loss: 117.8680496216 | test_loss: 6.1386046410 | \n",
      "Epoch: 2813 | train_loss: 117.8677444458 | test_loss: 6.1385521889 | \n",
      "Epoch: 2814 | train_loss: 117.8675460815 | test_loss: 6.1384973526 | \n",
      "Epoch: 2815 | train_loss: 117.8672943115 | test_loss: 6.1384444237 | \n",
      "Epoch: 2816 | train_loss: 117.8670501709 | test_loss: 6.1383905411 | \n",
      "Epoch: 2817 | train_loss: 117.8668441772 | test_loss: 6.1383366585 | \n",
      "Epoch: 2818 | train_loss: 117.8665618896 | test_loss: 6.1382842064 | \n",
      "Epoch: 2819 | train_loss: 117.8663177490 | test_loss: 6.1382293701 | \n",
      "Epoch: 2820 | train_loss: 117.8661193848 | test_loss: 6.1381816864 | \n",
      "Epoch: 2821 | train_loss: 117.8658676147 | test_loss: 6.1381254196 | \n",
      "Epoch: 2822 | train_loss: 117.8655700684 | test_loss: 6.1380720139 | \n",
      "Epoch: 2823 | train_loss: 117.8653335571 | test_loss: 6.1380176544 | \n",
      "Epoch: 2824 | train_loss: 117.8650817871 | test_loss: 6.1379637718 | \n",
      "Epoch: 2825 | train_loss: 117.8648834229 | test_loss: 6.1379127502 | \n",
      "Epoch: 2826 | train_loss: 117.8646087646 | test_loss: 6.1378564835 | \n",
      "Epoch: 2827 | train_loss: 117.8643493652 | test_loss: 6.1377992630 | \n",
      "Epoch: 2828 | train_loss: 117.8640823364 | test_loss: 6.1377449036 | \n",
      "Epoch: 2829 | train_loss: 117.8638381958 | test_loss: 6.1376843452 | \n",
      "Epoch: 2830 | train_loss: 117.8635940552 | test_loss: 6.1376361847 | \n",
      "Epoch: 2831 | train_loss: 117.8633270264 | test_loss: 6.1375803947 | \n",
      "Epoch: 2832 | train_loss: 117.8630828857 | test_loss: 6.1375303268 | \n",
      "Epoch: 2833 | train_loss: 117.8628311157 | test_loss: 6.1374773979 | \n",
      "Epoch: 2834 | train_loss: 117.8625717163 | test_loss: 6.1374211311 | \n",
      "Epoch: 2835 | train_loss: 117.8623123169 | test_loss: 6.1373753548 | \n",
      "Epoch: 2836 | train_loss: 117.8621215820 | test_loss: 6.1373138428 | \n",
      "Epoch: 2837 | train_loss: 117.8618545532 | test_loss: 6.1372632980 | \n",
      "Epoch: 2838 | train_loss: 117.8616027832 | test_loss: 6.1372117996 | \n",
      "Epoch: 2839 | train_loss: 117.8613739014 | test_loss: 6.1371564865 | \n",
      "Epoch: 2840 | train_loss: 117.8611450195 | test_loss: 6.1371059418 | \n",
      "Epoch: 2841 | train_loss: 117.8608779907 | test_loss: 6.1370525360 | \n",
      "Epoch: 2842 | train_loss: 117.8606185913 | test_loss: 6.1369953156 | \n",
      "Epoch: 2843 | train_loss: 117.8603744507 | test_loss: 6.1369395256 | \n",
      "Epoch: 2844 | train_loss: 117.8600997925 | test_loss: 6.1368861198 | \n",
      "Epoch: 2845 | train_loss: 117.8598327637 | test_loss: 6.1368293762 | \n",
      "Epoch: 2846 | train_loss: 117.8595581055 | test_loss: 6.1367759705 | \n",
      "Epoch: 2847 | train_loss: 117.8593063354 | test_loss: 6.1367216110 | \n",
      "Epoch: 2848 | train_loss: 117.8590621948 | test_loss: 6.1366691589 | \n",
      "Epoch: 2849 | train_loss: 117.8588256836 | test_loss: 6.1366157532 | \n",
      "Epoch: 2850 | train_loss: 117.8585662842 | test_loss: 6.1365666389 | \n",
      "Epoch: 2851 | train_loss: 117.8583297729 | test_loss: 6.1365113258 | \n",
      "Epoch: 2852 | train_loss: 117.8581085205 | test_loss: 6.1364588737 | \n",
      "Epoch: 2853 | train_loss: 117.8578491211 | test_loss: 6.1364097595 | \n",
      "Epoch: 2854 | train_loss: 117.8575744629 | test_loss: 6.1363520622 | \n",
      "Epoch: 2855 | train_loss: 117.8572845459 | test_loss: 6.1362996101 | \n",
      "Epoch: 2856 | train_loss: 117.8570785522 | test_loss: 6.1362476349 | \n",
      "Epoch: 2857 | train_loss: 117.8568191528 | test_loss: 6.1361937523 | \n",
      "Epoch: 2858 | train_loss: 117.8565521240 | test_loss: 6.1361384392 | \n",
      "Epoch: 2859 | train_loss: 117.8563308716 | test_loss: 6.1360850334 | \n",
      "Epoch: 2860 | train_loss: 117.8560485840 | test_loss: 6.1360344887 | \n",
      "Epoch: 2861 | train_loss: 117.8558120728 | test_loss: 6.1359815598 | \n",
      "Epoch: 2862 | train_loss: 117.8555755615 | test_loss: 6.1359324455 | \n",
      "Epoch: 2863 | train_loss: 117.8553314209 | test_loss: 6.1358752251 | \n",
      "Epoch: 2864 | train_loss: 117.8550567627 | test_loss: 6.1358270645 | \n",
      "Epoch: 2865 | train_loss: 117.8548660278 | test_loss: 6.1357712746 | \n",
      "Epoch: 2866 | train_loss: 117.8545608521 | test_loss: 6.1357188225 | \n",
      "Epoch: 2867 | train_loss: 117.8543472290 | test_loss: 6.1356630325 | \n",
      "Epoch: 2868 | train_loss: 117.8540420532 | test_loss: 6.1356153488 | \n",
      "Epoch: 2869 | train_loss: 117.8538208008 | test_loss: 6.1355576515 | \n",
      "Epoch: 2870 | train_loss: 117.8535537720 | test_loss: 6.1355090141 | \n",
      "Epoch: 2871 | train_loss: 117.8533172607 | test_loss: 6.1354579926 | \n",
      "Epoch: 2872 | train_loss: 117.8530883789 | test_loss: 6.1354022026 | \n",
      "Epoch: 2873 | train_loss: 117.8527908325 | test_loss: 6.1353492737 | \n",
      "Epoch: 2874 | train_loss: 117.8525772095 | test_loss: 6.1352910995 | \n",
      "Epoch: 2875 | train_loss: 117.8523178101 | test_loss: 6.1352391243 | \n",
      "Epoch: 2876 | train_loss: 117.8520278931 | test_loss: 6.1351857185 | \n",
      "Epoch: 2877 | train_loss: 117.8517837524 | test_loss: 6.1351361275 | \n",
      "Epoch: 2878 | train_loss: 117.8515319824 | test_loss: 6.1350831985 | \n",
      "Epoch: 2879 | train_loss: 117.8512878418 | test_loss: 6.1350293159 | \n",
      "Epoch: 2880 | train_loss: 117.8510360718 | test_loss: 6.1349730492 | \n",
      "Epoch: 2881 | train_loss: 117.8507919312 | test_loss: 6.1349253654 | \n",
      "Epoch: 2882 | train_loss: 117.8505249023 | test_loss: 6.1348686218 | \n",
      "Epoch: 2883 | train_loss: 117.8502807617 | test_loss: 6.1348114014 | \n",
      "Epoch: 2884 | train_loss: 117.8500289917 | test_loss: 6.1347575188 | \n",
      "Epoch: 2885 | train_loss: 117.8497390747 | test_loss: 6.1347088814 | \n",
      "Epoch: 2886 | train_loss: 117.8494720459 | test_loss: 6.1346559525 | \n",
      "Epoch: 2887 | train_loss: 117.8492202759 | test_loss: 6.1346020699 | \n",
      "Epoch: 2888 | train_loss: 117.8489685059 | test_loss: 6.1345534325 | \n",
      "Epoch: 2889 | train_loss: 117.8487014771 | test_loss: 6.1344981194 | \n",
      "Epoch: 2890 | train_loss: 117.8484573364 | test_loss: 6.1344490051 | \n",
      "Epoch: 2891 | train_loss: 117.8481750488 | test_loss: 6.1343903542 | \n",
      "Epoch: 2892 | train_loss: 117.8479080200 | test_loss: 6.1343359947 | \n",
      "Epoch: 2893 | train_loss: 117.8476867676 | test_loss: 6.1342802048 | \n",
      "Epoch: 2894 | train_loss: 117.8474349976 | test_loss: 6.1342310905 | \n",
      "Epoch: 2895 | train_loss: 117.8471603394 | test_loss: 6.1341724396 | \n",
      "Epoch: 2896 | train_loss: 117.8468780518 | test_loss: 6.1341204643 | \n",
      "Epoch: 2897 | train_loss: 117.8465805054 | test_loss: 6.1340608597 | \n",
      "Epoch: 2898 | train_loss: 117.8463592529 | test_loss: 6.1340131760 | \n",
      "Epoch: 2899 | train_loss: 117.8461303711 | test_loss: 6.1339678764 | \n",
      "Epoch: 2900 | train_loss: 117.8458786011 | test_loss: 6.1339173317 | \n",
      "Epoch: 2901 | train_loss: 117.8456268311 | test_loss: 6.1338691711 | \n",
      "Epoch: 2902 | train_loss: 117.8453521729 | test_loss: 6.1338081360 | \n",
      "Epoch: 2903 | train_loss: 117.8451004028 | test_loss: 6.1337537766 | \n",
      "Epoch: 2904 | train_loss: 117.8448333740 | test_loss: 6.1337046623 | \n",
      "Epoch: 2905 | train_loss: 117.8445739746 | test_loss: 6.1336531639 | \n",
      "Epoch: 2906 | train_loss: 117.8443145752 | test_loss: 6.1335997581 | \n",
      "Epoch: 2907 | train_loss: 117.8440628052 | test_loss: 6.1335453987 | \n",
      "Epoch: 2908 | train_loss: 117.8437728882 | test_loss: 6.1334977150 | \n",
      "Epoch: 2909 | train_loss: 117.8435134888 | test_loss: 6.1334376335 | \n",
      "Epoch: 2910 | train_loss: 117.8432235718 | test_loss: 6.1333842278 | \n",
      "Epoch: 2911 | train_loss: 117.8429718018 | test_loss: 6.1333289146 | \n",
      "Epoch: 2912 | train_loss: 117.8427124023 | test_loss: 6.1332716942 | \n",
      "Epoch: 2913 | train_loss: 117.8424606323 | test_loss: 6.1332178116 | \n",
      "Epoch: 2914 | train_loss: 117.8421859741 | test_loss: 6.1331639290 | \n",
      "Epoch: 2915 | train_loss: 117.8418731689 | test_loss: 6.1331148148 | \n",
      "Epoch: 2916 | train_loss: 117.8416442871 | test_loss: 6.1330590248 | \n",
      "Epoch: 2917 | train_loss: 117.8413543701 | test_loss: 6.1330084801 | \n",
      "Epoch: 2918 | train_loss: 117.8411331177 | test_loss: 6.1329607964 | \n",
      "Epoch: 2919 | train_loss: 117.8408813477 | test_loss: 6.1329059601 | \n",
      "Epoch: 2920 | train_loss: 117.8406066895 | test_loss: 6.1328544617 | \n",
      "Epoch: 2921 | train_loss: 117.8403778076 | test_loss: 6.1328058243 | \n",
      "Epoch: 2922 | train_loss: 117.8401184082 | test_loss: 6.1327471733 | \n",
      "Epoch: 2923 | train_loss: 117.8398666382 | test_loss: 6.1326889992 | \n",
      "Epoch: 2924 | train_loss: 117.8395996094 | test_loss: 6.1326365471 | \n",
      "Epoch: 2925 | train_loss: 117.8393249512 | test_loss: 6.1325821877 | \n",
      "Epoch: 2926 | train_loss: 117.8391113281 | test_loss: 6.1325292587 | \n",
      "Epoch: 2927 | train_loss: 117.8388214111 | test_loss: 6.1324782372 | \n",
      "Epoch: 2928 | train_loss: 117.8385543823 | test_loss: 6.1324238777 | \n",
      "Epoch: 2929 | train_loss: 117.8382873535 | test_loss: 6.1323738098 | \n",
      "Epoch: 2930 | train_loss: 117.8380203247 | test_loss: 6.1323223114 | \n",
      "Epoch: 2931 | train_loss: 117.8377838135 | test_loss: 6.1322689056 | \n",
      "Epoch: 2932 | train_loss: 117.8375167847 | test_loss: 6.1322112083 | \n",
      "Epoch: 2933 | train_loss: 117.8372650146 | test_loss: 6.1321601868 | \n",
      "Epoch: 2934 | train_loss: 117.8370132446 | test_loss: 6.1321063042 | \n",
      "Epoch: 2935 | train_loss: 117.8367385864 | test_loss: 6.1320514679 | \n",
      "Epoch: 2936 | train_loss: 117.8364868164 | test_loss: 6.1320061684 | \n",
      "Epoch: 2937 | train_loss: 117.8362121582 | test_loss: 6.1319561005 | \n",
      "Epoch: 2938 | train_loss: 117.8359603882 | test_loss: 6.1319031715 | \n",
      "Epoch: 2939 | train_loss: 117.8356857300 | test_loss: 6.1318516731 | \n",
      "Epoch: 2940 | train_loss: 117.8354415894 | test_loss: 6.1317930222 | \n",
      "Epoch: 2941 | train_loss: 117.8351745605 | test_loss: 6.1317420006 | \n",
      "Epoch: 2942 | train_loss: 117.8349151611 | test_loss: 6.1316857338 | \n",
      "Epoch: 2943 | train_loss: 117.8346405029 | test_loss: 6.1316285133 | \n",
      "Epoch: 2944 | train_loss: 117.8343734741 | test_loss: 6.1315808296 | \n",
      "Epoch: 2945 | train_loss: 117.8340988159 | test_loss: 6.1315326691 | \n",
      "Epoch: 2946 | train_loss: 117.8338546753 | test_loss: 6.1314764023 | \n",
      "Epoch: 2947 | train_loss: 117.8336029053 | test_loss: 6.1314234734 | \n",
      "Epoch: 2948 | train_loss: 117.8333129883 | test_loss: 6.1313719749 | \n",
      "Epoch: 2949 | train_loss: 117.8330383301 | test_loss: 6.1313219070 | \n",
      "Epoch: 2950 | train_loss: 117.8327713013 | test_loss: 6.1312689781 | \n",
      "Epoch: 2951 | train_loss: 117.8325195312 | test_loss: 6.1312165260 | \n",
      "Epoch: 2952 | train_loss: 117.8322372437 | test_loss: 6.1311631203 | \n",
      "Epoch: 2953 | train_loss: 117.8320236206 | test_loss: 6.1311097145 | \n",
      "Epoch: 2954 | train_loss: 117.8317184448 | test_loss: 6.1310553551 | \n",
      "Epoch: 2955 | train_loss: 117.8314514160 | test_loss: 6.1310029030 | \n",
      "Epoch: 2956 | train_loss: 117.8312072754 | test_loss: 6.1309533119 | \n",
      "Epoch: 2957 | train_loss: 117.8309631348 | test_loss: 6.1308984756 | \n",
      "Epoch: 2958 | train_loss: 117.8307037354 | test_loss: 6.1308412552 | \n",
      "Epoch: 2959 | train_loss: 117.8304519653 | test_loss: 6.1307878494 | \n",
      "Epoch: 2960 | train_loss: 117.8301849365 | test_loss: 6.1307368279 | \n",
      "Epoch: 2961 | train_loss: 117.8298950195 | test_loss: 6.1306843758 | \n",
      "Epoch: 2962 | train_loss: 117.8296203613 | test_loss: 6.1306309700 | \n",
      "Epoch: 2963 | train_loss: 117.8293838501 | test_loss: 6.1305756569 | \n",
      "Epoch: 2964 | train_loss: 117.8291091919 | test_loss: 6.1305241585 | \n",
      "Epoch: 2965 | train_loss: 117.8288574219 | test_loss: 6.1304664612 | \n",
      "Epoch: 2966 | train_loss: 117.8285675049 | test_loss: 6.1304173470 | \n",
      "Epoch: 2967 | train_loss: 117.8283004761 | test_loss: 6.1303601265 | \n",
      "Epoch: 2968 | train_loss: 117.8280487061 | test_loss: 6.1303114891 | \n",
      "Epoch: 2969 | train_loss: 117.8277816772 | test_loss: 6.1302585602 | \n",
      "Epoch: 2970 | train_loss: 117.8275299072 | test_loss: 6.1302080154 | \n",
      "Epoch: 2971 | train_loss: 117.8272552490 | test_loss: 6.1301574707 | \n",
      "Epoch: 2972 | train_loss: 117.8269805908 | test_loss: 6.1301074028 | \n",
      "Epoch: 2973 | train_loss: 117.8267364502 | test_loss: 6.1300611496 | \n",
      "Epoch: 2974 | train_loss: 117.8264846802 | test_loss: 6.1300053596 | \n",
      "Epoch: 2975 | train_loss: 117.8262100220 | test_loss: 6.1299510002 | \n",
      "Epoch: 2976 | train_loss: 117.8259658813 | test_loss: 6.1298975945 | \n",
      "Epoch: 2977 | train_loss: 117.8257064819 | test_loss: 6.1298413277 | \n",
      "Epoch: 2978 | train_loss: 117.8254165649 | test_loss: 6.1297893524 | \n",
      "Epoch: 2979 | train_loss: 117.8251266479 | test_loss: 6.1297345161 | \n",
      "Epoch: 2980 | train_loss: 117.8248748779 | test_loss: 6.1296801567 | \n",
      "Epoch: 2981 | train_loss: 117.8246002197 | test_loss: 6.1296253204 | \n",
      "Epoch: 2982 | train_loss: 117.8243331909 | test_loss: 6.1295795441 | \n",
      "Epoch: 2983 | train_loss: 117.8240509033 | test_loss: 6.1295251846 | \n",
      "Epoch: 2984 | train_loss: 117.8237915039 | test_loss: 6.1294746399 | \n",
      "Epoch: 2985 | train_loss: 117.8235549927 | test_loss: 6.1294269562 | \n",
      "Epoch: 2986 | train_loss: 117.8232803345 | test_loss: 6.1293697357 | \n",
      "Epoch: 2987 | train_loss: 117.8230056763 | test_loss: 6.1293196678 | \n",
      "Epoch: 2988 | train_loss: 117.8227539062 | test_loss: 6.1292653084 | \n",
      "Epoch: 2989 | train_loss: 117.8224639893 | test_loss: 6.1292119026 | \n",
      "Epoch: 2990 | train_loss: 117.8222045898 | test_loss: 6.1291561127 | \n",
      "Epoch: 2991 | train_loss: 117.8219451904 | test_loss: 6.1291027069 | \n",
      "Epoch: 2992 | train_loss: 117.8216476440 | test_loss: 6.1290531158 | \n",
      "Epoch: 2993 | train_loss: 117.8213882446 | test_loss: 6.1290001869 | \n",
      "Epoch: 2994 | train_loss: 117.8211212158 | test_loss: 6.1289501190 | \n",
      "Epoch: 2995 | train_loss: 117.8208999634 | test_loss: 6.1289005280 | \n",
      "Epoch: 2996 | train_loss: 117.8206253052 | test_loss: 6.1288557053 | \n",
      "Epoch: 2997 | train_loss: 117.8203430176 | test_loss: 6.1288056374 | \n",
      "Epoch: 2998 | train_loss: 117.8200988770 | test_loss: 6.1287493706 | \n",
      "Epoch: 2999 | train_loss: 117.8197937012 | test_loss: 6.1287021637 | \n",
      "Epoch: 3000 | train_loss: 117.8195800781 | test_loss: 6.1286463737 | \n",
      "Epoch: 3001 | train_loss: 117.8192672729 | test_loss: 6.1285982132 | \n",
      "Epoch: 3002 | train_loss: 117.8190307617 | test_loss: 6.1285395622 | \n",
      "Epoch: 3003 | train_loss: 117.8187789917 | test_loss: 6.1284880638 | \n",
      "Epoch: 3004 | train_loss: 117.8185043335 | test_loss: 6.1284356117 | \n",
      "Epoch: 3005 | train_loss: 117.8182296753 | test_loss: 6.1283812523 | \n",
      "Epoch: 3006 | train_loss: 117.8179397583 | test_loss: 6.1283259392 | \n",
      "Epoch: 3007 | train_loss: 117.8177185059 | test_loss: 6.1282715797 | \n",
      "Epoch: 3008 | train_loss: 117.8174285889 | test_loss: 6.1282215118 | \n",
      "Epoch: 3009 | train_loss: 117.8171768188 | test_loss: 6.1281714439 | \n",
      "Epoch: 3010 | train_loss: 117.8169021606 | test_loss: 6.1281261444 | \n",
      "Epoch: 3011 | train_loss: 117.8166427612 | test_loss: 6.1280698776 | \n",
      "Epoch: 3012 | train_loss: 117.8163604736 | test_loss: 6.1280188560 | \n",
      "Epoch: 3013 | train_loss: 117.8161315918 | test_loss: 6.1279706955 | \n",
      "Epoch: 3014 | train_loss: 117.8158416748 | test_loss: 6.1279158592 | \n",
      "Epoch: 3015 | train_loss: 117.8155670166 | test_loss: 6.1278681755 | \n",
      "Epoch: 3016 | train_loss: 117.8152770996 | test_loss: 6.1278138161 | \n",
      "Epoch: 3017 | train_loss: 117.8150100708 | test_loss: 6.1277594566 | \n",
      "Epoch: 3018 | train_loss: 117.8147659302 | test_loss: 6.1277089119 | \n",
      "Epoch: 3019 | train_loss: 117.8144912720 | test_loss: 6.1276593208 | \n",
      "Epoch: 3020 | train_loss: 117.8142166138 | test_loss: 6.1276054382 | \n",
      "Epoch: 3021 | train_loss: 117.8139572144 | test_loss: 6.1275520325 | \n",
      "Epoch: 3022 | train_loss: 117.8136978149 | test_loss: 6.1275000572 | \n",
      "Epoch: 3023 | train_loss: 117.8134384155 | test_loss: 6.1274480820 | \n",
      "Epoch: 3024 | train_loss: 117.8131942749 | test_loss: 6.1273970604 | \n",
      "Epoch: 3025 | train_loss: 117.8128967285 | test_loss: 6.1273436546 | \n",
      "Epoch: 3026 | train_loss: 117.8126068115 | test_loss: 6.1272959709 | \n",
      "Epoch: 3027 | train_loss: 117.8123550415 | test_loss: 6.1272459030 | \n",
      "Epoch: 3028 | train_loss: 117.8120803833 | test_loss: 6.1271958351 | \n",
      "Epoch: 3029 | train_loss: 117.8117980957 | test_loss: 6.1271433830 | \n",
      "Epoch: 3030 | train_loss: 117.8114624023 | test_loss: 6.1270942688 | \n",
      "Epoch: 3031 | train_loss: 117.8112182617 | test_loss: 6.1270389557 | \n",
      "Epoch: 3032 | train_loss: 117.8109436035 | test_loss: 6.1269917488 | \n",
      "Epoch: 3033 | train_loss: 117.8106994629 | test_loss: 6.1269350052 | \n",
      "Epoch: 3034 | train_loss: 117.8104324341 | test_loss: 6.1268815994 | \n",
      "Epoch: 3035 | train_loss: 117.8101272583 | test_loss: 6.1268296242 | \n",
      "Epoch: 3036 | train_loss: 117.8098907471 | test_loss: 6.1267771721 | \n",
      "Epoch: 3037 | train_loss: 117.8096313477 | test_loss: 6.1267271042 | \n",
      "Epoch: 3038 | train_loss: 117.8093643188 | test_loss: 6.1266736984 | \n",
      "Epoch: 3039 | train_loss: 117.8090820312 | test_loss: 6.1266288757 | \n",
      "Epoch: 3040 | train_loss: 117.8088378906 | test_loss: 6.1265716553 | \n",
      "Epoch: 3041 | train_loss: 117.8085479736 | test_loss: 6.1265172958 | \n",
      "Epoch: 3042 | train_loss: 117.8082733154 | test_loss: 6.1264734268 | \n",
      "Epoch: 3043 | train_loss: 117.8080139160 | test_loss: 6.1264176369 | \n",
      "Epoch: 3044 | train_loss: 117.8077239990 | test_loss: 6.1263689995 | \n",
      "Epoch: 3045 | train_loss: 117.8074722290 | test_loss: 6.1263117790 | \n",
      "Epoch: 3046 | train_loss: 117.8071365356 | test_loss: 6.1262621880 | \n",
      "Epoch: 3047 | train_loss: 117.8068771362 | test_loss: 6.1262159348 | \n",
      "Epoch: 3048 | train_loss: 117.8066101074 | test_loss: 6.1261634827 | \n",
      "Epoch: 3049 | train_loss: 117.8063354492 | test_loss: 6.1261067390 | \n",
      "Epoch: 3050 | train_loss: 117.8060760498 | test_loss: 6.1260542870 | \n",
      "Epoch: 3051 | train_loss: 117.8057785034 | test_loss: 6.1260070801 | \n",
      "Epoch: 3052 | train_loss: 117.8055114746 | test_loss: 6.1259565353 | \n",
      "Epoch: 3053 | train_loss: 117.8052520752 | test_loss: 6.1259107590 | \n",
      "Epoch: 3054 | train_loss: 117.8049774170 | test_loss: 6.1258559227 | \n",
      "Epoch: 3055 | train_loss: 117.8047256470 | test_loss: 6.1258025169 | \n",
      "Epoch: 3056 | train_loss: 117.8044204712 | test_loss: 6.1257538795 | \n",
      "Epoch: 3057 | train_loss: 117.8041381836 | test_loss: 6.1256995201 | \n",
      "Epoch: 3058 | train_loss: 117.8038864136 | test_loss: 6.1256513596 | \n",
      "Epoch: 3059 | train_loss: 117.8035583496 | test_loss: 6.1255993843 | \n",
      "Epoch: 3060 | train_loss: 117.8033142090 | test_loss: 6.1255435944 | \n",
      "Epoch: 3061 | train_loss: 117.8030166626 | test_loss: 6.1254887581 | \n",
      "Epoch: 3062 | train_loss: 117.8027572632 | test_loss: 6.1254420280 | \n",
      "Epoch: 3063 | train_loss: 117.8024902344 | test_loss: 6.1253833771 | \n",
      "Epoch: 3064 | train_loss: 117.8022155762 | test_loss: 6.1253309250 | \n",
      "Epoch: 3065 | train_loss: 117.8019409180 | test_loss: 6.1252832413 | \n",
      "Epoch: 3066 | train_loss: 117.8016891479 | test_loss: 6.1252346039 | \n",
      "Epoch: 3067 | train_loss: 117.8014068604 | test_loss: 6.1251826286 | \n",
      "Epoch: 3068 | train_loss: 117.8011016846 | test_loss: 6.1251311302 | \n",
      "Epoch: 3069 | train_loss: 117.8008270264 | test_loss: 6.1250801086 | \n",
      "Epoch: 3070 | train_loss: 117.8005752563 | test_loss: 6.1250333786 | \n",
      "Epoch: 3071 | train_loss: 117.8002929688 | test_loss: 6.1249766350 | \n",
      "Epoch: 3072 | train_loss: 117.7999877930 | test_loss: 6.1249232292 | \n",
      "Epoch: 3073 | train_loss: 117.7996902466 | test_loss: 6.1248731613 | \n",
      "Epoch: 3074 | train_loss: 117.7994155884 | test_loss: 6.1248168945 | \n",
      "Epoch: 3075 | train_loss: 117.7991333008 | test_loss: 6.1247668266 | \n",
      "Epoch: 3076 | train_loss: 117.7988967896 | test_loss: 6.1247138977 | \n",
      "Epoch: 3077 | train_loss: 117.7986145020 | test_loss: 6.1246628761 | \n",
      "Epoch: 3078 | train_loss: 117.7983398438 | test_loss: 6.1246037483 | \n",
      "Epoch: 3079 | train_loss: 117.7980575562 | test_loss: 6.1245546341 | \n",
      "Epoch: 3080 | train_loss: 117.7977523804 | test_loss: 6.1245045662 | \n",
      "Epoch: 3081 | train_loss: 117.7975006104 | test_loss: 6.1244530678 | \n",
      "Epoch: 3082 | train_loss: 117.7972106934 | test_loss: 6.1244063377 | \n",
      "Epoch: 3083 | train_loss: 117.7969360352 | test_loss: 6.1243562698 | \n",
      "Epoch: 3084 | train_loss: 117.7966766357 | test_loss: 6.1243052483 | \n",
      "Epoch: 3085 | train_loss: 117.7964096069 | test_loss: 6.1242523193 | \n",
      "Epoch: 3086 | train_loss: 117.7961044312 | test_loss: 6.1241984367 | \n",
      "Epoch: 3087 | train_loss: 117.7958374023 | test_loss: 6.1241483688 | \n",
      "Epoch: 3088 | train_loss: 117.7955780029 | test_loss: 6.1241011620 | \n",
      "Epoch: 3089 | train_loss: 117.7953109741 | test_loss: 6.1240501404 | \n",
      "Epoch: 3090 | train_loss: 117.7950515747 | test_loss: 6.1240019798 | \n",
      "Epoch: 3091 | train_loss: 117.7947540283 | test_loss: 6.1239471436 | \n",
      "Epoch: 3092 | train_loss: 117.7944793701 | test_loss: 6.1238965988 | \n",
      "Epoch: 3093 | train_loss: 117.7941894531 | test_loss: 6.1238446236 | \n",
      "Epoch: 3094 | train_loss: 117.7939071655 | test_loss: 6.1237921715 | \n",
      "Epoch: 3095 | train_loss: 117.7936325073 | test_loss: 6.1237378120 | \n",
      "Epoch: 3096 | train_loss: 117.7933654785 | test_loss: 6.1236853600 | \n",
      "Epoch: 3097 | train_loss: 117.7930450439 | test_loss: 6.1236362457 | \n",
      "Epoch: 3098 | train_loss: 117.7927703857 | test_loss: 6.1235866547 | \n",
      "Epoch: 3099 | train_loss: 117.7925109863 | test_loss: 6.1235380173 | \n",
      "Epoch: 3100 | train_loss: 117.7922286987 | test_loss: 6.1234917641 | \n",
      "Epoch: 3101 | train_loss: 117.7919540405 | test_loss: 6.1234412193 | \n",
      "Epoch: 3102 | train_loss: 117.7916717529 | test_loss: 6.1233897209 | \n",
      "Epoch: 3103 | train_loss: 117.7913970947 | test_loss: 6.1233372688 | \n",
      "Epoch: 3104 | train_loss: 117.7911071777 | test_loss: 6.1232843399 | \n",
      "Epoch: 3105 | train_loss: 117.7908630371 | test_loss: 6.1232323647 | \n",
      "Epoch: 3106 | train_loss: 117.7905960083 | test_loss: 6.1231770515 | \n",
      "Epoch: 3107 | train_loss: 117.7902755737 | test_loss: 6.1231298447 | \n",
      "Epoch: 3108 | train_loss: 117.7900009155 | test_loss: 6.1230778694 | \n",
      "Epoch: 3109 | train_loss: 117.7897033691 | test_loss: 6.1230220795 | \n",
      "Epoch: 3110 | train_loss: 117.7894515991 | test_loss: 6.1229691505 | \n",
      "Epoch: 3111 | train_loss: 117.7891769409 | test_loss: 6.1229252815 | \n",
      "Epoch: 3112 | train_loss: 117.7889099121 | test_loss: 6.1228723526 | \n",
      "Epoch: 3113 | train_loss: 117.7886123657 | test_loss: 6.1228218079 | \n",
      "Epoch: 3114 | train_loss: 117.7883529663 | test_loss: 6.1227707863 | \n",
      "Epoch: 3115 | train_loss: 117.7880630493 | test_loss: 6.1227183342 | \n",
      "Epoch: 3116 | train_loss: 117.7877807617 | test_loss: 6.1226716042 | \n",
      "Epoch: 3117 | train_loss: 117.7875366211 | test_loss: 6.1226215363 | \n",
      "Epoch: 3118 | train_loss: 117.7872009277 | test_loss: 6.1225700378 | \n",
      "Epoch: 3119 | train_loss: 117.7869338989 | test_loss: 6.1225194931 | \n",
      "Epoch: 3120 | train_loss: 117.7867202759 | test_loss: 6.1224694252 | \n",
      "Epoch: 3121 | train_loss: 117.7864456177 | test_loss: 6.1224141121 | \n",
      "Epoch: 3122 | train_loss: 117.7861557007 | test_loss: 6.1223607063 | \n",
      "Epoch: 3123 | train_loss: 117.7858505249 | test_loss: 6.1223125458 | \n",
      "Epoch: 3124 | train_loss: 117.7855682373 | test_loss: 6.1222620010 | \n",
      "Epoch: 3125 | train_loss: 117.7852859497 | test_loss: 6.1222147942 | \n",
      "Epoch: 3126 | train_loss: 117.7850189209 | test_loss: 6.1221661568 | \n",
      "Epoch: 3127 | train_loss: 117.7847595215 | test_loss: 6.1221156120 | \n",
      "Epoch: 3128 | train_loss: 117.7844467163 | test_loss: 6.1220622063 | \n",
      "Epoch: 3129 | train_loss: 117.7841567993 | test_loss: 6.1220078468 | \n",
      "Epoch: 3130 | train_loss: 117.7838745117 | test_loss: 6.1219658852 | \n",
      "Epoch: 3131 | train_loss: 117.7835845947 | test_loss: 6.1219091415 | \n",
      "Epoch: 3132 | train_loss: 117.7833175659 | test_loss: 6.1218590736 | \n",
      "Epoch: 3133 | train_loss: 117.7830276489 | test_loss: 6.1218047142 | \n",
      "Epoch: 3134 | train_loss: 117.7827377319 | test_loss: 6.1217579842 | \n",
      "Epoch: 3135 | train_loss: 117.7824783325 | test_loss: 6.1217045784 | \n",
      "Epoch: 3136 | train_loss: 117.7822036743 | test_loss: 6.1216545105 | \n",
      "Epoch: 3137 | train_loss: 117.7819366455 | test_loss: 6.1216058731 | \n",
      "Epoch: 3138 | train_loss: 117.7816467285 | test_loss: 6.1215510368 | \n",
      "Epoch: 3139 | train_loss: 117.7813796997 | test_loss: 6.1215014458 | \n",
      "Epoch: 3140 | train_loss: 117.7810974121 | test_loss: 6.1214475632 | \n",
      "Epoch: 3141 | train_loss: 117.7807998657 | test_loss: 6.1213970184 | \n",
      "Epoch: 3142 | train_loss: 117.7805099487 | test_loss: 6.1213445663 | \n",
      "Epoch: 3143 | train_loss: 117.7802505493 | test_loss: 6.1212925911 | \n",
      "Epoch: 3144 | train_loss: 117.7799148560 | test_loss: 6.1212425232 | \n",
      "Epoch: 3145 | train_loss: 117.7796707153 | test_loss: 6.1211934090 | \n",
      "Epoch: 3146 | train_loss: 117.7794036865 | test_loss: 6.1211462021 | \n",
      "Epoch: 3147 | train_loss: 117.7791442871 | test_loss: 6.1210937500 | \n",
      "Epoch: 3148 | train_loss: 117.7788543701 | test_loss: 6.1210451126 | \n",
      "Epoch: 3149 | train_loss: 117.7785644531 | test_loss: 6.1209979057 | \n",
      "Epoch: 3150 | train_loss: 117.7782974243 | test_loss: 6.1209516525 | \n",
      "Epoch: 3151 | train_loss: 117.7780303955 | test_loss: 6.1209015846 | \n",
      "Epoch: 3152 | train_loss: 117.7777175903 | test_loss: 6.1208543777 | \n",
      "Epoch: 3153 | train_loss: 117.7774429321 | test_loss: 6.1208000183 | \n",
      "Epoch: 3154 | train_loss: 117.7771759033 | test_loss: 6.1207532883 | \n",
      "Epoch: 3155 | train_loss: 117.7768859863 | test_loss: 6.1207032204 | \n",
      "Epoch: 3156 | train_loss: 117.7766189575 | test_loss: 6.1206531525 | \n",
      "Epoch: 3157 | train_loss: 117.7763214111 | test_loss: 6.1206068993 | \n",
      "Epoch: 3158 | train_loss: 117.7760543823 | test_loss: 6.1205568314 | \n",
      "Epoch: 3159 | train_loss: 117.7757720947 | test_loss: 6.1205048561 | \n",
      "Epoch: 3160 | train_loss: 117.7754974365 | test_loss: 6.1204543114 | \n",
      "Epoch: 3161 | train_loss: 117.7752151489 | test_loss: 6.1204013824 | \n",
      "Epoch: 3162 | train_loss: 117.7748947144 | test_loss: 6.1203489304 | \n",
      "Epoch: 3163 | train_loss: 117.7746505737 | test_loss: 6.1202945709 | \n",
      "Epoch: 3164 | train_loss: 117.7743682861 | test_loss: 6.1202406883 | \n",
      "Epoch: 3165 | train_loss: 117.7740707397 | test_loss: 6.1201910973 | \n",
      "Epoch: 3166 | train_loss: 117.7737884521 | test_loss: 6.1201391220 | \n",
      "Epoch: 3167 | train_loss: 117.7734832764 | test_loss: 6.1200890541 | \n",
      "Epoch: 3168 | train_loss: 117.7732391357 | test_loss: 6.1200447083 | \n",
      "Epoch: 3169 | train_loss: 117.7729721069 | test_loss: 6.1199922562 | \n",
      "Epoch: 3170 | train_loss: 117.7726593018 | test_loss: 6.1199412346 | \n",
      "Epoch: 3171 | train_loss: 117.7723999023 | test_loss: 6.1198868752 | \n",
      "Epoch: 3172 | train_loss: 117.7721099854 | test_loss: 6.1198377609 | \n",
      "Epoch: 3173 | train_loss: 117.7717971802 | test_loss: 6.1197872162 | \n",
      "Epoch: 3174 | train_loss: 117.7715530396 | test_loss: 6.1197338104 | \n",
      "Epoch: 3175 | train_loss: 117.7712326050 | test_loss: 6.1196813583 | \n",
      "Epoch: 3176 | train_loss: 117.7709655762 | test_loss: 6.1196346283 | \n",
      "Epoch: 3177 | train_loss: 117.7706375122 | test_loss: 6.1195831299 | \n",
      "Epoch: 3178 | train_loss: 117.7703704834 | test_loss: 6.1195287704 | \n",
      "Epoch: 3179 | train_loss: 117.7700805664 | test_loss: 6.1194763184 | \n",
      "Epoch: 3180 | train_loss: 117.7698059082 | test_loss: 6.1194243431 | \n",
      "Epoch: 3181 | train_loss: 117.7695312500 | test_loss: 6.1193714142 | \n",
      "Epoch: 3182 | train_loss: 117.7692489624 | test_loss: 6.1193242073 | \n",
      "Epoch: 3183 | train_loss: 117.7689437866 | test_loss: 6.1192641258 | \n",
      "Epoch: 3184 | train_loss: 117.7686843872 | test_loss: 6.1192216873 | \n",
      "Epoch: 3185 | train_loss: 117.7684249878 | test_loss: 6.1191697121 | \n",
      "Epoch: 3186 | train_loss: 117.7681350708 | test_loss: 6.1191186905 | \n",
      "Epoch: 3187 | train_loss: 117.7678527832 | test_loss: 6.1190733910 | \n",
      "Epoch: 3188 | train_loss: 117.7675781250 | test_loss: 6.1190271378 | \n",
      "Epoch: 3189 | train_loss: 117.7673034668 | test_loss: 6.1189746857 | \n",
      "Epoch: 3190 | train_loss: 117.7670211792 | test_loss: 6.1189246178 | \n",
      "Epoch: 3191 | train_loss: 117.7667312622 | test_loss: 6.1188716888 | \n",
      "Epoch: 3192 | train_loss: 117.7664566040 | test_loss: 6.1188240051 | \n",
      "Epoch: 3193 | train_loss: 117.7661590576 | test_loss: 6.1187744141 | \n",
      "Epoch: 3194 | train_loss: 117.7658615112 | test_loss: 6.1187262535 | \n",
      "Epoch: 3195 | train_loss: 117.7655639648 | test_loss: 6.1186723709 | \n",
      "Epoch: 3196 | train_loss: 117.7652816772 | test_loss: 6.1186256409 | \n",
      "Epoch: 3197 | train_loss: 117.7649688721 | test_loss: 6.1185755730 | \n",
      "Epoch: 3198 | train_loss: 117.7647094727 | test_loss: 6.1185240746 | \n",
      "Epoch: 3199 | train_loss: 117.7644653320 | test_loss: 6.1184716225 | \n",
      "Epoch: 3200 | train_loss: 117.7641601562 | test_loss: 6.1184220314 | \n",
      "Epoch: 3201 | train_loss: 117.7638778687 | test_loss: 6.1183753014 | \n",
      "Epoch: 3202 | train_loss: 117.7635650635 | test_loss: 6.1183247566 | \n",
      "Epoch: 3203 | train_loss: 117.7632751465 | test_loss: 6.1182703972 | \n",
      "Epoch: 3204 | train_loss: 117.7630157471 | test_loss: 6.1182169914 | \n",
      "Epoch: 3205 | train_loss: 117.7627410889 | test_loss: 6.1181669235 | \n",
      "Epoch: 3206 | train_loss: 117.7624740601 | test_loss: 6.1181201935 | \n",
      "Epoch: 3207 | train_loss: 117.7621841431 | test_loss: 6.1180686951 | \n",
      "Epoch: 3208 | train_loss: 117.7618942261 | test_loss: 6.1180195808 | \n",
      "Epoch: 3209 | train_loss: 117.7616195679 | test_loss: 6.1179704666 | \n",
      "Epoch: 3210 | train_loss: 117.7613296509 | test_loss: 6.1179237366 | \n",
      "Epoch: 3211 | train_loss: 117.7610168457 | test_loss: 6.1178736687 | \n",
      "Epoch: 3212 | train_loss: 117.7607650757 | test_loss: 6.1178288460 | \n",
      "Epoch: 3213 | train_loss: 117.7604751587 | test_loss: 6.1177730560 | \n",
      "Epoch: 3214 | train_loss: 117.7602233887 | test_loss: 6.1177177429 | \n",
      "Epoch: 3215 | train_loss: 117.7599182129 | test_loss: 6.1176686287 | \n",
      "Epoch: 3216 | train_loss: 117.7596206665 | test_loss: 6.1176176071 | \n",
      "Epoch: 3217 | train_loss: 117.7593154907 | test_loss: 6.1175708771 | \n",
      "Epoch: 3218 | train_loss: 117.7590637207 | test_loss: 6.1175165176 | \n",
      "Epoch: 3219 | train_loss: 117.7587890625 | test_loss: 6.1174707413 | \n",
      "Epoch: 3220 | train_loss: 117.7585144043 | test_loss: 6.1174182892 | \n",
      "Epoch: 3221 | train_loss: 117.7582397461 | test_loss: 6.1173644066 | \n",
      "Epoch: 3222 | train_loss: 117.7579574585 | test_loss: 6.1173200607 | \n",
      "Epoch: 3223 | train_loss: 117.7576828003 | test_loss: 6.1172699928 | \n",
      "Epoch: 3224 | train_loss: 117.7574157715 | test_loss: 6.1172208786 | \n",
      "Epoch: 3225 | train_loss: 117.7570877075 | test_loss: 6.1171722412 | \n",
      "Epoch: 3226 | train_loss: 117.7568130493 | test_loss: 6.1171207428 | \n",
      "Epoch: 3227 | train_loss: 117.7565231323 | test_loss: 6.1170701981 | \n",
      "Epoch: 3228 | train_loss: 117.7562713623 | test_loss: 6.1170167923 | \n",
      "Epoch: 3229 | train_loss: 117.7560348511 | test_loss: 6.1169667244 | \n",
      "Epoch: 3230 | train_loss: 117.7557601929 | test_loss: 6.1169209480 | \n",
      "Epoch: 3231 | train_loss: 117.7554702759 | test_loss: 6.1168766022 | \n",
      "Epoch: 3232 | train_loss: 117.7551574707 | test_loss: 6.1168236732 | \n",
      "Epoch: 3233 | train_loss: 117.7548599243 | test_loss: 6.1167707443 | \n",
      "Epoch: 3234 | train_loss: 117.7545776367 | test_loss: 6.1167187691 | \n",
      "Epoch: 3235 | train_loss: 117.7542953491 | test_loss: 6.1166667938 | \n",
      "Epoch: 3236 | train_loss: 117.7540206909 | test_loss: 6.1166214943 | \n",
      "Epoch: 3237 | train_loss: 117.7537231445 | test_loss: 6.1165647507 | \n",
      "Epoch: 3238 | train_loss: 117.7534408569 | test_loss: 6.1165189743 | \n",
      "Epoch: 3239 | train_loss: 117.7531433105 | test_loss: 6.1164665222 | \n",
      "Epoch: 3240 | train_loss: 117.7528533936 | test_loss: 6.1164207458 | \n",
      "Epoch: 3241 | train_loss: 117.7525405884 | test_loss: 6.1163730621 | \n",
      "Epoch: 3242 | train_loss: 117.7522583008 | test_loss: 6.1163244247 | \n",
      "Epoch: 3243 | train_loss: 117.7519989014 | test_loss: 6.1162800789 | \n",
      "Epoch: 3244 | train_loss: 117.7517013550 | test_loss: 6.1162276268 | \n",
      "Epoch: 3245 | train_loss: 117.7514343262 | test_loss: 6.1161766052 | \n",
      "Epoch: 3246 | train_loss: 117.7511749268 | test_loss: 6.1161241531 | \n",
      "Epoch: 3247 | train_loss: 117.7508621216 | test_loss: 6.1160697937 | \n",
      "Epoch: 3248 | train_loss: 117.7505874634 | test_loss: 6.1160182953 | \n",
      "Epoch: 3249 | train_loss: 117.7503356934 | test_loss: 6.1159715652 | \n",
      "Epoch: 3250 | train_loss: 117.7500228882 | test_loss: 6.1159229279 | \n",
      "Epoch: 3251 | train_loss: 117.7497329712 | test_loss: 6.1158714294 | \n",
      "Epoch: 3252 | train_loss: 117.7494735718 | test_loss: 6.1158237457 | \n",
      "Epoch: 3253 | train_loss: 117.7492218018 | test_loss: 6.1157684326 | \n",
      "Epoch: 3254 | train_loss: 117.7489318848 | test_loss: 6.1157183647 | \n",
      "Epoch: 3255 | train_loss: 117.7486419678 | test_loss: 6.1156673431 | \n",
      "Epoch: 3256 | train_loss: 117.7483367920 | test_loss: 6.1156187057 | \n",
      "Epoch: 3257 | train_loss: 117.7480773926 | test_loss: 6.1155705452 | \n",
      "Epoch: 3258 | train_loss: 117.7478332520 | test_loss: 6.1155200005 | \n",
      "Epoch: 3259 | train_loss: 117.7475662231 | test_loss: 6.1154704094 | \n",
      "Epoch: 3260 | train_loss: 117.7472915649 | test_loss: 6.1154189110 | \n",
      "Epoch: 3261 | train_loss: 117.7470016479 | test_loss: 6.1153702736 | \n",
      "Epoch: 3262 | train_loss: 117.7467193604 | test_loss: 6.1153192520 | \n",
      "Epoch: 3263 | train_loss: 117.7464294434 | test_loss: 6.1152706146 | \n",
      "Epoch: 3264 | train_loss: 117.7461624146 | test_loss: 6.1152238846 | \n",
      "Epoch: 3265 | train_loss: 117.7458572388 | test_loss: 6.1151714325 | \n",
      "Epoch: 3266 | train_loss: 117.7455673218 | test_loss: 6.1151218414 | \n",
      "Epoch: 3267 | train_loss: 117.7452926636 | test_loss: 6.1150732040 | \n",
      "Epoch: 3268 | train_loss: 117.7450103760 | test_loss: 6.1150255203 | \n",
      "Epoch: 3269 | train_loss: 117.7447052002 | test_loss: 6.1149721146 | \n",
      "Epoch: 3270 | train_loss: 117.7443542480 | test_loss: 6.1149230003 | \n",
      "Epoch: 3271 | train_loss: 117.7440719604 | test_loss: 6.1148738861 | \n",
      "Epoch: 3272 | train_loss: 117.7437973022 | test_loss: 6.1148281097 | \n",
      "Epoch: 3273 | train_loss: 117.7435073853 | test_loss: 6.1147809029 | \n",
      "Epoch: 3274 | train_loss: 117.7432250977 | test_loss: 6.1147289276 | \n",
      "Epoch: 3275 | train_loss: 117.7429199219 | test_loss: 6.1146783829 | \n",
      "Epoch: 3276 | train_loss: 117.7426147461 | test_loss: 6.1146254539 | \n",
      "Epoch: 3277 | train_loss: 117.7423248291 | test_loss: 6.1145782471 | \n",
      "Epoch: 3278 | train_loss: 117.7420730591 | test_loss: 6.1145339012 | \n",
      "Epoch: 3279 | train_loss: 117.7417755127 | test_loss: 6.1144819260 | \n",
      "Epoch: 3280 | train_loss: 117.7415313721 | test_loss: 6.1144280434 | \n",
      "Epoch: 3281 | train_loss: 117.7412414551 | test_loss: 6.1143779755 | \n",
      "Epoch: 3282 | train_loss: 117.7409439087 | test_loss: 6.1143274307 | \n",
      "Epoch: 3283 | train_loss: 117.7406539917 | test_loss: 6.1142745018 | \n",
      "Epoch: 3284 | train_loss: 117.7403793335 | test_loss: 6.1142287254 | \n",
      "Epoch: 3285 | train_loss: 117.7401199341 | test_loss: 6.1141829491 | \n",
      "Epoch: 3286 | train_loss: 117.7398223877 | test_loss: 6.1141290665 | \n",
      "Epoch: 3287 | train_loss: 117.7395019531 | test_loss: 6.1140804291 | \n",
      "Epoch: 3288 | train_loss: 117.7392272949 | test_loss: 6.1140232086 | \n",
      "Epoch: 3289 | train_loss: 117.7389221191 | test_loss: 6.1139736176 | \n",
      "Epoch: 3290 | train_loss: 117.7386550903 | test_loss: 6.1139221191 | \n",
      "Epoch: 3291 | train_loss: 117.7383880615 | test_loss: 6.1138691902 | \n",
      "Epoch: 3292 | train_loss: 117.7381057739 | test_loss: 6.1138176918 | \n",
      "Epoch: 3293 | train_loss: 117.7378234863 | test_loss: 6.1137752533 | \n",
      "Epoch: 3294 | train_loss: 117.7375411987 | test_loss: 6.1137280464 | \n",
      "Epoch: 3295 | train_loss: 117.7372436523 | test_loss: 6.1136775017 | \n",
      "Epoch: 3296 | train_loss: 117.7369613647 | test_loss: 6.1136293411 | \n",
      "Epoch: 3297 | train_loss: 117.7366790771 | test_loss: 6.1135835648 | \n",
      "Epoch: 3298 | train_loss: 117.7363967896 | test_loss: 6.1135301590 | \n",
      "Epoch: 3299 | train_loss: 117.7361068726 | test_loss: 6.1134796143 | \n",
      "Epoch: 3300 | train_loss: 117.7358474731 | test_loss: 6.1134338379 | \n",
      "Epoch: 3301 | train_loss: 117.7355575562 | test_loss: 6.1133866310 | \n",
      "Epoch: 3302 | train_loss: 117.7352752686 | test_loss: 6.1133351326 | \n",
      "Epoch: 3303 | train_loss: 117.7350006104 | test_loss: 6.1132874489 | \n",
      "Epoch: 3304 | train_loss: 117.7346649170 | test_loss: 6.1132326126 | \n",
      "Epoch: 3305 | train_loss: 117.7343750000 | test_loss: 6.1131839752 | \n",
      "Epoch: 3306 | train_loss: 117.7340927124 | test_loss: 6.1131358147 | \n",
      "Epoch: 3307 | train_loss: 117.7337951660 | test_loss: 6.1130762100 | \n",
      "Epoch: 3308 | train_loss: 117.7335128784 | test_loss: 6.1130299568 | \n",
      "Epoch: 3309 | train_loss: 117.7332458496 | test_loss: 6.1129789352 | \n",
      "Epoch: 3310 | train_loss: 117.7329330444 | test_loss: 6.1129317284 | \n",
      "Epoch: 3311 | train_loss: 117.7326278687 | test_loss: 6.1128807068 | \n",
      "Epoch: 3312 | train_loss: 117.7323608398 | test_loss: 6.1128363609 | \n",
      "Epoch: 3313 | train_loss: 117.7320861816 | test_loss: 6.1127820015 | \n",
      "Epoch: 3314 | train_loss: 117.7318038940 | test_loss: 6.1127319336 | \n",
      "Epoch: 3315 | train_loss: 117.7315521240 | test_loss: 6.1126818657 | \n",
      "Epoch: 3316 | train_loss: 117.7312774658 | test_loss: 6.1126294136 | \n",
      "Epoch: 3317 | train_loss: 117.7309951782 | test_loss: 6.1125779152 | \n",
      "Epoch: 3318 | train_loss: 117.7307128906 | test_loss: 6.1125311852 | \n",
      "Epoch: 3319 | train_loss: 117.7304611206 | test_loss: 6.1124782562 | \n",
      "Epoch: 3320 | train_loss: 117.7301864624 | test_loss: 6.1124296188 | \n",
      "Epoch: 3321 | train_loss: 117.7298812866 | test_loss: 6.1123790741 | \n",
      "Epoch: 3322 | train_loss: 117.7295913696 | test_loss: 6.1123304367 | \n",
      "Epoch: 3323 | train_loss: 117.7293167114 | test_loss: 6.1122813225 | \n",
      "Epoch: 3324 | train_loss: 117.7290039062 | test_loss: 6.1122307777 | \n",
      "Epoch: 3325 | train_loss: 117.7287216187 | test_loss: 6.1121821404 | \n",
      "Epoch: 3326 | train_loss: 117.7284698486 | test_loss: 6.1121306419 | \n",
      "Epoch: 3327 | train_loss: 117.7281417847 | test_loss: 6.1120843887 | \n",
      "Epoch: 3328 | train_loss: 117.7278594971 | test_loss: 6.1120338440 | \n",
      "Epoch: 3329 | train_loss: 117.7275695801 | test_loss: 6.1119880676 | \n",
      "Epoch: 3330 | train_loss: 117.7272949219 | test_loss: 6.1119408607 | \n",
      "Epoch: 3331 | train_loss: 117.7270278931 | test_loss: 6.1118845940 | \n",
      "Epoch: 3332 | train_loss: 117.7267379761 | test_loss: 6.1118330956 | \n",
      "Epoch: 3333 | train_loss: 117.7264709473 | test_loss: 6.1117839813 | \n",
      "Epoch: 3334 | train_loss: 117.7261886597 | test_loss: 6.1117358208 | \n",
      "Epoch: 3335 | train_loss: 117.7258987427 | test_loss: 6.1116857529 | \n",
      "Epoch: 3336 | train_loss: 117.7256317139 | test_loss: 6.1116394997 | \n",
      "Epoch: 3337 | train_loss: 117.7253417969 | test_loss: 6.1115870476 | \n",
      "Epoch: 3338 | train_loss: 117.7250366211 | test_loss: 6.1115355492 | \n",
      "Epoch: 3339 | train_loss: 117.7247543335 | test_loss: 6.1114830971 | \n",
      "Epoch: 3340 | train_loss: 117.7244567871 | test_loss: 6.1114287376 | \n",
      "Epoch: 3341 | train_loss: 117.7241592407 | test_loss: 6.1113786697 | \n",
      "Epoch: 3342 | train_loss: 117.7238693237 | test_loss: 6.1113276482 | \n",
      "Epoch: 3343 | train_loss: 117.7235946655 | test_loss: 6.1112713814 | \n",
      "Epoch: 3344 | train_loss: 117.7232971191 | test_loss: 6.1112270355 | \n",
      "Epoch: 3345 | train_loss: 117.7230300903 | test_loss: 6.1111755371 | \n",
      "Epoch: 3346 | train_loss: 117.7227325439 | test_loss: 6.1111268997 | \n",
      "Epoch: 3347 | train_loss: 117.7224349976 | test_loss: 6.1110768318 | \n",
      "Epoch: 3348 | train_loss: 117.7221450806 | test_loss: 6.1110301018 | \n",
      "Epoch: 3349 | train_loss: 117.7218933105 | test_loss: 6.1109800339 | \n",
      "Epoch: 3350 | train_loss: 117.7216186523 | test_loss: 6.1109375954 | \n",
      "Epoch: 3351 | train_loss: 117.7213363647 | test_loss: 6.1108832359 | \n",
      "Epoch: 3352 | train_loss: 117.7210540771 | test_loss: 6.1108350754 | \n",
      "Epoch: 3353 | train_loss: 117.7207717896 | test_loss: 6.1107897758 | \n",
      "Epoch: 3354 | train_loss: 117.7204895020 | test_loss: 6.1107368469 | \n",
      "Epoch: 3355 | train_loss: 117.7202224731 | test_loss: 6.1106824875 | \n",
      "Epoch: 3356 | train_loss: 117.7199401855 | test_loss: 6.1106300354 | \n",
      "Epoch: 3357 | train_loss: 117.7196350098 | test_loss: 6.1105833054 | \n",
      "Epoch: 3358 | train_loss: 117.7193679810 | test_loss: 6.1105322838 | \n",
      "Epoch: 3359 | train_loss: 117.7190628052 | test_loss: 6.1104850769 | \n",
      "Epoch: 3360 | train_loss: 117.7188110352 | test_loss: 6.1104278564 | \n",
      "Epoch: 3361 | train_loss: 117.7185287476 | test_loss: 6.1103820801 | \n",
      "Epoch: 3362 | train_loss: 117.7182388306 | test_loss: 6.1103258133 | \n",
      "Epoch: 3363 | train_loss: 117.7179794312 | test_loss: 6.1102790833 | \n",
      "Epoch: 3364 | train_loss: 117.7176818848 | test_loss: 6.1102237701 | \n",
      "Epoch: 3365 | train_loss: 117.7174224854 | test_loss: 6.1101741791 | \n",
      "Epoch: 3366 | train_loss: 117.7171554565 | test_loss: 6.1101222038 | \n",
      "Epoch: 3367 | train_loss: 117.7168426514 | test_loss: 6.1100707054 | \n",
      "Epoch: 3368 | train_loss: 117.7165451050 | test_loss: 6.1100230217 | \n",
      "Epoch: 3369 | train_loss: 117.7162704468 | test_loss: 6.1099758148 | \n",
      "Epoch: 3370 | train_loss: 117.7159881592 | test_loss: 6.1099219322 | \n",
      "Epoch: 3371 | train_loss: 117.7157211304 | test_loss: 6.1098756790 | \n",
      "Epoch: 3372 | train_loss: 117.7154006958 | test_loss: 6.1098279953 | \n",
      "Epoch: 3373 | train_loss: 117.7151260376 | test_loss: 6.1097760201 | \n",
      "Epoch: 3374 | train_loss: 117.7148666382 | test_loss: 6.1097211838 | \n",
      "Epoch: 3375 | train_loss: 117.7146224976 | test_loss: 6.1096773148 | \n",
      "Epoch: 3376 | train_loss: 117.7143554688 | test_loss: 6.1096296310 | \n",
      "Epoch: 3377 | train_loss: 117.7140655518 | test_loss: 6.1095795631 | \n",
      "Epoch: 3378 | train_loss: 117.7138214111 | test_loss: 6.1095261574 | \n",
      "Epoch: 3379 | train_loss: 117.7134857178 | test_loss: 6.1094732285 | \n",
      "Epoch: 3380 | train_loss: 117.7132034302 | test_loss: 6.1094202995 | \n",
      "Epoch: 3381 | train_loss: 117.7129287720 | test_loss: 6.1093726158 | \n",
      "Epoch: 3382 | train_loss: 117.7126388550 | test_loss: 6.1093244553 | \n",
      "Epoch: 3383 | train_loss: 117.7123565674 | test_loss: 6.1092762947 | \n",
      "Epoch: 3384 | train_loss: 117.7120666504 | test_loss: 6.1092243195 | \n",
      "Epoch: 3385 | train_loss: 117.7117767334 | test_loss: 6.1091742516 | \n",
      "Epoch: 3386 | train_loss: 117.7114944458 | test_loss: 6.1091332436 | \n",
      "Epoch: 3387 | train_loss: 117.7112045288 | test_loss: 6.1090784073 | \n",
      "Epoch: 3388 | train_loss: 117.7109069824 | test_loss: 6.1090259552 | \n",
      "Epoch: 3389 | train_loss: 117.7106475830 | test_loss: 6.1089677811 | \n",
      "Epoch: 3390 | train_loss: 117.7103729248 | test_loss: 6.1089191437 | \n",
      "Epoch: 3391 | train_loss: 117.7100753784 | test_loss: 6.1088633537 | \n",
      "Epoch: 3392 | train_loss: 117.7097778320 | test_loss: 6.1088166237 | \n",
      "Epoch: 3393 | train_loss: 117.7095184326 | test_loss: 6.1087660789 | \n",
      "Epoch: 3394 | train_loss: 117.7092514038 | test_loss: 6.1087141037 | \n",
      "Epoch: 3395 | train_loss: 117.7089462280 | test_loss: 6.1086635590 | \n",
      "Epoch: 3396 | train_loss: 117.7086639404 | test_loss: 6.1086106300 | \n",
      "Epoch: 3397 | train_loss: 117.7083816528 | test_loss: 6.1085605621 | \n",
      "Epoch: 3398 | train_loss: 117.7081146240 | test_loss: 6.1085057259 | \n",
      "Epoch: 3399 | train_loss: 117.7077941895 | test_loss: 6.1084594727 | \n",
      "Epoch: 3400 | train_loss: 117.7075195312 | test_loss: 6.1084074974 | \n",
      "Epoch: 3401 | train_loss: 117.7072448730 | test_loss: 6.1083655357 | \n",
      "Epoch: 3402 | train_loss: 117.7069702148 | test_loss: 6.1083197594 | \n",
      "Epoch: 3403 | train_loss: 117.7066955566 | test_loss: 6.1082701683 | \n",
      "Epoch: 3404 | train_loss: 117.7063903809 | test_loss: 6.1082148552 | \n",
      "Epoch: 3405 | train_loss: 117.7061233521 | test_loss: 6.1081633568 | \n",
      "Epoch: 3406 | train_loss: 117.7058105469 | test_loss: 6.1081166267 | \n",
      "Epoch: 3407 | train_loss: 117.7055358887 | test_loss: 6.1080656052 | \n",
      "Epoch: 3408 | train_loss: 117.7052307129 | test_loss: 6.1080112457 | \n",
      "Epoch: 3409 | train_loss: 117.7049636841 | test_loss: 6.1079626083 | \n",
      "Epoch: 3410 | train_loss: 117.7046890259 | test_loss: 6.1079101562 | \n",
      "Epoch: 3411 | train_loss: 117.7044067383 | test_loss: 6.1078691483 | \n",
      "Epoch: 3412 | train_loss: 117.7041473389 | test_loss: 6.1078200340 | \n",
      "Epoch: 3413 | train_loss: 117.7038345337 | test_loss: 6.1077642441 | \n",
      "Epoch: 3414 | train_loss: 117.7035369873 | test_loss: 6.1077175140 | \n",
      "Epoch: 3415 | train_loss: 117.7032928467 | test_loss: 6.1076683998 | \n",
      "Epoch: 3416 | train_loss: 117.7030181885 | test_loss: 6.1076159477 | \n",
      "Epoch: 3417 | train_loss: 117.7027359009 | test_loss: 6.1075639725 | \n",
      "Epoch: 3418 | train_loss: 117.7024307251 | test_loss: 6.1075119972 | \n",
      "Epoch: 3419 | train_loss: 117.7021789551 | test_loss: 6.1074614525 | \n",
      "Epoch: 3420 | train_loss: 117.7018737793 | test_loss: 6.1074156761 | \n",
      "Epoch: 3421 | train_loss: 117.7016143799 | test_loss: 6.1073603630 | \n",
      "Epoch: 3422 | train_loss: 117.7013397217 | test_loss: 6.1073112488 | \n",
      "Epoch: 3423 | train_loss: 117.7010345459 | test_loss: 6.1072630882 | \n",
      "Epoch: 3424 | train_loss: 117.7007446289 | test_loss: 6.1072087288 | \n",
      "Epoch: 3425 | train_loss: 117.7004699707 | test_loss: 6.1071586609 | \n",
      "Epoch: 3426 | train_loss: 117.7001647949 | test_loss: 6.1071038246 | \n",
      "Epoch: 3427 | train_loss: 117.6998596191 | test_loss: 6.1070575714 | \n",
      "Epoch: 3428 | train_loss: 117.6996078491 | test_loss: 6.1070070267 | \n",
      "Epoch: 3429 | train_loss: 117.6993331909 | test_loss: 6.1069583893 | \n",
      "Epoch: 3430 | train_loss: 117.6990509033 | test_loss: 6.1069011688 | \n",
      "Epoch: 3431 | train_loss: 117.6988220215 | test_loss: 6.1068525314 | \n",
      "Epoch: 3432 | train_loss: 117.6985168457 | test_loss: 6.1068034172 | \n",
      "Epoch: 3433 | train_loss: 117.6982498169 | test_loss: 6.1067571640 | \n",
      "Epoch: 3434 | train_loss: 117.6979370117 | test_loss: 6.1067008972 | \n",
      "Epoch: 3435 | train_loss: 117.6976776123 | test_loss: 6.1066503525 | \n",
      "Epoch: 3436 | train_loss: 117.6973724365 | test_loss: 6.1065998077 | \n",
      "Epoch: 3437 | train_loss: 117.6970977783 | test_loss: 6.1065516472 | \n",
      "Epoch: 3438 | train_loss: 117.6968154907 | test_loss: 6.1065006256 | \n",
      "Epoch: 3439 | train_loss: 117.6965560913 | test_loss: 6.1064448357 | \n",
      "Epoch: 3440 | train_loss: 117.6962432861 | test_loss: 6.1063971519 | \n",
      "Epoch: 3441 | train_loss: 117.6959381104 | test_loss: 6.1063475609 | \n",
      "Epoch: 3442 | train_loss: 117.6956863403 | test_loss: 6.1062932014 | \n",
      "Epoch: 3443 | train_loss: 117.6953735352 | test_loss: 6.1062426567 | \n",
      "Epoch: 3444 | train_loss: 117.6950912476 | test_loss: 6.1061949730 | \n",
      "Epoch: 3445 | train_loss: 117.6948013306 | test_loss: 6.1061463356 | \n",
      "Epoch: 3446 | train_loss: 117.6945571899 | test_loss: 6.1060957909 | \n",
      "Epoch: 3447 | train_loss: 117.6942214966 | test_loss: 6.1060414314 | \n",
      "Epoch: 3448 | train_loss: 117.6939620972 | test_loss: 6.1059947014 | \n",
      "Epoch: 3449 | train_loss: 117.6936798096 | test_loss: 6.1059436798 | \n",
      "Epoch: 3450 | train_loss: 117.6933975220 | test_loss: 6.1058921814 | \n",
      "Epoch: 3451 | train_loss: 117.6931152344 | test_loss: 6.1058387756 | \n",
      "Epoch: 3452 | train_loss: 117.6928024292 | test_loss: 6.1057877541 | \n",
      "Epoch: 3453 | train_loss: 117.6925354004 | test_loss: 6.1057438850 | \n",
      "Epoch: 3454 | train_loss: 117.6922454834 | test_loss: 6.1056914330 | \n",
      "Epoch: 3455 | train_loss: 117.6919555664 | test_loss: 6.1056394577 | \n",
      "Epoch: 3456 | train_loss: 117.6916198730 | test_loss: 6.1055860519 | \n",
      "Epoch: 3457 | train_loss: 117.6913681030 | test_loss: 6.1055359840 | \n",
      "Epoch: 3458 | train_loss: 117.6910858154 | test_loss: 6.1054868698 | \n",
      "Epoch: 3459 | train_loss: 117.6908111572 | test_loss: 6.1054296494 | \n",
      "Epoch: 3460 | train_loss: 117.6905441284 | test_loss: 6.1053857803 | \n",
      "Epoch: 3461 | train_loss: 117.6902847290 | test_loss: 6.1053328514 | \n",
      "Epoch: 3462 | train_loss: 117.6900024414 | test_loss: 6.1052775383 | \n",
      "Epoch: 3463 | train_loss: 117.6897201538 | test_loss: 6.1052269936 | \n",
      "Epoch: 3464 | train_loss: 117.6894302368 | test_loss: 6.1051802635 | \n",
      "Epoch: 3465 | train_loss: 117.6891632080 | test_loss: 6.1051306725 | \n",
      "Epoch: 3466 | train_loss: 117.6888504028 | test_loss: 6.1050810814 | \n",
      "Epoch: 3467 | train_loss: 117.6885452271 | test_loss: 6.1050319672 | \n",
      "Epoch: 3468 | train_loss: 117.6882705688 | test_loss: 6.1049818993 | \n",
      "Epoch: 3469 | train_loss: 117.6880111694 | test_loss: 6.1049318314 | \n",
      "Epoch: 3470 | train_loss: 117.6877212524 | test_loss: 6.1048784256 | \n",
      "Epoch: 3471 | train_loss: 117.6874771118 | test_loss: 6.1048326492 | \n",
      "Epoch: 3472 | train_loss: 117.6871871948 | test_loss: 6.1047835350 | \n",
      "Epoch: 3473 | train_loss: 117.6868820190 | test_loss: 6.1047301292 | \n",
      "Epoch: 3474 | train_loss: 117.6866149902 | test_loss: 6.1046748161 | \n",
      "Epoch: 3475 | train_loss: 117.6863250732 | test_loss: 6.1046257019 | \n",
      "Epoch: 3476 | train_loss: 117.6860427856 | test_loss: 6.1045775414 | \n",
      "Epoch: 3477 | train_loss: 117.6857681274 | test_loss: 6.1045255661 | \n",
      "Epoch: 3478 | train_loss: 117.6854705811 | test_loss: 6.1044783592 | \n",
      "Epoch: 3479 | train_loss: 117.6852035522 | test_loss: 6.1044225693 | \n",
      "Epoch: 3480 | train_loss: 117.6849212646 | test_loss: 6.1043686867 | \n",
      "Epoch: 3481 | train_loss: 117.6846618652 | test_loss: 6.1043224335 | \n",
      "Epoch: 3482 | train_loss: 117.6843566895 | test_loss: 6.1042737961 | \n",
      "Epoch: 3483 | train_loss: 117.6840896606 | test_loss: 6.1042256355 | \n",
      "Epoch: 3484 | train_loss: 117.6838302612 | test_loss: 6.1041717529 | \n",
      "Epoch: 3485 | train_loss: 117.6835327148 | test_loss: 6.1041231155 | \n",
      "Epoch: 3486 | train_loss: 117.6832351685 | test_loss: 6.1040697098 | \n",
      "Epoch: 3487 | train_loss: 117.6829605103 | test_loss: 6.1040177345 | \n",
      "Epoch: 3488 | train_loss: 117.6826782227 | test_loss: 6.1039690971 | \n",
      "Epoch: 3489 | train_loss: 117.6823959351 | test_loss: 6.1039190292 | \n",
      "Epoch: 3490 | train_loss: 117.6821289062 | test_loss: 6.1038694382 | \n",
      "Epoch: 3491 | train_loss: 117.6818466187 | test_loss: 6.1038179398 | \n",
      "Epoch: 3492 | train_loss: 117.6815490723 | test_loss: 6.1037640572 | \n",
      "Epoch: 3493 | train_loss: 117.6812515259 | test_loss: 6.1037182808 | \n",
      "Epoch: 3494 | train_loss: 117.6809997559 | test_loss: 6.1036615372 | \n",
      "Epoch: 3495 | train_loss: 117.6806869507 | test_loss: 6.1036114693 | \n",
      "Epoch: 3496 | train_loss: 117.6804275513 | test_loss: 6.1035599709 | \n",
      "Epoch: 3497 | train_loss: 117.6801300049 | test_loss: 6.1035084724 | \n",
      "Epoch: 3498 | train_loss: 117.6798553467 | test_loss: 6.1034564972 | \n",
      "Epoch: 3499 | train_loss: 117.6795501709 | test_loss: 6.1034049988 | \n",
      "Epoch: 3500 | train_loss: 117.6792907715 | test_loss: 6.1033630371 | \n",
      "Epoch: 3501 | train_loss: 117.6790161133 | test_loss: 6.1033124924 | \n",
      "Epoch: 3502 | train_loss: 117.6787490845 | test_loss: 6.1032624245 | \n",
      "Epoch: 3503 | train_loss: 117.6784744263 | test_loss: 6.1032123566 | \n",
      "Epoch: 3504 | train_loss: 117.6781768799 | test_loss: 6.1031622887 | \n",
      "Epoch: 3505 | train_loss: 117.6778869629 | test_loss: 6.1031103134 | \n",
      "Epoch: 3506 | train_loss: 117.6776046753 | test_loss: 6.1030564308 | \n",
      "Epoch: 3507 | train_loss: 117.6773452759 | test_loss: 6.1030073166 | \n",
      "Epoch: 3508 | train_loss: 117.6770706177 | test_loss: 6.1029577255 | \n",
      "Epoch: 3509 | train_loss: 117.6767654419 | test_loss: 6.1029100418 | \n",
      "Epoch: 3510 | train_loss: 117.6764526367 | test_loss: 6.1028523445 | \n",
      "Epoch: 3511 | train_loss: 117.6762084961 | test_loss: 6.1028013229 | \n",
      "Epoch: 3512 | train_loss: 117.6759262085 | test_loss: 6.1027503014 | \n",
      "Epoch: 3513 | train_loss: 117.6756134033 | test_loss: 6.1027002335 | \n",
      "Epoch: 3514 | train_loss: 117.6753234863 | test_loss: 6.1026444435 | \n",
      "Epoch: 3515 | train_loss: 117.6750564575 | test_loss: 6.1026000977 | \n",
      "Epoch: 3516 | train_loss: 117.6748275757 | test_loss: 6.1025500298 | \n",
      "Epoch: 3517 | train_loss: 117.6745300293 | test_loss: 6.1025028229 | \n",
      "Epoch: 3518 | train_loss: 117.6742172241 | test_loss: 6.1024518013 | \n",
      "Epoch: 3519 | train_loss: 117.6739654541 | test_loss: 6.1024060249 | \n",
      "Epoch: 3520 | train_loss: 117.6736602783 | test_loss: 6.1023530960 | \n",
      "Epoch: 3521 | train_loss: 117.6734085083 | test_loss: 6.1023035049 | \n",
      "Epoch: 3522 | train_loss: 117.6731033325 | test_loss: 6.1022467613 | \n",
      "Epoch: 3523 | train_loss: 117.6728134155 | test_loss: 6.1021924019 | \n",
      "Epoch: 3524 | train_loss: 117.6725387573 | test_loss: 6.1021428108 | \n",
      "Epoch: 3525 | train_loss: 117.6722564697 | test_loss: 6.1020870209 | \n",
      "Epoch: 3526 | train_loss: 117.6719512939 | test_loss: 6.1020355225 | \n",
      "Epoch: 3527 | train_loss: 117.6717224121 | test_loss: 6.1019835472 | \n",
      "Epoch: 3528 | train_loss: 117.6714324951 | test_loss: 6.1019334793 | \n",
      "Epoch: 3529 | train_loss: 117.6711502075 | test_loss: 6.1018815041 | \n",
      "Epoch: 3530 | train_loss: 117.6708908081 | test_loss: 6.1018323898 | \n",
      "Epoch: 3531 | train_loss: 117.6706085205 | test_loss: 6.1017804146 | \n",
      "Epoch: 3532 | train_loss: 117.6703033447 | test_loss: 6.1017384529 | \n",
      "Epoch: 3533 | train_loss: 117.6700668335 | test_loss: 6.1016888618 | \n",
      "Epoch: 3534 | train_loss: 117.6697921753 | test_loss: 6.1016407013 | \n",
      "Epoch: 3535 | train_loss: 117.6694946289 | test_loss: 6.1015906334 | \n",
      "Epoch: 3536 | train_loss: 117.6692199707 | test_loss: 6.1015448570 | \n",
      "Epoch: 3537 | train_loss: 117.6689758301 | test_loss: 6.1014909744 | \n",
      "Epoch: 3538 | train_loss: 117.6686630249 | test_loss: 6.1014418602 | \n",
      "Epoch: 3539 | train_loss: 117.6683654785 | test_loss: 6.1013903618 | \n",
      "Epoch: 3540 | train_loss: 117.6680984497 | test_loss: 6.1013364792 | \n",
      "Epoch: 3541 | train_loss: 117.6677932739 | test_loss: 6.1012864113 | \n",
      "Epoch: 3542 | train_loss: 117.6675491333 | test_loss: 6.1012363434 | \n",
      "Epoch: 3543 | train_loss: 117.6672744751 | test_loss: 6.1011838913 | \n",
      "Epoch: 3544 | train_loss: 117.6669692993 | test_loss: 6.1011285782 | \n",
      "Epoch: 3545 | train_loss: 117.6667022705 | test_loss: 6.1010746956 | \n",
      "Epoch: 3546 | train_loss: 117.6664581299 | test_loss: 6.1010193825 | \n",
      "Epoch: 3547 | train_loss: 117.6661224365 | test_loss: 6.1009702682 | \n",
      "Epoch: 3548 | train_loss: 117.6658477783 | test_loss: 6.1009240150 | \n",
      "Epoch: 3549 | train_loss: 117.6655731201 | test_loss: 6.1008677483 | \n",
      "Epoch: 3550 | train_loss: 117.6652908325 | test_loss: 6.1008276939 | \n",
      "Epoch: 3551 | train_loss: 117.6650009155 | test_loss: 6.1007747650 | \n",
      "Epoch: 3552 | train_loss: 117.6647186279 | test_loss: 6.1007194519 | \n",
      "Epoch: 3553 | train_loss: 117.6644744873 | test_loss: 6.1006712914 | \n",
      "Epoch: 3554 | train_loss: 117.6642074585 | test_loss: 6.1006274223 | \n",
      "Epoch: 3555 | train_loss: 117.6639099121 | test_loss: 6.1005787849 | \n",
      "Epoch: 3556 | train_loss: 117.6636657715 | test_loss: 6.1005229950 | \n",
      "Epoch: 3557 | train_loss: 117.6633682251 | test_loss: 6.1004743576 | \n",
      "Epoch: 3558 | train_loss: 117.6631164551 | test_loss: 6.1004271507 | \n",
      "Epoch: 3559 | train_loss: 117.6628036499 | test_loss: 6.1003737450 | \n",
      "Epoch: 3560 | train_loss: 117.6625289917 | test_loss: 6.1003198624 | \n",
      "Epoch: 3561 | train_loss: 117.6622390747 | test_loss: 6.1002683640 | \n",
      "Epoch: 3562 | train_loss: 117.6619262695 | test_loss: 6.1002168655 | \n",
      "Epoch: 3563 | train_loss: 117.6616439819 | test_loss: 6.1001667976 | \n",
      "Epoch: 3564 | train_loss: 117.6613769531 | test_loss: 6.1001186371 | \n",
      "Epoch: 3565 | train_loss: 117.6611099243 | test_loss: 6.1000666618 | \n",
      "Epoch: 3566 | train_loss: 117.6608200073 | test_loss: 6.1000185013 | \n",
      "Epoch: 3567 | train_loss: 117.6605453491 | test_loss: 6.0999646187 | \n",
      "Epoch: 3568 | train_loss: 117.6602478027 | test_loss: 6.0999150276 | \n",
      "Epoch: 3569 | train_loss: 117.6599502563 | test_loss: 6.0998640060 | \n",
      "Epoch: 3570 | train_loss: 117.6596755981 | test_loss: 6.0998120308 | \n",
      "Epoch: 3571 | train_loss: 117.6593933105 | test_loss: 6.0997524261 | \n",
      "Epoch: 3572 | train_loss: 117.6591110229 | test_loss: 6.0997028351 | \n",
      "Epoch: 3573 | train_loss: 117.6588363647 | test_loss: 6.0996546745 | \n",
      "Epoch: 3574 | train_loss: 117.6585845947 | test_loss: 6.0996026993 | \n",
      "Epoch: 3575 | train_loss: 117.6583099365 | test_loss: 6.0995512009 | \n",
      "Epoch: 3576 | train_loss: 117.6580581665 | test_loss: 6.0995049477 | \n",
      "Epoch: 3577 | train_loss: 117.6577606201 | test_loss: 6.0994524956 | \n",
      "Epoch: 3578 | train_loss: 117.6574783325 | test_loss: 6.0993943214 | \n",
      "Epoch: 3579 | train_loss: 117.6571960449 | test_loss: 6.0993442535 | \n",
      "Epoch: 3580 | train_loss: 117.6569061279 | test_loss: 6.0992946625 | \n",
      "Epoch: 3581 | train_loss: 117.6566314697 | test_loss: 6.0992412567 | \n",
      "Epoch: 3582 | train_loss: 117.6563491821 | test_loss: 6.0991888046 | \n",
      "Epoch: 3583 | train_loss: 117.6560974121 | test_loss: 6.0991401672 | \n",
      "Epoch: 3584 | train_loss: 117.6558227539 | test_loss: 6.0990962982 | \n",
      "Epoch: 3585 | train_loss: 117.6555786133 | test_loss: 6.0990490913 | \n",
      "Epoch: 3586 | train_loss: 117.6552658081 | test_loss: 6.0989975929 | \n",
      "Epoch: 3587 | train_loss: 117.6549835205 | test_loss: 6.0989451408 | \n",
      "Epoch: 3588 | train_loss: 117.6546859741 | test_loss: 6.0988917351 | \n",
      "Epoch: 3589 | train_loss: 117.6544113159 | test_loss: 6.0988440514 | \n",
      "Epoch: 3590 | train_loss: 117.6541137695 | test_loss: 6.0987915993 | \n",
      "Epoch: 3591 | train_loss: 117.6538314819 | test_loss: 6.0987396240 | \n",
      "Epoch: 3592 | train_loss: 117.6535720825 | test_loss: 6.0986828804 | \n",
      "Epoch: 3593 | train_loss: 117.6532974243 | test_loss: 6.0986371040 | \n",
      "Epoch: 3594 | train_loss: 117.6530303955 | test_loss: 6.0985865593 | \n",
      "Epoch: 3595 | train_loss: 117.6527252197 | test_loss: 6.0985369682 | \n",
      "Epoch: 3596 | train_loss: 117.6524505615 | test_loss: 6.0984845161 | \n",
      "Epoch: 3597 | train_loss: 117.6521759033 | test_loss: 6.0984358788 | \n",
      "Epoch: 3598 | train_loss: 117.6519317627 | test_loss: 6.0983881950 | \n",
      "Epoch: 3599 | train_loss: 117.6516342163 | test_loss: 6.0983371735 | \n",
      "Epoch: 3600 | train_loss: 117.6513595581 | test_loss: 6.0982880592 | \n",
      "Epoch: 3601 | train_loss: 117.6510848999 | test_loss: 6.0982379913 | \n",
      "Epoch: 3602 | train_loss: 117.6508178711 | test_loss: 6.0981884003 | \n",
      "Epoch: 3603 | train_loss: 117.6505432129 | test_loss: 6.0981297493 | \n",
      "Epoch: 3604 | train_loss: 117.6502838135 | test_loss: 6.0980768204 | \n",
      "Epoch: 3605 | train_loss: 117.6500320435 | test_loss: 6.0980224609 | \n",
      "Epoch: 3606 | train_loss: 117.6497192383 | test_loss: 6.0979695320 | \n",
      "Epoch: 3607 | train_loss: 117.6494369507 | test_loss: 6.0979213715 | \n",
      "Epoch: 3608 | train_loss: 117.6491622925 | test_loss: 6.0978713036 | \n",
      "Epoch: 3609 | train_loss: 117.6488876343 | test_loss: 6.0978159904 | \n",
      "Epoch: 3610 | train_loss: 117.6485900879 | test_loss: 6.0977630615 | \n",
      "Epoch: 3611 | train_loss: 117.6482696533 | test_loss: 6.0977125168 | \n",
      "Epoch: 3612 | train_loss: 117.6479873657 | test_loss: 6.0976691246 | \n",
      "Epoch: 3613 | train_loss: 117.6477127075 | test_loss: 6.0976214409 | \n",
      "Epoch: 3614 | train_loss: 117.6474533081 | test_loss: 6.0975694656 | \n",
      "Epoch: 3615 | train_loss: 117.6471557617 | test_loss: 6.0975217819 | \n",
      "Epoch: 3616 | train_loss: 117.6469116211 | test_loss: 6.0974726677 | \n",
      "Epoch: 3617 | train_loss: 117.6466217041 | test_loss: 6.0974225998 | \n",
      "Epoch: 3618 | train_loss: 117.6463394165 | test_loss: 6.0973706245 | \n",
      "Epoch: 3619 | train_loss: 117.6460952759 | test_loss: 6.0973200798 | \n",
      "Epoch: 3620 | train_loss: 117.6458129883 | test_loss: 6.0972681046 | \n",
      "Epoch: 3621 | train_loss: 117.6455383301 | test_loss: 6.0972108841 | \n",
      "Epoch: 3622 | train_loss: 117.6452560425 | test_loss: 6.0971589088 | \n",
      "Epoch: 3623 | train_loss: 117.6450119019 | test_loss: 6.0971140862 | \n",
      "Epoch: 3624 | train_loss: 117.6447372437 | test_loss: 6.0970678329 | \n",
      "Epoch: 3625 | train_loss: 117.6444320679 | test_loss: 6.0970129967 | \n",
      "Epoch: 3626 | train_loss: 117.6441497803 | test_loss: 6.0969586372 | \n",
      "Epoch: 3627 | train_loss: 117.6438446045 | test_loss: 6.0969018936 | \n",
      "Epoch: 3628 | train_loss: 117.6435623169 | test_loss: 6.0968503952 | \n",
      "Epoch: 3629 | train_loss: 117.6432723999 | test_loss: 6.0967955589 | \n",
      "Epoch: 3630 | train_loss: 117.6429672241 | test_loss: 6.0967431068 | \n",
      "Epoch: 3631 | train_loss: 117.6427154541 | test_loss: 6.0966954231 | \n",
      "Epoch: 3632 | train_loss: 117.6424407959 | test_loss: 6.0966444016 | \n",
      "Epoch: 3633 | train_loss: 117.6421585083 | test_loss: 6.0965943336 | \n",
      "Epoch: 3634 | train_loss: 117.6418685913 | test_loss: 6.0965466499 | \n",
      "Epoch: 3635 | train_loss: 117.6416091919 | test_loss: 6.0964937210 | \n",
      "Epoch: 3636 | train_loss: 117.6413192749 | test_loss: 6.0964493752 | \n",
      "Epoch: 3637 | train_loss: 117.6410446167 | test_loss: 6.0963993073 | \n",
      "Epoch: 3638 | train_loss: 117.6407928467 | test_loss: 6.0963459015 | \n",
      "Epoch: 3639 | train_loss: 117.6404876709 | test_loss: 6.0962901115 | \n",
      "Epoch: 3640 | train_loss: 117.6402053833 | test_loss: 6.0962400436 | \n",
      "Epoch: 3641 | train_loss: 117.6399612427 | test_loss: 6.0961933136 | \n",
      "Epoch: 3642 | train_loss: 117.6396865845 | test_loss: 6.0961418152 | \n",
      "Epoch: 3643 | train_loss: 117.6394119263 | test_loss: 6.0960917473 | \n",
      "Epoch: 3644 | train_loss: 117.6391601562 | test_loss: 6.0960435867 | \n",
      "Epoch: 3645 | train_loss: 117.6388473511 | test_loss: 6.0959906578 | \n",
      "Epoch: 3646 | train_loss: 117.6385650635 | test_loss: 6.0959415436 | \n",
      "Epoch: 3647 | train_loss: 117.6382904053 | test_loss: 6.0958876610 | \n",
      "Epoch: 3648 | train_loss: 117.6380157471 | test_loss: 6.0958356857 | \n",
      "Epoch: 3649 | train_loss: 117.6377029419 | test_loss: 6.0957856178 | \n",
      "Epoch: 3650 | train_loss: 117.6374511719 | test_loss: 6.0957331657 | \n",
      "Epoch: 3651 | train_loss: 117.6371841431 | test_loss: 6.0956835747 | \n",
      "Epoch: 3652 | train_loss: 117.6369247437 | test_loss: 6.0956296921 | \n",
      "Epoch: 3653 | train_loss: 117.6366271973 | test_loss: 6.0955815315 | \n",
      "Epoch: 3654 | train_loss: 117.6363601685 | test_loss: 6.0955243111 | \n",
      "Epoch: 3655 | train_loss: 117.6360549927 | test_loss: 6.0954761505 | \n",
      "Epoch: 3656 | train_loss: 117.6357803345 | test_loss: 6.0954289436 | \n",
      "Epoch: 3657 | train_loss: 117.6355361938 | test_loss: 6.0953783989 | \n",
      "Epoch: 3658 | train_loss: 117.6352157593 | test_loss: 6.0953221321 | \n",
      "Epoch: 3659 | train_loss: 117.6349639893 | test_loss: 6.0952734947 | \n",
      "Epoch: 3660 | train_loss: 117.6346969604 | test_loss: 6.0952239037 | \n",
      "Epoch: 3661 | train_loss: 117.6344299316 | test_loss: 6.0951714516 | \n",
      "Epoch: 3662 | train_loss: 117.6341400146 | test_loss: 6.0951228142 | \n",
      "Epoch: 3663 | train_loss: 117.6338958740 | test_loss: 6.0950708389 | \n",
      "Epoch: 3664 | train_loss: 117.6335830688 | test_loss: 6.0950193405 | \n",
      "Epoch: 3665 | train_loss: 117.6333084106 | test_loss: 6.0949726105 | \n",
      "Epoch: 3666 | train_loss: 117.6330490112 | test_loss: 6.0949234962 | \n",
      "Epoch: 3667 | train_loss: 117.6327743530 | test_loss: 6.0948677063 | \n",
      "Epoch: 3668 | train_loss: 117.6325073242 | test_loss: 6.0948181152 | \n",
      "Epoch: 3669 | train_loss: 117.6322479248 | test_loss: 6.0947704315 | \n",
      "Epoch: 3670 | train_loss: 117.6319427490 | test_loss: 6.0947175026 | \n",
      "Epoch: 3671 | train_loss: 117.6316833496 | test_loss: 6.0946631432 | \n",
      "Epoch: 3672 | train_loss: 117.6314086914 | test_loss: 6.0946083069 | \n",
      "Epoch: 3673 | train_loss: 117.6311645508 | test_loss: 6.0945568085 | \n",
      "Epoch: 3674 | train_loss: 117.6308593750 | test_loss: 6.0945134163 | \n",
      "Epoch: 3675 | train_loss: 117.6306152344 | test_loss: 6.0944614410 | \n",
      "Epoch: 3676 | train_loss: 117.6303329468 | test_loss: 6.0944089890 | \n",
      "Epoch: 3677 | train_loss: 117.6300506592 | test_loss: 6.0943622589 | \n",
      "Epoch: 3678 | train_loss: 117.6297683716 | test_loss: 6.0943145752 | \n",
      "Epoch: 3679 | train_loss: 117.6295013428 | test_loss: 6.0942625999 | \n",
      "Epoch: 3680 | train_loss: 117.6292419434 | test_loss: 6.0942120552 | \n",
      "Epoch: 3681 | train_loss: 117.6289672852 | test_loss: 6.0941591263 | \n",
      "Epoch: 3682 | train_loss: 117.6286697388 | test_loss: 6.0941081047 | \n",
      "Epoch: 3683 | train_loss: 117.6283950806 | test_loss: 6.0940551758 | \n",
      "Epoch: 3684 | train_loss: 117.6281127930 | test_loss: 6.0940055847 | \n",
      "Epoch: 3685 | train_loss: 117.6278228760 | test_loss: 6.0939564705 | \n",
      "Epoch: 3686 | train_loss: 117.6275711060 | test_loss: 6.0939064026 | \n",
      "Epoch: 3687 | train_loss: 117.6272964478 | test_loss: 6.0938496590 | \n",
      "Epoch: 3688 | train_loss: 117.6270217896 | test_loss: 6.0937986374 | \n",
      "Epoch: 3689 | train_loss: 117.6267318726 | test_loss: 6.0937461853 | \n",
      "Epoch: 3690 | train_loss: 117.6264419556 | test_loss: 6.0936918259 | \n",
      "Epoch: 3691 | train_loss: 117.6261520386 | test_loss: 6.0936393738 | \n",
      "Epoch: 3692 | train_loss: 117.6259155273 | test_loss: 6.0935907364 | \n",
      "Epoch: 3693 | train_loss: 117.6256027222 | test_loss: 6.0935459137 | \n",
      "Epoch: 3694 | train_loss: 117.6253204346 | test_loss: 6.0934972763 | \n",
      "Epoch: 3695 | train_loss: 117.6250610352 | test_loss: 6.0934424400 | \n",
      "Epoch: 3696 | train_loss: 117.6247940063 | test_loss: 6.0933947563 | \n",
      "Epoch: 3697 | train_loss: 117.6245040894 | test_loss: 6.0933446884 | \n",
      "Epoch: 3698 | train_loss: 117.6242370605 | test_loss: 6.0932936668 | \n",
      "Epoch: 3699 | train_loss: 117.6239318848 | test_loss: 6.0932407379 | \n",
      "Epoch: 3700 | train_loss: 117.6236648560 | test_loss: 6.0931925774 | \n",
      "Epoch: 3701 | train_loss: 117.6233901978 | test_loss: 6.0931410789 | \n",
      "Epoch: 3702 | train_loss: 117.6231460571 | test_loss: 6.0930910110 | \n",
      "Epoch: 3703 | train_loss: 117.6228790283 | test_loss: 6.0930433273 | \n",
      "Epoch: 3704 | train_loss: 117.6226043701 | test_loss: 6.0929913521 | \n",
      "Epoch: 3705 | train_loss: 117.6223068237 | test_loss: 6.0929398537 | \n",
      "Epoch: 3706 | train_loss: 117.6220245361 | test_loss: 6.0928931236 | \n",
      "Epoch: 3707 | train_loss: 117.6217346191 | test_loss: 6.0928401947 | \n",
      "Epoch: 3708 | train_loss: 117.6215133667 | test_loss: 6.0927872658 | \n",
      "Epoch: 3709 | train_loss: 117.6212005615 | test_loss: 6.0927305222 | \n",
      "Epoch: 3710 | train_loss: 117.6208877563 | test_loss: 6.0926756859 | \n",
      "Epoch: 3711 | train_loss: 117.6206283569 | test_loss: 6.0926313400 | \n",
      "Epoch: 3712 | train_loss: 117.6203536987 | test_loss: 6.0925765038 | \n",
      "Epoch: 3713 | train_loss: 117.6200942993 | test_loss: 6.0925254822 | \n",
      "Epoch: 3714 | train_loss: 117.6197967529 | test_loss: 6.0924730301 | \n",
      "Epoch: 3715 | train_loss: 117.6195297241 | test_loss: 6.0924253464 | \n",
      "Epoch: 3716 | train_loss: 117.6192703247 | test_loss: 6.0923781395 | \n",
      "Epoch: 3717 | train_loss: 117.6189727783 | test_loss: 6.0923228264 | \n",
      "Epoch: 3718 | train_loss: 117.6187286377 | test_loss: 6.0922789574 | \n",
      "Epoch: 3719 | train_loss: 117.6184692383 | test_loss: 6.0922279358 | \n",
      "Epoch: 3720 | train_loss: 117.6181640625 | test_loss: 6.0921831131 | \n",
      "Epoch: 3721 | train_loss: 117.6178970337 | test_loss: 6.0921349525 | \n",
      "Epoch: 3722 | train_loss: 117.6176147461 | test_loss: 6.0920863152 | \n",
      "Epoch: 3723 | train_loss: 117.6173553467 | test_loss: 6.0920343399 | \n",
      "Epoch: 3724 | train_loss: 117.6170730591 | test_loss: 6.0919866562 | \n",
      "Epoch: 3725 | train_loss: 117.6167907715 | test_loss: 6.0919337273 | \n",
      "Epoch: 3726 | train_loss: 117.6165466309 | test_loss: 6.0918779373 | \n",
      "Epoch: 3727 | train_loss: 117.6162185669 | test_loss: 6.0918288231 | \n",
      "Epoch: 3728 | train_loss: 117.6159439087 | test_loss: 6.0917773247 | \n",
      "Epoch: 3729 | train_loss: 117.6156845093 | test_loss: 6.0917248726 | \n",
      "Epoch: 3730 | train_loss: 117.6154174805 | test_loss: 6.0916748047 | \n",
      "Epoch: 3731 | train_loss: 117.6151428223 | test_loss: 6.0916204453 | \n",
      "Epoch: 3732 | train_loss: 117.6148605347 | test_loss: 6.0915684700 | \n",
      "Epoch: 3733 | train_loss: 117.6146087646 | test_loss: 6.0915193558 | \n",
      "Epoch: 3734 | train_loss: 117.6143341064 | test_loss: 6.0914735794 | \n",
      "Epoch: 3735 | train_loss: 117.6140823364 | test_loss: 6.0914201736 | \n",
      "Epoch: 3736 | train_loss: 117.6137924194 | test_loss: 6.0913715363 | \n",
      "Epoch: 3737 | train_loss: 117.6135482788 | test_loss: 6.0913181305 | \n",
      "Epoch: 3738 | train_loss: 117.6132431030 | test_loss: 6.0912647247 | \n",
      "Epoch: 3739 | train_loss: 117.6129684448 | test_loss: 6.0912146568 | \n",
      "Epoch: 3740 | train_loss: 117.6127166748 | test_loss: 6.0911598206 | \n",
      "Epoch: 3741 | train_loss: 117.6124191284 | test_loss: 6.0911121368 | \n",
      "Epoch: 3742 | train_loss: 117.6121597290 | test_loss: 6.0910620689 | \n",
      "Epoch: 3743 | train_loss: 117.6118850708 | test_loss: 6.0910072327 | \n",
      "Epoch: 3744 | train_loss: 117.6116333008 | test_loss: 6.0909590721 | \n",
      "Epoch: 3745 | train_loss: 117.6113510132 | test_loss: 6.0909051895 | \n",
      "Epoch: 3746 | train_loss: 117.6110763550 | test_loss: 6.0908522606 | \n",
      "Epoch: 3747 | train_loss: 117.6107940674 | test_loss: 6.0908007622 | \n",
      "Epoch: 3748 | train_loss: 117.6105346680 | test_loss: 6.0907511711 | \n",
      "Epoch: 3749 | train_loss: 117.6102752686 | test_loss: 6.0907044411 | \n",
      "Epoch: 3750 | train_loss: 117.6100311279 | test_loss: 6.0906553268 | \n",
      "Epoch: 3751 | train_loss: 117.6097183228 | test_loss: 6.0906047821 | \n",
      "Epoch: 3752 | train_loss: 117.6094207764 | test_loss: 6.0905594826 | \n",
      "Epoch: 3753 | train_loss: 117.6091613770 | test_loss: 6.0905056000 | \n",
      "Epoch: 3754 | train_loss: 117.6088638306 | test_loss: 6.0904603004 | \n",
      "Epoch: 3755 | train_loss: 117.6085815430 | test_loss: 6.0904016495 | \n",
      "Epoch: 3756 | train_loss: 117.6083526611 | test_loss: 6.0903539658 | \n",
      "Epoch: 3757 | train_loss: 117.6080932617 | test_loss: 6.0903019905 | \n",
      "Epoch: 3758 | train_loss: 117.6078491211 | test_loss: 6.0902495384 | \n",
      "Epoch: 3759 | train_loss: 117.6075744629 | test_loss: 6.0901989937 | \n",
      "Epoch: 3760 | train_loss: 117.6072769165 | test_loss: 6.0901494026 | \n",
      "Epoch: 3761 | train_loss: 117.6070327759 | test_loss: 6.0900945663 | \n",
      "Epoch: 3762 | train_loss: 117.6067581177 | test_loss: 6.0900406837 | \n",
      "Epoch: 3763 | train_loss: 117.6064834595 | test_loss: 6.0899982452 | \n",
      "Epoch: 3764 | train_loss: 117.6062393188 | test_loss: 6.0899500847 | \n",
      "Epoch: 3765 | train_loss: 117.6059951782 | test_loss: 6.0899047852 | \n",
      "Epoch: 3766 | train_loss: 117.6057052612 | test_loss: 6.0898561478 | \n",
      "Epoch: 3767 | train_loss: 117.6054306030 | test_loss: 6.0898003578 | \n",
      "Epoch: 3768 | train_loss: 117.6051483154 | test_loss: 6.0897502899 | \n",
      "Epoch: 3769 | train_loss: 117.6048660278 | test_loss: 6.0897035599 | \n",
      "Epoch: 3770 | train_loss: 117.6045837402 | test_loss: 6.0896549225 | \n",
      "Epoch: 3771 | train_loss: 117.6042785645 | test_loss: 6.0896019936 | \n",
      "Epoch: 3772 | train_loss: 117.6040344238 | test_loss: 6.0895471573 | \n",
      "Epoch: 3773 | train_loss: 117.6037521362 | test_loss: 6.0894975662 | \n",
      "Epoch: 3774 | train_loss: 117.6034774780 | test_loss: 6.0894455910 | \n",
      "Epoch: 3775 | train_loss: 117.6031723022 | test_loss: 6.0893974304 | \n",
      "Epoch: 3776 | train_loss: 117.6028976440 | test_loss: 6.0893445015 | \n",
      "Epoch: 3777 | train_loss: 117.6026000977 | test_loss: 6.0892906189 | \n",
      "Epoch: 3778 | train_loss: 117.6023864746 | test_loss: 6.0892419815 | \n",
      "Epoch: 3779 | train_loss: 117.6020965576 | test_loss: 6.0891900063 | \n",
      "Epoch: 3780 | train_loss: 117.6018142700 | test_loss: 6.0891337395 | \n",
      "Epoch: 3781 | train_loss: 117.6015548706 | test_loss: 6.0890874863 | \n",
      "Epoch: 3782 | train_loss: 117.6013412476 | test_loss: 6.0890474319 | \n",
      "Epoch: 3783 | train_loss: 117.6010513306 | test_loss: 6.0889906883 | \n",
      "Epoch: 3784 | train_loss: 117.6007690430 | test_loss: 6.0889410973 | \n",
      "Epoch: 3785 | train_loss: 117.6004867554 | test_loss: 6.0888910294 | \n",
      "Epoch: 3786 | train_loss: 117.6002349854 | test_loss: 6.0888423920 | \n",
      "Epoch: 3787 | train_loss: 117.5999832153 | test_loss: 6.0887904167 | \n",
      "Epoch: 3788 | train_loss: 117.5997238159 | test_loss: 6.0887446404 | \n",
      "Epoch: 3789 | train_loss: 117.5994338989 | test_loss: 6.0886888504 | \n",
      "Epoch: 3790 | train_loss: 117.5991516113 | test_loss: 6.0886440277 | \n",
      "Epoch: 3791 | train_loss: 117.5988693237 | test_loss: 6.0885887146 | \n",
      "Epoch: 3792 | train_loss: 117.5985870361 | test_loss: 6.0885400772 | \n",
      "Epoch: 3793 | train_loss: 117.5982971191 | test_loss: 6.0884923935 | \n",
      "Epoch: 3794 | train_loss: 117.5980453491 | test_loss: 6.0884385109 | \n",
      "Epoch: 3795 | train_loss: 117.5977630615 | test_loss: 6.0883836746 | \n",
      "Epoch: 3796 | train_loss: 117.5975265503 | test_loss: 6.0883326530 | \n",
      "Epoch: 3797 | train_loss: 117.5972442627 | test_loss: 6.0882787704 | \n",
      "Epoch: 3798 | train_loss: 117.5969696045 | test_loss: 6.0882287025 | \n",
      "Epoch: 3799 | train_loss: 117.5967178345 | test_loss: 6.0881800652 | \n",
      "Epoch: 3800 | train_loss: 117.5964279175 | test_loss: 6.0881342888 | \n",
      "Epoch: 3801 | train_loss: 117.5961608887 | test_loss: 6.0880751610 | \n",
      "Epoch: 3802 | train_loss: 117.5958862305 | test_loss: 6.0880284309 | \n",
      "Epoch: 3803 | train_loss: 117.5956115723 | test_loss: 6.0879821777 | \n",
      "Epoch: 3804 | train_loss: 117.5953521729 | test_loss: 6.0879316330 | \n",
      "Epoch: 3805 | train_loss: 117.5950775146 | test_loss: 6.0878849030 | \n",
      "Epoch: 3806 | train_loss: 117.5948257446 | test_loss: 6.0878324509 | \n",
      "Epoch: 3807 | train_loss: 117.5945281982 | test_loss: 6.0877857208 | \n",
      "Epoch: 3808 | train_loss: 117.5942687988 | test_loss: 6.0877323151 | \n",
      "Epoch: 3809 | train_loss: 117.5940170288 | test_loss: 6.0876784325 | \n",
      "Epoch: 3810 | train_loss: 117.5937118530 | test_loss: 6.0876274109 | \n",
      "Epoch: 3811 | train_loss: 117.5934295654 | test_loss: 6.0875749588 | \n",
      "Epoch: 3812 | train_loss: 117.5931854248 | test_loss: 6.0875301361 | \n",
      "Epoch: 3813 | train_loss: 117.5929031372 | test_loss: 6.0874805450 | \n",
      "Epoch: 3814 | train_loss: 117.5926284790 | test_loss: 6.0874280930 | \n",
      "Epoch: 3815 | train_loss: 117.5923843384 | test_loss: 6.0873799324 | \n",
      "Epoch: 3816 | train_loss: 117.5921401978 | test_loss: 6.0873265266 | \n",
      "Epoch: 3817 | train_loss: 117.5918655396 | test_loss: 6.0872802734 | \n",
      "Epoch: 3818 | train_loss: 117.5915985107 | test_loss: 6.0872325897 | \n",
      "Epoch: 3819 | train_loss: 117.5913314819 | test_loss: 6.0871868134 | \n",
      "Epoch: 3820 | train_loss: 117.5910339355 | test_loss: 6.0871338844 | \n",
      "Epoch: 3821 | train_loss: 117.5907592773 | test_loss: 6.0870771408 | \n",
      "Epoch: 3822 | train_loss: 117.5904998779 | test_loss: 6.0870270729 | \n",
      "Epoch: 3823 | train_loss: 117.5902557373 | test_loss: 6.0869722366 | \n",
      "Epoch: 3824 | train_loss: 117.5899505615 | test_loss: 6.0869221687 | \n",
      "Epoch: 3825 | train_loss: 117.5896682739 | test_loss: 6.0868678093 | \n",
      "Epoch: 3826 | train_loss: 117.5894241333 | test_loss: 6.0868196487 | \n",
      "Epoch: 3827 | train_loss: 117.5891113281 | test_loss: 6.0867681503 | \n",
      "Epoch: 3828 | train_loss: 117.5888595581 | test_loss: 6.0867109299 | \n",
      "Epoch: 3829 | train_loss: 117.5886001587 | test_loss: 6.0866627693 | \n",
      "Epoch: 3830 | train_loss: 117.5883102417 | test_loss: 6.0866184235 | \n",
      "Epoch: 3831 | train_loss: 117.5880279541 | test_loss: 6.0865702629 | \n",
      "Epoch: 3832 | train_loss: 117.5877227783 | test_loss: 6.0865197182 | \n",
      "Epoch: 3833 | train_loss: 117.5874710083 | test_loss: 6.0864701271 | \n",
      "Epoch: 3834 | train_loss: 117.5872344971 | test_loss: 6.0864186287 | \n",
      "Epoch: 3835 | train_loss: 117.5869827271 | test_loss: 6.0863780975 | \n",
      "Epoch: 3836 | train_loss: 117.5867309570 | test_loss: 6.0863251686 | \n",
      "Epoch: 3837 | train_loss: 117.5864486694 | test_loss: 6.0862708092 | \n",
      "Epoch: 3838 | train_loss: 117.5861740112 | test_loss: 6.0862169266 | \n",
      "Epoch: 3839 | train_loss: 117.5858917236 | test_loss: 6.0861682892 | \n",
      "Epoch: 3840 | train_loss: 117.5856323242 | test_loss: 6.0861167908 | \n",
      "Epoch: 3841 | train_loss: 117.5853805542 | test_loss: 6.0860638618 | \n",
      "Epoch: 3842 | train_loss: 117.5850982666 | test_loss: 6.0860133171 | \n",
      "Epoch: 3843 | train_loss: 117.5847930908 | test_loss: 6.0859622955 | \n",
      "Epoch: 3844 | train_loss: 117.5845413208 | test_loss: 6.0859127045 | \n",
      "Epoch: 3845 | train_loss: 117.5842285156 | test_loss: 6.0858612061 | \n",
      "Epoch: 3846 | train_loss: 117.5839614868 | test_loss: 6.0858116150 | \n",
      "Epoch: 3847 | train_loss: 117.5836486816 | test_loss: 6.0857586861 | \n",
      "Epoch: 3848 | train_loss: 117.5834045410 | test_loss: 6.0857100487 | \n",
      "Epoch: 3849 | train_loss: 117.5831604004 | test_loss: 6.0856604576 | \n",
      "Epoch: 3850 | train_loss: 117.5829010010 | test_loss: 6.0856142044 | \n",
      "Epoch: 3851 | train_loss: 117.5826263428 | test_loss: 6.0855584145 | \n",
      "Epoch: 3852 | train_loss: 117.5823440552 | test_loss: 6.0855078697 | \n",
      "Epoch: 3853 | train_loss: 117.5820693970 | test_loss: 6.0854563713 | \n",
      "Epoch: 3854 | train_loss: 117.5817947388 | test_loss: 6.0854077339 | \n",
      "Epoch: 3855 | train_loss: 117.5815811157 | test_loss: 6.0853562355 | \n",
      "Epoch: 3856 | train_loss: 117.5812759399 | test_loss: 6.0853047371 | \n",
      "Epoch: 3857 | train_loss: 117.5809936523 | test_loss: 6.0852618217 | \n",
      "Epoch: 3858 | train_loss: 117.5807723999 | test_loss: 6.0852079391 | \n",
      "Epoch: 3859 | train_loss: 117.5804519653 | test_loss: 6.0851531029 | \n",
      "Epoch: 3860 | train_loss: 117.5802078247 | test_loss: 6.0851020813 | \n",
      "Epoch: 3861 | train_loss: 117.5799484253 | test_loss: 6.0850548744 | \n",
      "Epoch: 3862 | train_loss: 117.5796279907 | test_loss: 6.0849981308 | \n",
      "Epoch: 3863 | train_loss: 117.5793762207 | test_loss: 6.0849452019 | \n",
      "Epoch: 3864 | train_loss: 117.5791091919 | test_loss: 6.0848984718 | \n",
      "Epoch: 3865 | train_loss: 117.5788192749 | test_loss: 6.0848498344 | \n",
      "Epoch: 3866 | train_loss: 117.5785675049 | test_loss: 6.0847992897 | \n",
      "Epoch: 3867 | train_loss: 117.5783081055 | test_loss: 6.0847554207 | \n",
      "Epoch: 3868 | train_loss: 117.5780334473 | test_loss: 6.0847034454 | \n",
      "Epoch: 3869 | train_loss: 117.5778045654 | test_loss: 6.0846543312 | \n",
      "Epoch: 3870 | train_loss: 117.5775222778 | test_loss: 6.0846028328 | \n",
      "Epoch: 3871 | train_loss: 117.5772476196 | test_loss: 6.0845570564 | \n",
      "Epoch: 3872 | train_loss: 117.5770187378 | test_loss: 6.0845060349 | \n",
      "Epoch: 3873 | train_loss: 117.5767669678 | test_loss: 6.0844535828 | \n",
      "Epoch: 3874 | train_loss: 117.5764923096 | test_loss: 6.0844001770 | \n",
      "Epoch: 3875 | train_loss: 117.5762023926 | test_loss: 6.0843529701 | \n",
      "Epoch: 3876 | train_loss: 117.5759353638 | test_loss: 6.0843009949 | \n",
      "Epoch: 3877 | train_loss: 117.5756607056 | test_loss: 6.0842499733 | \n",
      "Epoch: 3878 | train_loss: 117.5754089355 | test_loss: 6.0841994286 | \n",
      "Epoch: 3879 | train_loss: 117.5751342773 | test_loss: 6.0841517448 | \n",
      "Epoch: 3880 | train_loss: 117.5748596191 | test_loss: 6.0841021538 | \n",
      "Epoch: 3881 | train_loss: 117.5746307373 | test_loss: 6.0840501785 | \n",
      "Epoch: 3882 | train_loss: 117.5743865967 | test_loss: 6.0839991570 | \n",
      "Epoch: 3883 | train_loss: 117.5740890503 | test_loss: 6.0839476585 | \n",
      "Epoch: 3884 | train_loss: 117.5738372803 | test_loss: 6.0839014053 | \n",
      "Epoch: 3885 | train_loss: 117.5735855103 | test_loss: 6.0838499069 | \n",
      "Epoch: 3886 | train_loss: 117.5732803345 | test_loss: 6.0837993622 | \n",
      "Epoch: 3887 | train_loss: 117.5730209351 | test_loss: 6.0837492943 | \n",
      "Epoch: 3888 | train_loss: 117.5727462769 | test_loss: 6.0837016106 | \n",
      "Epoch: 3889 | train_loss: 117.5724868774 | test_loss: 6.0836529732 | \n",
      "Epoch: 3890 | train_loss: 117.5722198486 | test_loss: 6.0836043358 | \n",
      "Epoch: 3891 | train_loss: 117.5719451904 | test_loss: 6.0835480690 | \n",
      "Epoch: 3892 | train_loss: 117.5716705322 | test_loss: 6.0834999084 | \n",
      "Epoch: 3893 | train_loss: 117.5713882446 | test_loss: 6.0834550858 | \n",
      "Epoch: 3894 | train_loss: 117.5711364746 | test_loss: 6.0833950043 | \n",
      "Epoch: 3895 | train_loss: 117.5708618164 | test_loss: 6.0833430290 | \n",
      "Epoch: 3896 | train_loss: 117.5706024170 | test_loss: 6.0832939148 | \n",
      "Epoch: 3897 | train_loss: 117.5703125000 | test_loss: 6.0832433701 | \n",
      "Epoch: 3898 | train_loss: 117.5700225830 | test_loss: 6.0831975937 | \n",
      "Epoch: 3899 | train_loss: 117.5697555542 | test_loss: 6.0831522942 | \n",
      "Epoch: 3900 | train_loss: 117.5695037842 | test_loss: 6.0830988884 | \n",
      "Epoch: 3901 | train_loss: 117.5692062378 | test_loss: 6.0830554962 | \n",
      "Epoch: 3902 | train_loss: 117.5689620972 | test_loss: 6.0830073357 | \n",
      "Epoch: 3903 | train_loss: 117.5686950684 | test_loss: 6.0829582214 | \n",
      "Epoch: 3904 | train_loss: 117.5683975220 | test_loss: 6.0829081535 | \n",
      "Epoch: 3905 | train_loss: 117.5681381226 | test_loss: 6.0828547478 | \n",
      "Epoch: 3906 | train_loss: 117.5678710938 | test_loss: 6.0827970505 | \n",
      "Epoch: 3907 | train_loss: 117.5675964355 | test_loss: 6.0827555656 | \n",
      "Epoch: 3908 | train_loss: 117.5673522949 | test_loss: 6.0827012062 | \n",
      "Epoch: 3909 | train_loss: 117.5670776367 | test_loss: 6.0826544762 | \n",
      "Epoch: 3910 | train_loss: 117.5668640137 | test_loss: 6.0826001167 | \n",
      "Epoch: 3911 | train_loss: 117.5665588379 | test_loss: 6.0825543404 | \n",
      "Epoch: 3912 | train_loss: 117.5663146973 | test_loss: 6.0825028419 | \n",
      "Epoch: 3913 | train_loss: 117.5660705566 | test_loss: 6.0824522972 | \n",
      "Epoch: 3914 | train_loss: 117.5657806396 | test_loss: 6.0823988914 | \n",
      "Epoch: 3915 | train_loss: 117.5654754639 | test_loss: 6.0823478699 | \n",
      "Epoch: 3916 | train_loss: 117.5652084351 | test_loss: 6.0823016167 | \n",
      "Epoch: 3917 | train_loss: 117.5649490356 | test_loss: 6.0822539330 | \n",
      "Epoch: 3918 | train_loss: 117.5646514893 | test_loss: 6.0822029114 | \n",
      "Epoch: 3919 | train_loss: 117.5644073486 | test_loss: 6.0821504593 | \n",
      "Epoch: 3920 | train_loss: 117.5641326904 | test_loss: 6.0821003914 | \n",
      "Epoch: 3921 | train_loss: 117.5638504028 | test_loss: 6.0820555687 | \n",
      "Epoch: 3922 | train_loss: 117.5636215210 | test_loss: 6.0819993019 | \n",
      "Epoch: 3923 | train_loss: 117.5633239746 | test_loss: 6.0819458961 | \n",
      "Epoch: 3924 | train_loss: 117.5630798340 | test_loss: 6.0819001198 | \n",
      "Epoch: 3925 | train_loss: 117.5628128052 | test_loss: 6.0818519592 | \n",
      "Epoch: 3926 | train_loss: 117.5625610352 | test_loss: 6.0818037987 | \n",
      "Epoch: 3927 | train_loss: 117.5622940063 | test_loss: 6.0817480087 | \n",
      "Epoch: 3928 | train_loss: 117.5620346069 | test_loss: 6.0816969872 | \n",
      "Epoch: 3929 | train_loss: 117.5617675781 | test_loss: 6.0816435814 | \n",
      "Epoch: 3930 | train_loss: 117.5614929199 | test_loss: 6.0815992355 | \n",
      "Epoch: 3931 | train_loss: 117.5612106323 | test_loss: 6.0815472603 | \n",
      "Epoch: 3932 | train_loss: 117.5609207153 | test_loss: 6.0814976692 | \n",
      "Epoch: 3933 | train_loss: 117.5606613159 | test_loss: 6.0814542770 | \n",
      "Epoch: 3934 | train_loss: 117.5604171753 | test_loss: 6.0814008713 | \n",
      "Epoch: 3935 | train_loss: 117.5600814819 | test_loss: 6.0813508034 | \n",
      "Epoch: 3936 | train_loss: 117.5598373413 | test_loss: 6.0812973976 | \n",
      "Epoch: 3937 | train_loss: 117.5595779419 | test_loss: 6.0812492371 | \n",
      "Epoch: 3938 | train_loss: 117.5593032837 | test_loss: 6.0812025070 | \n",
      "Epoch: 3939 | train_loss: 117.5590515137 | test_loss: 6.0811529160 | \n",
      "Epoch: 3940 | train_loss: 117.5587768555 | test_loss: 6.0811028481 | \n",
      "Epoch: 3941 | train_loss: 117.5585174561 | test_loss: 6.0810594559 | \n",
      "Epoch: 3942 | train_loss: 117.5582733154 | test_loss: 6.0810065269 | \n",
      "Epoch: 3943 | train_loss: 117.5579910278 | test_loss: 6.0809559822 | \n",
      "Epoch: 3944 | train_loss: 117.5577163696 | test_loss: 6.0809078217 | \n",
      "Epoch: 3945 | train_loss: 117.5574417114 | test_loss: 6.0808520317 | \n",
      "Epoch: 3946 | train_loss: 117.5571746826 | test_loss: 6.0808014870 | \n",
      "Epoch: 3947 | train_loss: 117.5569152832 | test_loss: 6.0807542801 | \n",
      "Epoch: 3948 | train_loss: 117.5566482544 | test_loss: 6.0807042122 | \n",
      "Epoch: 3949 | train_loss: 117.5563735962 | test_loss: 6.0806555748 | \n",
      "Epoch: 3950 | train_loss: 117.5561294556 | test_loss: 6.0806059837 | \n",
      "Epoch: 3951 | train_loss: 117.5558776855 | test_loss: 6.0805606842 | \n",
      "Epoch: 3952 | train_loss: 117.5555877686 | test_loss: 6.0805101395 | \n",
      "Epoch: 3953 | train_loss: 117.5553283691 | test_loss: 6.0804624557 | \n",
      "Epoch: 3954 | train_loss: 117.5550460815 | test_loss: 6.0804123878 | \n",
      "Epoch: 3955 | train_loss: 117.5547866821 | test_loss: 6.0803637505 | \n",
      "Epoch: 3956 | train_loss: 117.5545120239 | test_loss: 6.0803141594 | \n",
      "Epoch: 3957 | train_loss: 117.5542297363 | test_loss: 6.0802640915 | \n",
      "Epoch: 3958 | train_loss: 117.5539855957 | test_loss: 6.0802149773 | \n",
      "Epoch: 3959 | train_loss: 117.5537033081 | test_loss: 6.0801606178 | \n",
      "Epoch: 3960 | train_loss: 117.5534210205 | test_loss: 6.0801115036 | \n",
      "Epoch: 3961 | train_loss: 117.5531616211 | test_loss: 6.0800585747 | \n",
      "Epoch: 3962 | train_loss: 117.5528869629 | test_loss: 6.0800023079 | \n",
      "Epoch: 3963 | train_loss: 117.5526199341 | test_loss: 6.0799579620 | \n",
      "Epoch: 3964 | train_loss: 117.5523834229 | test_loss: 6.0799007416 | \n",
      "Epoch: 3965 | train_loss: 117.5521011353 | test_loss: 6.0798540115 | \n",
      "Epoch: 3966 | train_loss: 117.5518264771 | test_loss: 6.0798010826 | \n",
      "Epoch: 3967 | train_loss: 117.5515899658 | test_loss: 6.0797505379 | \n",
      "Epoch: 3968 | train_loss: 117.5513458252 | test_loss: 6.0797080994 | \n",
      "Epoch: 3969 | train_loss: 117.5510711670 | test_loss: 6.0796575546 | \n",
      "Epoch: 3970 | train_loss: 117.5508117676 | test_loss: 6.0796122551 | \n",
      "Epoch: 3971 | train_loss: 117.5505371094 | test_loss: 6.0795588493 | \n",
      "Epoch: 3972 | train_loss: 117.5502624512 | test_loss: 6.0795111656 | \n",
      "Epoch: 3973 | train_loss: 117.5500030518 | test_loss: 6.0794591904 | \n",
      "Epoch: 3974 | train_loss: 117.5497283936 | test_loss: 6.0794115067 | \n",
      "Epoch: 3975 | train_loss: 117.5494842529 | test_loss: 6.0793566704 | \n",
      "Epoch: 3976 | train_loss: 117.5492553711 | test_loss: 6.0793070793 | \n",
      "Epoch: 3977 | train_loss: 117.5489425659 | test_loss: 6.0792579651 | \n",
      "Epoch: 3978 | train_loss: 117.5486984253 | test_loss: 6.0792126656 | \n",
      "Epoch: 3979 | train_loss: 117.5484313965 | test_loss: 6.0791563988 | \n",
      "Epoch: 3980 | train_loss: 117.5481567383 | test_loss: 6.0791063309 | \n",
      "Epoch: 3981 | train_loss: 117.5479278564 | test_loss: 6.0790586472 | \n",
      "Epoch: 3982 | train_loss: 117.5476837158 | test_loss: 6.0790128708 | \n",
      "Epoch: 3983 | train_loss: 117.5474090576 | test_loss: 6.0789575577 | \n",
      "Epoch: 3984 | train_loss: 117.5471572876 | test_loss: 6.0789055824 | \n",
      "Epoch: 3985 | train_loss: 117.5468978882 | test_loss: 6.0788621902 | \n",
      "Epoch: 3986 | train_loss: 117.5466308594 | test_loss: 6.0788140297 | \n",
      "Epoch: 3987 | train_loss: 117.5463562012 | test_loss: 6.0787634850 | \n",
      "Epoch: 3988 | train_loss: 117.5460968018 | test_loss: 6.0787105560 | \n",
      "Epoch: 3989 | train_loss: 117.5458221436 | test_loss: 6.0786638260 | \n",
      "Epoch: 3990 | train_loss: 117.5455474854 | test_loss: 6.0786213875 | \n",
      "Epoch: 3991 | train_loss: 117.5453033447 | test_loss: 6.0785689354 | \n",
      "Epoch: 3992 | train_loss: 117.5450363159 | test_loss: 6.0785155296 | \n",
      "Epoch: 3993 | train_loss: 117.5447540283 | test_loss: 6.0784635544 | \n",
      "Epoch: 3994 | train_loss: 117.5444717407 | test_loss: 6.0784149170 | \n",
      "Epoch: 3995 | train_loss: 117.5442276001 | test_loss: 6.0783677101 | \n",
      "Epoch: 3996 | train_loss: 117.5439682007 | test_loss: 6.0783162117 | \n",
      "Epoch: 3997 | train_loss: 117.5436935425 | test_loss: 6.0782666206 | \n",
      "Epoch: 3998 | train_loss: 117.5434494019 | test_loss: 6.0782151222 | \n",
      "Epoch: 3999 | train_loss: 117.5432052612 | test_loss: 6.0781688690 | \n",
      "Epoch: 4000 | train_loss: 117.5429229736 | test_loss: 6.0781211853 | \n",
      "Epoch: 4001 | train_loss: 117.5426330566 | test_loss: 6.0780711174 | \n",
      "Epoch: 4002 | train_loss: 117.5423889160 | test_loss: 6.0780177116 | \n",
      "Epoch: 4003 | train_loss: 117.5421218872 | test_loss: 6.0779676437 | \n",
      "Epoch: 4004 | train_loss: 117.5418777466 | test_loss: 6.0779199600 | \n",
      "Epoch: 4005 | train_loss: 117.5416107178 | test_loss: 6.0778717995 | \n",
      "Epoch: 4006 | train_loss: 117.5413360596 | test_loss: 6.0778245926 | \n",
      "Epoch: 4007 | train_loss: 117.5410842896 | test_loss: 6.0777716637 | \n",
      "Epoch: 4008 | train_loss: 117.5407943726 | test_loss: 6.0777220726 | \n",
      "Epoch: 4009 | train_loss: 117.5405273438 | test_loss: 6.0776739120 | \n",
      "Epoch: 4010 | train_loss: 117.5402755737 | test_loss: 6.0776171684 | \n",
      "Epoch: 4011 | train_loss: 117.5400009155 | test_loss: 6.0775666237 | \n",
      "Epoch: 4012 | train_loss: 117.5397338867 | test_loss: 6.0775184631 | \n",
      "Epoch: 4013 | train_loss: 117.5394897461 | test_loss: 6.0774717331 | \n",
      "Epoch: 4014 | train_loss: 117.5392150879 | test_loss: 6.0774226189 | \n",
      "Epoch: 4015 | train_loss: 117.5389633179 | test_loss: 6.0773735046 | \n",
      "Epoch: 4016 | train_loss: 117.5387039185 | test_loss: 6.0773205757 | \n",
      "Epoch: 4017 | train_loss: 117.5384292603 | test_loss: 6.0772733688 | \n",
      "Epoch: 4018 | train_loss: 117.5381851196 | test_loss: 6.0772223473 | \n",
      "Epoch: 4019 | train_loss: 117.5379333496 | test_loss: 6.0771794319 | \n",
      "Epoch: 4020 | train_loss: 117.5376739502 | test_loss: 6.0771312714 | \n",
      "Epoch: 4021 | train_loss: 117.5374221802 | test_loss: 6.0770859718 | \n",
      "Epoch: 4022 | train_loss: 117.5371551514 | test_loss: 6.0770320892 | \n",
      "Epoch: 4023 | train_loss: 117.5369033813 | test_loss: 6.0769829750 | \n",
      "Epoch: 4024 | train_loss: 117.5365905762 | test_loss: 6.0769319534 | \n",
      "Epoch: 4025 | train_loss: 117.5363388062 | test_loss: 6.0768809319 | \n",
      "Epoch: 4026 | train_loss: 117.5360946655 | test_loss: 6.0768280029 | \n",
      "Epoch: 4027 | train_loss: 117.5358200073 | test_loss: 6.0767779350 | \n",
      "Epoch: 4028 | train_loss: 117.5355072021 | test_loss: 6.0767259598 | \n",
      "Epoch: 4029 | train_loss: 117.5352630615 | test_loss: 6.0766749382 | \n",
      "Epoch: 4030 | train_loss: 117.5350112915 | test_loss: 6.0766248703 | \n",
      "Epoch: 4031 | train_loss: 117.5347595215 | test_loss: 6.0765810013 | \n",
      "Epoch: 4032 | train_loss: 117.5344696045 | test_loss: 6.0765256882 | \n",
      "Epoch: 4033 | train_loss: 117.5341796875 | test_loss: 6.0764808655 | \n",
      "Epoch: 4034 | train_loss: 117.5339355469 | test_loss: 6.0764293671 | \n",
      "Epoch: 4035 | train_loss: 117.5336685181 | test_loss: 6.0763783455 | \n",
      "Epoch: 4036 | train_loss: 117.5334243774 | test_loss: 6.0763244629 | \n",
      "Epoch: 4037 | train_loss: 117.5331726074 | test_loss: 6.0762748718 | \n",
      "Epoch: 4038 | train_loss: 117.5328903198 | test_loss: 6.0762271881 | \n",
      "Epoch: 4039 | train_loss: 117.5326538086 | test_loss: 6.0761828423 | \n",
      "Epoch: 4040 | train_loss: 117.5323715210 | test_loss: 6.0761322975 | \n",
      "Epoch: 4041 | train_loss: 117.5321273804 | test_loss: 6.0760860443 | \n",
      "Epoch: 4042 | train_loss: 117.5318450928 | test_loss: 6.0760383606 | \n",
      "Epoch: 4043 | train_loss: 117.5315704346 | test_loss: 6.0759882927 | \n",
      "Epoch: 4044 | train_loss: 117.5313415527 | test_loss: 6.0759382248 | \n",
      "Epoch: 4045 | train_loss: 117.5310745239 | test_loss: 6.0758900642 | \n",
      "Epoch: 4046 | train_loss: 117.5308227539 | test_loss: 6.0758390427 | \n",
      "Epoch: 4047 | train_loss: 117.5305404663 | test_loss: 6.0757970810 | \n",
      "Epoch: 4048 | train_loss: 117.5302734375 | test_loss: 6.0757536888 | \n",
      "Epoch: 4049 | train_loss: 117.5299911499 | test_loss: 6.0757045746 | \n",
      "Epoch: 4050 | train_loss: 117.5297470093 | test_loss: 6.0756516457 | \n",
      "Epoch: 4051 | train_loss: 117.5295028687 | test_loss: 6.0755991936 | \n",
      "Epoch: 4052 | train_loss: 117.5292510986 | test_loss: 6.0755443573 | \n",
      "Epoch: 4053 | train_loss: 117.5289840698 | test_loss: 6.0754985809 | \n",
      "Epoch: 4054 | train_loss: 117.5287322998 | test_loss: 6.0754489899 | \n",
      "Epoch: 4055 | train_loss: 117.5284729004 | test_loss: 6.0753955841 | \n",
      "Epoch: 4056 | train_loss: 117.5282211304 | test_loss: 6.0753517151 | \n",
      "Epoch: 4057 | train_loss: 117.5279312134 | test_loss: 6.0753040314 | \n",
      "Epoch: 4058 | train_loss: 117.5276489258 | test_loss: 6.0752582550 | \n",
      "Epoch: 4059 | train_loss: 117.5274200439 | test_loss: 6.0752100945 | \n",
      "Epoch: 4060 | train_loss: 117.5271301270 | test_loss: 6.0751609802 | \n",
      "Epoch: 4061 | train_loss: 117.5268859863 | test_loss: 6.0751109123 | \n",
      "Epoch: 4062 | train_loss: 117.5266342163 | test_loss: 6.0750646591 | \n",
      "Epoch: 4063 | train_loss: 117.5263442993 | test_loss: 6.0750117302 | \n",
      "Epoch: 4064 | train_loss: 117.5261001587 | test_loss: 6.0749645233 | \n",
      "Epoch: 4065 | train_loss: 117.5258560181 | test_loss: 6.0749135017 | \n",
      "Epoch: 4066 | train_loss: 117.5255889893 | test_loss: 6.0748686790 | \n",
      "Epoch: 4067 | train_loss: 117.5252990723 | test_loss: 6.0748133659 | \n",
      "Epoch: 4068 | train_loss: 117.5250701904 | test_loss: 6.0747656822 | \n",
      "Epoch: 4069 | train_loss: 117.5248107910 | test_loss: 6.0747151375 | \n",
      "Epoch: 4070 | train_loss: 117.5245437622 | test_loss: 6.0746617317 | \n",
      "Epoch: 4071 | train_loss: 117.5242767334 | test_loss: 6.0746197701 | \n",
      "Epoch: 4072 | train_loss: 117.5240554810 | test_loss: 6.0745711327 | \n",
      "Epoch: 4073 | train_loss: 117.5237731934 | test_loss: 6.0745220184 | \n",
      "Epoch: 4074 | train_loss: 117.5235137939 | test_loss: 6.0744738579 | \n",
      "Epoch: 4075 | train_loss: 117.5232315063 | test_loss: 6.0744266510 | \n",
      "Epoch: 4076 | train_loss: 117.5229797363 | test_loss: 6.0743751526 | \n",
      "Epoch: 4077 | train_loss: 117.5227279663 | test_loss: 6.0743241310 | \n",
      "Epoch: 4078 | train_loss: 117.5224380493 | test_loss: 6.0742769241 | \n",
      "Epoch: 4079 | train_loss: 117.5222244263 | test_loss: 6.0742206573 | \n",
      "Epoch: 4080 | train_loss: 117.5219116211 | test_loss: 6.0741744041 | \n",
      "Epoch: 4081 | train_loss: 117.5216674805 | test_loss: 6.0741248131 | \n",
      "Epoch: 4082 | train_loss: 117.5213928223 | test_loss: 6.0740756989 | \n",
      "Epoch: 4083 | train_loss: 117.5211410522 | test_loss: 6.0740261078 | \n",
      "Epoch: 4084 | train_loss: 117.5208511353 | test_loss: 6.0739750862 | \n",
      "Epoch: 4085 | train_loss: 117.5206146240 | test_loss: 6.0739240646 | \n",
      "Epoch: 4086 | train_loss: 117.5203094482 | test_loss: 6.0738701820 | \n",
      "Epoch: 4087 | train_loss: 117.5200881958 | test_loss: 6.0738248825 | \n",
      "Epoch: 4088 | train_loss: 117.5198059082 | test_loss: 6.0737757683 | \n",
      "Epoch: 4089 | train_loss: 117.5195770264 | test_loss: 6.0737266541 | \n",
      "Epoch: 4090 | train_loss: 117.5192871094 | test_loss: 6.0736780167 | \n",
      "Epoch: 4091 | train_loss: 117.5190887451 | test_loss: 6.0736241341 | \n",
      "Epoch: 4092 | train_loss: 117.5188369751 | test_loss: 6.0735807419 | \n",
      "Epoch: 4093 | train_loss: 117.5185470581 | test_loss: 6.0735340118 | \n",
      "Epoch: 4094 | train_loss: 117.5183029175 | test_loss: 6.0734848976 | \n",
      "Epoch: 4095 | train_loss: 117.5180282593 | test_loss: 6.0734348297 | \n",
      "Epoch: 4096 | train_loss: 117.5177612305 | test_loss: 6.0733904839 | \n",
      "Epoch: 4097 | train_loss: 117.5175170898 | test_loss: 6.0733408928 | \n",
      "Epoch: 4098 | train_loss: 117.5172424316 | test_loss: 6.0732975006 | \n",
      "Epoch: 4099 | train_loss: 117.5169754028 | test_loss: 6.0732450485 | \n",
      "Epoch: 4100 | train_loss: 117.5166778564 | test_loss: 6.0731945038 | \n",
      "Epoch: 4101 | train_loss: 117.5164184570 | test_loss: 6.0731458664 | \n",
      "Epoch: 4102 | train_loss: 117.5161590576 | test_loss: 6.0730895996 | \n",
      "Epoch: 4103 | train_loss: 117.5158996582 | test_loss: 6.0730395317 | \n",
      "Epoch: 4104 | train_loss: 117.5156326294 | test_loss: 6.0729899406 | \n",
      "Epoch: 4105 | train_loss: 117.5153884888 | test_loss: 6.0729432106 | \n",
      "Epoch: 4106 | train_loss: 117.5151138306 | test_loss: 6.0728969574 | \n",
      "Epoch: 4107 | train_loss: 117.5148468018 | test_loss: 6.0728383064 | \n",
      "Epoch: 4108 | train_loss: 117.5146026611 | test_loss: 6.0727953911 | \n",
      "Epoch: 4109 | train_loss: 117.5143356323 | test_loss: 6.0727415085 | \n",
      "Epoch: 4110 | train_loss: 117.5140686035 | test_loss: 6.0727066994 | \n",
      "Epoch: 4111 | train_loss: 117.5138244629 | test_loss: 6.0726490021 | \n",
      "Epoch: 4112 | train_loss: 117.5135421753 | test_loss: 6.0726070404 | \n",
      "Epoch: 4113 | train_loss: 117.5132598877 | test_loss: 6.0725541115 | \n",
      "Epoch: 4114 | train_loss: 117.5129852295 | test_loss: 6.0725035667 | \n",
      "Epoch: 4115 | train_loss: 117.5127410889 | test_loss: 6.0724577904 | \n",
      "Epoch: 4116 | train_loss: 117.5124816895 | test_loss: 6.0724101067 | \n",
      "Epoch: 4117 | train_loss: 117.5122375488 | test_loss: 6.0723595619 | \n",
      "Epoch: 4118 | train_loss: 117.5119934082 | test_loss: 6.0723071098 | \n",
      "Epoch: 4119 | train_loss: 117.5117263794 | test_loss: 6.0722589493 | \n",
      "Epoch: 4120 | train_loss: 117.5114517212 | test_loss: 6.0722088814 | \n",
      "Epoch: 4121 | train_loss: 117.5111923218 | test_loss: 6.0721573830 | \n",
      "Epoch: 4122 | train_loss: 117.5109252930 | test_loss: 6.0721044540 | \n",
      "Epoch: 4123 | train_loss: 117.5107040405 | test_loss: 6.0720610619 | \n",
      "Epoch: 4124 | train_loss: 117.5104293823 | test_loss: 6.0720176697 | \n",
      "Epoch: 4125 | train_loss: 117.5101623535 | test_loss: 6.0719676018 | \n",
      "Epoch: 4126 | train_loss: 117.5099029541 | test_loss: 6.0719213486 | \n",
      "Epoch: 4127 | train_loss: 117.5096588135 | test_loss: 6.0718750954 | \n",
      "Epoch: 4128 | train_loss: 117.5094299316 | test_loss: 6.0718269348 | \n",
      "Epoch: 4129 | train_loss: 117.5091247559 | test_loss: 6.0717792511 | \n",
      "Epoch: 4130 | train_loss: 117.5088806152 | test_loss: 6.0717267990 | \n",
      "Epoch: 4131 | train_loss: 117.5086364746 | test_loss: 6.0716700554 | \n",
      "Epoch: 4132 | train_loss: 117.5083694458 | test_loss: 6.0716204643 | \n",
      "Epoch: 4133 | train_loss: 117.5081176758 | test_loss: 6.0715723038 | \n",
      "Epoch: 4134 | train_loss: 117.5078735352 | test_loss: 6.0715222359 | \n",
      "Epoch: 4135 | train_loss: 117.5076065063 | test_loss: 6.0714759827 | \n",
      "Epoch: 4136 | train_loss: 117.5073318481 | test_loss: 6.0714263916 | \n",
      "Epoch: 4137 | train_loss: 117.5070724487 | test_loss: 6.0713801384 | \n",
      "Epoch: 4138 | train_loss: 117.5068588257 | test_loss: 6.0713300705 | \n",
      "Epoch: 4139 | train_loss: 117.5065612793 | test_loss: 6.0712761879 | \n",
      "Epoch: 4140 | train_loss: 117.5063018799 | test_loss: 6.0712256432 | \n",
      "Epoch: 4141 | train_loss: 117.5060348511 | test_loss: 6.0711765289 | \n",
      "Epoch: 4142 | train_loss: 117.5057830811 | test_loss: 6.0711259842 | \n",
      "Epoch: 4143 | train_loss: 117.5055160522 | test_loss: 6.0710783005 | \n",
      "Epoch: 4144 | train_loss: 117.5052795410 | test_loss: 6.0710248947 | \n",
      "Epoch: 4145 | train_loss: 117.5049819946 | test_loss: 6.0709824562 | \n",
      "Epoch: 4146 | train_loss: 117.5047302246 | test_loss: 6.0709409714 | \n",
      "Epoch: 4147 | train_loss: 117.5045013428 | test_loss: 6.0708904266 | \n",
      "Epoch: 4148 | train_loss: 117.5042572021 | test_loss: 6.0708408356 | \n",
      "Epoch: 4149 | train_loss: 117.5040130615 | test_loss: 6.0708031654 | \n",
      "Epoch: 4150 | train_loss: 117.5037536621 | test_loss: 6.0707492828 | \n",
      "Epoch: 4151 | train_loss: 117.5035095215 | test_loss: 6.0706968307 | \n",
      "Epoch: 4152 | train_loss: 117.5032348633 | test_loss: 6.0706453323 | \n",
      "Epoch: 4153 | train_loss: 117.5029678345 | test_loss: 6.0706019402 | \n",
      "Epoch: 4154 | train_loss: 117.5027313232 | test_loss: 6.0705523491 | \n",
      "Epoch: 4155 | train_loss: 117.5024642944 | test_loss: 6.0704984665 | \n",
      "Epoch: 4156 | train_loss: 117.5022201538 | test_loss: 6.0704474449 | \n",
      "Epoch: 4157 | train_loss: 117.5019226074 | test_loss: 6.0704040527 | \n",
      "Epoch: 4158 | train_loss: 117.5016860962 | test_loss: 6.0703606606 | \n",
      "Epoch: 4159 | train_loss: 117.5014648438 | test_loss: 6.0703110695 | \n",
      "Epoch: 4160 | train_loss: 117.5011901855 | test_loss: 6.0702619553 | \n",
      "Epoch: 4161 | train_loss: 117.5009384155 | test_loss: 6.0702104568 | \n",
      "Epoch: 4162 | train_loss: 117.5006942749 | test_loss: 6.0701594353 | \n",
      "Epoch: 4163 | train_loss: 117.5003967285 | test_loss: 6.0701165199 | \n",
      "Epoch: 4164 | train_loss: 117.5001373291 | test_loss: 6.0700745583 | \n",
      "Epoch: 4165 | train_loss: 117.4999008179 | test_loss: 6.0700230598 | \n",
      "Epoch: 4166 | train_loss: 117.4996566772 | test_loss: 6.0699763298 | \n",
      "Epoch: 4167 | train_loss: 117.4993820190 | test_loss: 6.0699286461 | \n",
      "Epoch: 4168 | train_loss: 117.4991226196 | test_loss: 6.0698785782 | \n",
      "Epoch: 4169 | train_loss: 117.4988708496 | test_loss: 6.0698261261 | \n",
      "Epoch: 4170 | train_loss: 117.4986114502 | test_loss: 6.0697760582 | \n",
      "Epoch: 4171 | train_loss: 117.4983520508 | test_loss: 6.0697302818 | \n",
      "Epoch: 4172 | train_loss: 117.4980926514 | test_loss: 6.0696802139 | \n",
      "Epoch: 4173 | train_loss: 117.4978713989 | test_loss: 6.0696287155 | \n",
      "Epoch: 4174 | train_loss: 117.4976043701 | test_loss: 6.0695796013 | \n",
      "Epoch: 4175 | train_loss: 117.4973449707 | test_loss: 6.0695323944 | \n",
      "Epoch: 4176 | train_loss: 117.4971084595 | test_loss: 6.0694866180 | \n",
      "Epoch: 4177 | train_loss: 117.4968490601 | test_loss: 6.0694341660 | \n",
      "Epoch: 4178 | train_loss: 117.4966049194 | test_loss: 6.0693883896 | \n",
      "Epoch: 4179 | train_loss: 117.4963378906 | test_loss: 6.0693373680 | \n",
      "Epoch: 4180 | train_loss: 117.4961013794 | test_loss: 6.0692901611 | \n",
      "Epoch: 4181 | train_loss: 117.4958190918 | test_loss: 6.0692458153 | \n",
      "Epoch: 4182 | train_loss: 117.4955825806 | test_loss: 6.0691928864 | \n",
      "Epoch: 4183 | train_loss: 117.4953689575 | test_loss: 6.0691432953 | \n",
      "Epoch: 4184 | train_loss: 117.4950790405 | test_loss: 6.0690941811 | \n",
      "Epoch: 4185 | train_loss: 117.4948272705 | test_loss: 6.0690445900 | \n",
      "Epoch: 4186 | train_loss: 117.4945526123 | test_loss: 6.0689930916 | \n",
      "Epoch: 4187 | train_loss: 117.4943237305 | test_loss: 6.0689482689 | \n",
      "Epoch: 4188 | train_loss: 117.4940414429 | test_loss: 6.0688996315 | \n",
      "Epoch: 4189 | train_loss: 117.4937896729 | test_loss: 6.0688538551 | \n",
      "Epoch: 4190 | train_loss: 117.4935226440 | test_loss: 6.0688042641 | \n",
      "Epoch: 4191 | train_loss: 117.4932708740 | test_loss: 6.0687561035 | \n",
      "Epoch: 4192 | train_loss: 117.4930114746 | test_loss: 6.0687098503 | \n",
      "Epoch: 4193 | train_loss: 117.4927749634 | test_loss: 6.0686588287 | \n",
      "Epoch: 4194 | train_loss: 117.4925003052 | test_loss: 6.0686039925 | \n",
      "Epoch: 4195 | train_loss: 117.4922485352 | test_loss: 6.0685625076 | \n",
      "Epoch: 4196 | train_loss: 117.4919738770 | test_loss: 6.0685133934 | \n",
      "Epoch: 4197 | train_loss: 117.4916992188 | test_loss: 6.0684633255 | \n",
      "Epoch: 4198 | train_loss: 117.4914245605 | test_loss: 6.0684137344 | \n",
      "Epoch: 4199 | train_loss: 117.4911499023 | test_loss: 6.0683650970 | \n",
      "Epoch: 4200 | train_loss: 117.4908981323 | test_loss: 6.0683140755 | \n",
      "Epoch: 4201 | train_loss: 117.4906616211 | test_loss: 6.0682678223 | \n",
      "Epoch: 4202 | train_loss: 117.4904098511 | test_loss: 6.0682244301 | \n",
      "Epoch: 4203 | train_loss: 117.4901580811 | test_loss: 6.0681748390 | \n",
      "Epoch: 4204 | train_loss: 117.4898834229 | test_loss: 6.0681281090 | \n",
      "Epoch: 4205 | train_loss: 117.4896392822 | test_loss: 6.0680761337 | \n",
      "Epoch: 4206 | train_loss: 117.4893188477 | test_loss: 6.0680303574 | \n",
      "Epoch: 4207 | train_loss: 117.4891204834 | test_loss: 6.0679774284 | \n",
      "Epoch: 4208 | train_loss: 117.4888381958 | test_loss: 6.0679321289 | \n",
      "Epoch: 4209 | train_loss: 117.4885635376 | test_loss: 6.0678796768 | \n",
      "Epoch: 4210 | train_loss: 117.4882888794 | test_loss: 6.0678272247 | \n",
      "Epoch: 4211 | train_loss: 117.4880676270 | test_loss: 6.0677790642 | \n",
      "Epoch: 4212 | train_loss: 117.4878005981 | test_loss: 6.0677366257 | \n",
      "Epoch: 4213 | train_loss: 117.4875488281 | test_loss: 6.0676879883 | \n",
      "Epoch: 4214 | train_loss: 117.4872970581 | test_loss: 6.0676417351 | \n",
      "Epoch: 4215 | train_loss: 117.4870605469 | test_loss: 6.0675921440 | \n",
      "Epoch: 4216 | train_loss: 117.4867935181 | test_loss: 6.0675449371 | \n",
      "Epoch: 4217 | train_loss: 117.4865493774 | test_loss: 6.0674962997 | \n",
      "Epoch: 4218 | train_loss: 117.4863281250 | test_loss: 6.0674481392 | \n",
      "Epoch: 4219 | train_loss: 117.4860382080 | test_loss: 6.0673985481 | \n",
      "Epoch: 4220 | train_loss: 117.4857788086 | test_loss: 6.0673542023 | \n",
      "Epoch: 4221 | train_loss: 117.4855270386 | test_loss: 6.0673065186 | \n",
      "Epoch: 4222 | train_loss: 117.4852752686 | test_loss: 6.0672578812 | \n",
      "Epoch: 4223 | train_loss: 117.4850387573 | test_loss: 6.0672163963 | \n",
      "Epoch: 4224 | train_loss: 117.4848022461 | test_loss: 6.0671653748 | \n",
      "Epoch: 4225 | train_loss: 117.4845199585 | test_loss: 6.0671234131 | \n",
      "Epoch: 4226 | train_loss: 117.4842834473 | test_loss: 6.0670728683 | \n",
      "Epoch: 4227 | train_loss: 117.4840011597 | test_loss: 6.0670223236 | \n",
      "Epoch: 4228 | train_loss: 117.4837493896 | test_loss: 6.0669736862 | \n",
      "Epoch: 4229 | train_loss: 117.4835281372 | test_loss: 6.0669269562 | \n",
      "Epoch: 4230 | train_loss: 117.4832458496 | test_loss: 6.0668773651 | \n",
      "Epoch: 4231 | train_loss: 117.4829788208 | test_loss: 6.0668277740 | \n",
      "Epoch: 4232 | train_loss: 117.4827346802 | test_loss: 6.0667729378 | \n",
      "Epoch: 4233 | train_loss: 117.4825134277 | test_loss: 6.0667219162 | \n",
      "Epoch: 4234 | train_loss: 117.4822540283 | test_loss: 6.0666761398 | \n",
      "Epoch: 4235 | train_loss: 117.4820022583 | test_loss: 6.0666265488 | \n",
      "Epoch: 4236 | train_loss: 117.4817810059 | test_loss: 6.0665807724 | \n",
      "Epoch: 4237 | train_loss: 117.4815063477 | test_loss: 6.0665302277 | \n",
      "Epoch: 4238 | train_loss: 117.4812469482 | test_loss: 6.0664854050 | \n",
      "Epoch: 4239 | train_loss: 117.4809875488 | test_loss: 6.0664348602 | \n",
      "Epoch: 4240 | train_loss: 117.4807357788 | test_loss: 6.0663838387 | \n",
      "Epoch: 4241 | train_loss: 117.4804229736 | test_loss: 6.0663328171 | \n",
      "Epoch: 4242 | train_loss: 117.4801788330 | test_loss: 6.0662889481 | \n",
      "Epoch: 4243 | train_loss: 117.4799194336 | test_loss: 6.0662431717 | \n",
      "Epoch: 4244 | train_loss: 117.4796676636 | test_loss: 6.0661969185 | \n",
      "Epoch: 4245 | train_loss: 117.4794006348 | test_loss: 6.0661554337 | \n",
      "Epoch: 4246 | train_loss: 117.4791488647 | test_loss: 6.0661010742 | \n",
      "Epoch: 4247 | train_loss: 117.4788970947 | test_loss: 6.0660467148 | \n",
      "Epoch: 4248 | train_loss: 117.4785842896 | test_loss: 6.0659980774 | \n",
      "Epoch: 4249 | train_loss: 117.4783477783 | test_loss: 6.0659484863 | \n",
      "Epoch: 4250 | train_loss: 117.4780883789 | test_loss: 6.0658946037 | \n",
      "Epoch: 4251 | train_loss: 117.4778518677 | test_loss: 6.0658507347 | \n",
      "Epoch: 4252 | train_loss: 117.4776077271 | test_loss: 6.0658040047 | \n",
      "Epoch: 4253 | train_loss: 117.4773559570 | test_loss: 6.0657539368 | \n",
      "Epoch: 4254 | train_loss: 117.4770812988 | test_loss: 6.0657038689 | \n",
      "Epoch: 4255 | train_loss: 117.4768142700 | test_loss: 6.0656609535 | \n",
      "Epoch: 4256 | train_loss: 117.4765930176 | test_loss: 6.0656132698 | \n",
      "Epoch: 4257 | train_loss: 117.4763412476 | test_loss: 6.0655708313 | \n",
      "Epoch: 4258 | train_loss: 117.4760971069 | test_loss: 6.0655250549 | \n",
      "Epoch: 4259 | train_loss: 117.4758377075 | test_loss: 6.0654792786 | \n",
      "Epoch: 4260 | train_loss: 117.4755783081 | test_loss: 6.0654249191 | \n",
      "Epoch: 4261 | train_loss: 117.4753112793 | test_loss: 6.0653781891 | \n",
      "Epoch: 4262 | train_loss: 117.4750289917 | test_loss: 6.0653333664 | \n",
      "Epoch: 4263 | train_loss: 117.4747848511 | test_loss: 6.0652818680 | \n",
      "Epoch: 4264 | train_loss: 117.4745407104 | test_loss: 6.0652337074 | \n",
      "Epoch: 4265 | train_loss: 117.4742889404 | test_loss: 6.0651874542 | \n",
      "Epoch: 4266 | train_loss: 117.4740295410 | test_loss: 6.0651378632 | \n",
      "Epoch: 4267 | train_loss: 117.4737701416 | test_loss: 6.0650901794 | \n",
      "Epoch: 4268 | train_loss: 117.4734802246 | test_loss: 6.0650463104 | \n",
      "Epoch: 4269 | train_loss: 117.4732437134 | test_loss: 6.0649967194 | \n",
      "Epoch: 4270 | train_loss: 117.4729766846 | test_loss: 6.0649456978 | \n",
      "Epoch: 4271 | train_loss: 117.4727630615 | test_loss: 6.0648975372 | \n",
      "Epoch: 4272 | train_loss: 117.4724731445 | test_loss: 6.0648493767 | \n",
      "Epoch: 4273 | train_loss: 117.4722061157 | test_loss: 6.0648016930 | \n",
      "Epoch: 4274 | train_loss: 117.4719314575 | test_loss: 6.0647487640 | \n",
      "Epoch: 4275 | train_loss: 117.4717025757 | test_loss: 6.0646991730 | \n",
      "Epoch: 4276 | train_loss: 117.4714126587 | test_loss: 6.0646643639 | \n",
      "Epoch: 4277 | train_loss: 117.4712371826 | test_loss: 6.0646085739 | \n",
      "Epoch: 4278 | train_loss: 117.4709777832 | test_loss: 6.0645599365 | \n",
      "Epoch: 4279 | train_loss: 117.4707107544 | test_loss: 6.0645165443 | \n",
      "Epoch: 4280 | train_loss: 117.4704360962 | test_loss: 6.0644617081 | \n",
      "Epoch: 4281 | train_loss: 117.4701690674 | test_loss: 6.0644154549 | \n",
      "Epoch: 4282 | train_loss: 117.4699020386 | test_loss: 6.0643706322 | \n",
      "Epoch: 4283 | train_loss: 117.4696884155 | test_loss: 6.0643205643 | \n",
      "Epoch: 4284 | train_loss: 117.4694442749 | test_loss: 6.0642771721 | \n",
      "Epoch: 4285 | train_loss: 117.4691925049 | test_loss: 6.0642323494 | \n",
      "Epoch: 4286 | train_loss: 117.4689559937 | test_loss: 6.0641841888 | \n",
      "Epoch: 4287 | train_loss: 117.4686889648 | test_loss: 6.0641350746 | \n",
      "Epoch: 4288 | train_loss: 117.4684066772 | test_loss: 6.0640883446 | \n",
      "Epoch: 4289 | train_loss: 117.4681854248 | test_loss: 6.0640411377 | \n",
      "Epoch: 4290 | train_loss: 117.4679031372 | test_loss: 6.0639905930 | \n",
      "Epoch: 4291 | train_loss: 117.4676666260 | test_loss: 6.0639357567 | \n",
      "Epoch: 4292 | train_loss: 117.4673614502 | test_loss: 6.0638885498 | \n",
      "Epoch: 4293 | train_loss: 117.4671173096 | test_loss: 6.0638413429 | \n",
      "Epoch: 4294 | train_loss: 117.4668426514 | test_loss: 6.0637993813 | \n",
      "Epoch: 4295 | train_loss: 117.4666213989 | test_loss: 6.0637555122 | \n",
      "Epoch: 4296 | train_loss: 117.4664001465 | test_loss: 6.0637030602 | \n",
      "Epoch: 4297 | train_loss: 117.4661102295 | test_loss: 6.0636553764 | \n",
      "Epoch: 4298 | train_loss: 117.4658432007 | test_loss: 6.0636005402 | \n",
      "Epoch: 4299 | train_loss: 117.4656143188 | test_loss: 6.0635523796 | \n",
      "Epoch: 4300 | train_loss: 117.4654006958 | test_loss: 6.0635128021 | \n",
      "Epoch: 4301 | train_loss: 117.4651184082 | test_loss: 6.0634632111 | \n",
      "Epoch: 4302 | train_loss: 117.4648666382 | test_loss: 6.0634126663 | \n",
      "Epoch: 4303 | train_loss: 117.4646224976 | test_loss: 6.0633678436 | \n",
      "Epoch: 4304 | train_loss: 117.4643783569 | test_loss: 6.0633149147 | \n",
      "Epoch: 4305 | train_loss: 117.4640884399 | test_loss: 6.0632672310 | \n",
      "Epoch: 4306 | train_loss: 117.4638442993 | test_loss: 6.0632185936 | \n",
      "Epoch: 4307 | train_loss: 117.4635772705 | test_loss: 6.0631675720 | \n",
      "Epoch: 4308 | train_loss: 117.4633178711 | test_loss: 6.0631208420 | \n",
      "Epoch: 4309 | train_loss: 117.4630889893 | test_loss: 6.0630731583 | \n",
      "Epoch: 4310 | train_loss: 117.4628219604 | test_loss: 6.0630235672 | \n",
      "Epoch: 4311 | train_loss: 117.4625701904 | test_loss: 6.0629754066 | \n",
      "Epoch: 4312 | train_loss: 117.4623260498 | test_loss: 6.0629258156 | \n",
      "Epoch: 4313 | train_loss: 117.4620590210 | test_loss: 6.0628786087 | \n",
      "Epoch: 4314 | train_loss: 117.4618377686 | test_loss: 6.0628361702 | \n",
      "Epoch: 4315 | train_loss: 117.4616088867 | test_loss: 6.0627913475 | \n",
      "Epoch: 4316 | train_loss: 117.4613647461 | test_loss: 6.0627398491 | \n",
      "Epoch: 4317 | train_loss: 117.4610824585 | test_loss: 6.0626988411 | \n",
      "Epoch: 4318 | train_loss: 117.4608764648 | test_loss: 6.0626482964 | \n",
      "Epoch: 4319 | train_loss: 117.4606018066 | test_loss: 6.0626025200 | \n",
      "Epoch: 4320 | train_loss: 117.4603881836 | test_loss: 6.0625514984 | \n",
      "Epoch: 4321 | train_loss: 117.4601440430 | test_loss: 6.0624995232 | \n",
      "Epoch: 4322 | train_loss: 117.4598922729 | test_loss: 6.0624518394 | \n",
      "Epoch: 4323 | train_loss: 117.4596176147 | test_loss: 6.0624046326 | \n",
      "Epoch: 4324 | train_loss: 117.4593963623 | test_loss: 6.0623588562 | \n",
      "Epoch: 4325 | train_loss: 117.4591293335 | test_loss: 6.0623064041 | \n",
      "Epoch: 4326 | train_loss: 117.4588470459 | test_loss: 6.0622606277 | \n",
      "Epoch: 4327 | train_loss: 117.4586257935 | test_loss: 6.0622224808 | \n",
      "Epoch: 4328 | train_loss: 117.4583892822 | test_loss: 6.0621705055 | \n",
      "Epoch: 4329 | train_loss: 117.4581146240 | test_loss: 6.0621256828 | \n",
      "Epoch: 4330 | train_loss: 117.4578933716 | test_loss: 6.0620746613 | \n",
      "Epoch: 4331 | train_loss: 117.4576110840 | test_loss: 6.0620250702 | \n",
      "Epoch: 4332 | train_loss: 117.4573440552 | test_loss: 6.0619826317 | \n",
      "Epoch: 4333 | train_loss: 117.4571075439 | test_loss: 6.0619359016 | \n",
      "Epoch: 4334 | train_loss: 117.4568557739 | test_loss: 6.0618877411 | \n",
      "Epoch: 4335 | train_loss: 117.4565429688 | test_loss: 6.0618453026 | \n",
      "Epoch: 4336 | train_loss: 117.4563217163 | test_loss: 6.0618004799 | \n",
      "Epoch: 4337 | train_loss: 117.4560928345 | test_loss: 6.0617523193 | \n",
      "Epoch: 4338 | train_loss: 117.4558105469 | test_loss: 6.0617046356 | \n",
      "Epoch: 4339 | train_loss: 117.4555892944 | test_loss: 6.0616569519 | \n",
      "Epoch: 4340 | train_loss: 117.4553222656 | test_loss: 6.0616087914 | \n",
      "Epoch: 4341 | train_loss: 117.4550552368 | test_loss: 6.0615553856 | \n",
      "Epoch: 4342 | train_loss: 117.4548110962 | test_loss: 6.0615081787 | \n",
      "Epoch: 4343 | train_loss: 117.4545822144 | test_loss: 6.0614643097 | \n",
      "Epoch: 4344 | train_loss: 117.4543380737 | test_loss: 6.0614142418 | \n",
      "Epoch: 4345 | train_loss: 117.4540939331 | test_loss: 6.0613703728 | \n",
      "Epoch: 4346 | train_loss: 117.4538269043 | test_loss: 6.0613188744 | \n",
      "Epoch: 4347 | train_loss: 117.4535751343 | test_loss: 6.0612711906 | \n",
      "Epoch: 4348 | train_loss: 117.4533462524 | test_loss: 6.0612273216 | \n",
      "Epoch: 4349 | train_loss: 117.4531021118 | test_loss: 6.0611782074 | \n",
      "Epoch: 4350 | train_loss: 117.4528350830 | test_loss: 6.0611281395 | \n",
      "Epoch: 4351 | train_loss: 117.4525375366 | test_loss: 6.0610814095 | \n",
      "Epoch: 4352 | train_loss: 117.4523162842 | test_loss: 6.0610408783 | \n",
      "Epoch: 4353 | train_loss: 117.4520874023 | test_loss: 6.0609874725 | \n",
      "Epoch: 4354 | train_loss: 117.4518356323 | test_loss: 6.0609436035 | \n",
      "Epoch: 4355 | train_loss: 117.4515609741 | test_loss: 6.0608997345 | \n",
      "Epoch: 4356 | train_loss: 117.4513397217 | test_loss: 6.0608491898 | \n",
      "Epoch: 4357 | train_loss: 117.4510650635 | test_loss: 6.0608043671 | \n",
      "Epoch: 4358 | train_loss: 117.4507980347 | test_loss: 6.0607509613 | \n",
      "Epoch: 4359 | train_loss: 117.4505462646 | test_loss: 6.0607018471 | \n",
      "Epoch: 4360 | train_loss: 117.4503021240 | test_loss: 6.0606498718 | \n",
      "Epoch: 4361 | train_loss: 117.4500579834 | test_loss: 6.0606040955 | \n",
      "Epoch: 4362 | train_loss: 117.4498138428 | test_loss: 6.0605554581 | \n",
      "Epoch: 4363 | train_loss: 117.4495544434 | test_loss: 6.0605068207 | \n",
      "Epoch: 4364 | train_loss: 117.4493103027 | test_loss: 6.0604591370 | \n",
      "Epoch: 4365 | train_loss: 117.4490737915 | test_loss: 6.0604133606 | \n",
      "Epoch: 4366 | train_loss: 117.4488296509 | test_loss: 6.0603656769 | \n",
      "Epoch: 4367 | train_loss: 117.4485626221 | test_loss: 6.0603137016 | \n",
      "Epoch: 4368 | train_loss: 117.4483337402 | test_loss: 6.0602660179 | \n",
      "Epoch: 4369 | train_loss: 117.4480590820 | test_loss: 6.0602183342 | \n",
      "Epoch: 4370 | train_loss: 117.4477996826 | test_loss: 6.0601673126 | \n",
      "Epoch: 4371 | train_loss: 117.4475555420 | test_loss: 6.0601253510 | \n",
      "Epoch: 4372 | train_loss: 117.4473037720 | test_loss: 6.0600800514 | \n",
      "Epoch: 4373 | train_loss: 117.4470291138 | test_loss: 6.0600371361 | \n",
      "Epoch: 4374 | train_loss: 117.4467849731 | test_loss: 6.0599865913 | \n",
      "Epoch: 4375 | train_loss: 117.4465255737 | test_loss: 6.0599427223 | \n",
      "Epoch: 4376 | train_loss: 117.4462814331 | test_loss: 6.0598931313 | \n",
      "Epoch: 4377 | train_loss: 117.4460449219 | test_loss: 6.0598478317 | \n",
      "Epoch: 4378 | train_loss: 117.4457931519 | test_loss: 6.0598006248 | \n",
      "Epoch: 4379 | train_loss: 117.4455795288 | test_loss: 6.0597534180 | \n",
      "Epoch: 4380 | train_loss: 117.4453048706 | test_loss: 6.0597047806 | \n",
      "Epoch: 4381 | train_loss: 117.4450683594 | test_loss: 6.0596618652 | \n",
      "Epoch: 4382 | train_loss: 117.4448165894 | test_loss: 6.0596113205 | \n",
      "Epoch: 4383 | train_loss: 117.4445877075 | test_loss: 6.0595622063 | \n",
      "Epoch: 4384 | train_loss: 117.4443359375 | test_loss: 6.0595121384 | \n",
      "Epoch: 4385 | train_loss: 117.4440841675 | test_loss: 6.0594658852 | \n",
      "Epoch: 4386 | train_loss: 117.4438400269 | test_loss: 6.0594182014 | \n",
      "Epoch: 4387 | train_loss: 117.4435729980 | test_loss: 6.0593714714 | \n",
      "Epoch: 4388 | train_loss: 117.4432907104 | test_loss: 6.0593247414 | \n",
      "Epoch: 4389 | train_loss: 117.4430236816 | test_loss: 6.0592784882 | \n",
      "Epoch: 4390 | train_loss: 117.4427795410 | test_loss: 6.0592327118 | \n",
      "Epoch: 4391 | train_loss: 117.4425506592 | test_loss: 6.0591888428 | \n",
      "Epoch: 4392 | train_loss: 117.4422836304 | test_loss: 6.0591392517 | \n",
      "Epoch: 4393 | train_loss: 117.4420242310 | test_loss: 6.0590906143 | \n",
      "Epoch: 4394 | train_loss: 117.4417648315 | test_loss: 6.0590467453 | \n",
      "Epoch: 4395 | train_loss: 117.4415740967 | test_loss: 6.0589952469 | \n",
      "Epoch: 4396 | train_loss: 117.4412689209 | test_loss: 6.0589528084 | \n",
      "Epoch: 4397 | train_loss: 117.4410247803 | test_loss: 6.0589032173 | \n",
      "Epoch: 4398 | train_loss: 117.4407882690 | test_loss: 6.0588626862 | \n",
      "Epoch: 4399 | train_loss: 117.4405670166 | test_loss: 6.0588145256 | \n",
      "Epoch: 4400 | train_loss: 117.4402847290 | test_loss: 6.0587673187 | \n",
      "Epoch: 4401 | train_loss: 117.4400177002 | test_loss: 6.0587182045 | \n",
      "Epoch: 4402 | train_loss: 117.4397735596 | test_loss: 6.0586724281 | \n",
      "Epoch: 4403 | train_loss: 117.4395141602 | test_loss: 6.0586247444 | \n",
      "Epoch: 4404 | train_loss: 117.4392852783 | test_loss: 6.0585761070 | \n",
      "Epoch: 4405 | train_loss: 117.4390335083 | test_loss: 6.0585298538 | \n",
      "Epoch: 4406 | train_loss: 117.4387588501 | test_loss: 6.0584831238 | \n",
      "Epoch: 4407 | train_loss: 117.4385070801 | test_loss: 6.0584373474 | \n",
      "Epoch: 4408 | train_loss: 117.4383087158 | test_loss: 6.0583844185 | \n",
      "Epoch: 4409 | train_loss: 117.4380569458 | test_loss: 6.0583343506 | \n",
      "Epoch: 4410 | train_loss: 117.4377746582 | test_loss: 6.0582909584 | \n",
      "Epoch: 4411 | train_loss: 117.4375457764 | test_loss: 6.0582494736 | \n",
      "Epoch: 4412 | train_loss: 117.4373168945 | test_loss: 6.0582008362 | \n",
      "Epoch: 4413 | train_loss: 117.4370193481 | test_loss: 6.0581560135 | \n",
      "Epoch: 4414 | train_loss: 117.4367904663 | test_loss: 6.0581078529 | \n",
      "Epoch: 4415 | train_loss: 117.4365234375 | test_loss: 6.0580620766 | \n",
      "Epoch: 4416 | train_loss: 117.4362716675 | test_loss: 6.0580201149 | \n",
      "Epoch: 4417 | train_loss: 117.4360351562 | test_loss: 6.0579724312 | \n",
      "Epoch: 4418 | train_loss: 117.4357986450 | test_loss: 6.0579237938 | \n",
      "Epoch: 4419 | train_loss: 117.4355392456 | test_loss: 6.0578823090 | \n",
      "Epoch: 4420 | train_loss: 117.4352722168 | test_loss: 6.0578355789 | \n",
      "Epoch: 4421 | train_loss: 117.4350814819 | test_loss: 6.0577878952 | \n",
      "Epoch: 4422 | train_loss: 117.4348144531 | test_loss: 6.0577449799 | \n",
      "Epoch: 4423 | train_loss: 117.4345932007 | test_loss: 6.0577001572 | \n",
      "Epoch: 4424 | train_loss: 117.4343261719 | test_loss: 6.0576515198 | \n",
      "Epoch: 4425 | train_loss: 117.4340820312 | test_loss: 6.0575990677 | \n",
      "Epoch: 4426 | train_loss: 117.4338378906 | test_loss: 6.0575532913 | \n",
      "Epoch: 4427 | train_loss: 117.4335632324 | test_loss: 6.0575013161 | \n",
      "Epoch: 4428 | train_loss: 117.4333190918 | test_loss: 6.0574474335 | \n",
      "Epoch: 4429 | train_loss: 117.4330825806 | test_loss: 6.0574016571 | \n",
      "Epoch: 4430 | train_loss: 117.4328613281 | test_loss: 6.0573601723 | \n",
      "Epoch: 4431 | train_loss: 117.4326477051 | test_loss: 6.0573148727 | \n",
      "Epoch: 4432 | train_loss: 117.4323806763 | test_loss: 6.0572676659 | \n",
      "Epoch: 4433 | train_loss: 117.4321289062 | test_loss: 6.0572228432 | \n",
      "Epoch: 4434 | train_loss: 117.4318923950 | test_loss: 6.0571765900 | \n",
      "Epoch: 4435 | train_loss: 117.4316482544 | test_loss: 6.0571289062 | \n",
      "Epoch: 4436 | train_loss: 117.4314041138 | test_loss: 6.0570769310 | \n",
      "Epoch: 4437 | train_loss: 117.4311523438 | test_loss: 6.0570344925 | \n",
      "Epoch: 4438 | train_loss: 117.4309005737 | test_loss: 6.0569877625 | \n",
      "Epoch: 4439 | train_loss: 117.4306716919 | test_loss: 6.0569400787 | \n",
      "Epoch: 4440 | train_loss: 117.4304275513 | test_loss: 6.0568938255 | \n",
      "Epoch: 4441 | train_loss: 117.4301986694 | test_loss: 6.0568470955 | \n",
      "Epoch: 4442 | train_loss: 117.4299316406 | test_loss: 6.0568013191 | \n",
      "Epoch: 4443 | train_loss: 117.4296951294 | test_loss: 6.0567545891 | \n",
      "Epoch: 4444 | train_loss: 117.4294662476 | test_loss: 6.0567064285 | \n",
      "Epoch: 4445 | train_loss: 117.4291992188 | test_loss: 6.0566601753 | \n",
      "Epoch: 4446 | train_loss: 117.4289627075 | test_loss: 6.0566115379 | \n",
      "Epoch: 4447 | train_loss: 117.4287185669 | test_loss: 6.0565638542 | \n",
      "Epoch: 4448 | train_loss: 117.4285049438 | test_loss: 6.0565147400 | \n",
      "Epoch: 4449 | train_loss: 117.4282226562 | test_loss: 6.0564665794 | \n",
      "Epoch: 4450 | train_loss: 117.4279785156 | test_loss: 6.0564155579 | \n",
      "Epoch: 4451 | train_loss: 117.4276962280 | test_loss: 6.0563731194 | \n",
      "Epoch: 4452 | train_loss: 117.4274597168 | test_loss: 6.0563287735 | \n",
      "Epoch: 4453 | train_loss: 117.4272232056 | test_loss: 6.0562763214 | \n",
      "Epoch: 4454 | train_loss: 117.4269790649 | test_loss: 6.0562310219 | \n",
      "Epoch: 4455 | train_loss: 117.4267578125 | test_loss: 6.0561885834 | \n",
      "Epoch: 4456 | train_loss: 117.4264831543 | test_loss: 6.0561442375 | \n",
      "Epoch: 4457 | train_loss: 117.4262390137 | test_loss: 6.0561008453 | \n",
      "Epoch: 4458 | train_loss: 117.4260177612 | test_loss: 6.0560493469 | \n",
      "Epoch: 4459 | train_loss: 117.4257354736 | test_loss: 6.0560045242 | \n",
      "Epoch: 4460 | train_loss: 117.4254989624 | test_loss: 6.0559525490 | \n",
      "Epoch: 4461 | train_loss: 117.4252548218 | test_loss: 6.0559082031 | \n",
      "Epoch: 4462 | train_loss: 117.4250106812 | test_loss: 6.0558662415 | \n",
      "Epoch: 4463 | train_loss: 117.4247741699 | test_loss: 6.0558166504 | \n",
      "Epoch: 4464 | train_loss: 117.4244995117 | test_loss: 6.0557694435 | \n",
      "Epoch: 4465 | train_loss: 117.4242248535 | test_loss: 6.0557227135 | \n",
      "Epoch: 4466 | train_loss: 117.4239807129 | test_loss: 6.0556731224 | \n",
      "Epoch: 4467 | train_loss: 117.4237670898 | test_loss: 6.0556254387 | \n",
      "Epoch: 4468 | train_loss: 117.4234924316 | test_loss: 6.0555810928 | \n",
      "Epoch: 4469 | train_loss: 117.4232482910 | test_loss: 6.0555329323 | \n",
      "Epoch: 4470 | train_loss: 117.4230041504 | test_loss: 6.0554881096 | \n",
      "Epoch: 4471 | train_loss: 117.4227523804 | test_loss: 6.0554485321 | \n",
      "Epoch: 4472 | train_loss: 117.4225387573 | test_loss: 6.0554003716 | \n",
      "Epoch: 4473 | train_loss: 117.4222640991 | test_loss: 6.0553507805 | \n",
      "Epoch: 4474 | train_loss: 117.4220199585 | test_loss: 6.0553126335 | \n",
      "Epoch: 4475 | train_loss: 117.4218292236 | test_loss: 6.0552644730 | \n",
      "Epoch: 4476 | train_loss: 117.4215545654 | test_loss: 6.0552172661 | \n",
      "Epoch: 4477 | train_loss: 117.4213180542 | test_loss: 6.0551710129 | \n",
      "Epoch: 4478 | train_loss: 117.4210662842 | test_loss: 6.0551238060 | \n",
      "Epoch: 4479 | train_loss: 117.4208221436 | test_loss: 6.0550775528 | \n",
      "Epoch: 4480 | train_loss: 117.4205627441 | test_loss: 6.0550270081 | \n",
      "Epoch: 4481 | train_loss: 117.4203033447 | test_loss: 6.0549778938 | \n",
      "Epoch: 4482 | train_loss: 117.4200744629 | test_loss: 6.0549330711 | \n",
      "Epoch: 4483 | train_loss: 117.4198226929 | test_loss: 6.0548820496 | \n",
      "Epoch: 4484 | train_loss: 117.4195632935 | test_loss: 6.0548377037 | \n",
      "Epoch: 4485 | train_loss: 117.4193344116 | test_loss: 6.0547924042 | \n",
      "Epoch: 4486 | train_loss: 117.4190826416 | test_loss: 6.0547437668 | \n",
      "Epoch: 4487 | train_loss: 117.4188385010 | test_loss: 6.0547013283 | \n",
      "Epoch: 4488 | train_loss: 117.4185714722 | test_loss: 6.0546550751 | \n",
      "Epoch: 4489 | train_loss: 117.4183273315 | test_loss: 6.0546069145 | \n",
      "Epoch: 4490 | train_loss: 117.4180603027 | test_loss: 6.0545635223 | \n",
      "Epoch: 4491 | train_loss: 117.4178314209 | test_loss: 6.0545225143 | \n",
      "Epoch: 4492 | train_loss: 117.4176101685 | test_loss: 6.0544710159 | \n",
      "Epoch: 4493 | train_loss: 117.4173660278 | test_loss: 6.0544233322 | \n",
      "Epoch: 4494 | train_loss: 117.4171218872 | test_loss: 6.0543785095 | \n",
      "Epoch: 4495 | train_loss: 117.4168472290 | test_loss: 6.0543308258 | \n",
      "Epoch: 4496 | train_loss: 117.4166183472 | test_loss: 6.0542850494 | \n",
      "Epoch: 4497 | train_loss: 117.4163742065 | test_loss: 6.0542411804 | \n",
      "Epoch: 4498 | train_loss: 117.4161911011 | test_loss: 6.0541987419 | \n",
      "Epoch: 4499 | train_loss: 117.4159240723 | test_loss: 6.0541453362 | \n",
      "Epoch: 4500 | train_loss: 117.4156951904 | test_loss: 6.0540990829 | \n",
      "Epoch: 4501 | train_loss: 117.4154510498 | test_loss: 6.0540552139 | \n",
      "Epoch: 4502 | train_loss: 117.4151916504 | test_loss: 6.0540118217 | \n",
      "Epoch: 4503 | train_loss: 117.4149703979 | test_loss: 6.0539660454 | \n",
      "Epoch: 4504 | train_loss: 117.4147262573 | test_loss: 6.0539140701 | \n",
      "Epoch: 4505 | train_loss: 117.4144210815 | test_loss: 6.0538659096 | \n",
      "Epoch: 4506 | train_loss: 117.4142074585 | test_loss: 6.0538182259 | \n",
      "Epoch: 4507 | train_loss: 117.4139480591 | test_loss: 6.0537695885 | \n",
      "Epoch: 4508 | train_loss: 117.4137039185 | test_loss: 6.0537276268 | \n",
      "Epoch: 4509 | train_loss: 117.4134445190 | test_loss: 6.0536789894 | \n",
      "Epoch: 4510 | train_loss: 117.4132156372 | test_loss: 6.0536289215 | \n",
      "Epoch: 4511 | train_loss: 117.4129486084 | test_loss: 6.0535912514 | \n",
      "Epoch: 4512 | train_loss: 117.4126968384 | test_loss: 6.0535478592 | \n",
      "Epoch: 4513 | train_loss: 117.4124679565 | test_loss: 6.0535054207 | \n",
      "Epoch: 4514 | train_loss: 117.4122085571 | test_loss: 6.0534596443 | \n",
      "Epoch: 4515 | train_loss: 117.4119796753 | test_loss: 6.0534152985 | \n",
      "Epoch: 4516 | train_loss: 117.4117126465 | test_loss: 6.0533599854 | \n",
      "Epoch: 4517 | train_loss: 117.4114761353 | test_loss: 6.0533075333 | \n",
      "Epoch: 4518 | train_loss: 117.4112243652 | test_loss: 6.0532636642 | \n",
      "Epoch: 4519 | train_loss: 117.4109878540 | test_loss: 6.0532226562 | \n",
      "Epoch: 4520 | train_loss: 117.4107742310 | test_loss: 6.0531673431 | \n",
      "Epoch: 4521 | train_loss: 117.4105453491 | test_loss: 6.0531239510 | \n",
      "Epoch: 4522 | train_loss: 117.4102706909 | test_loss: 6.0530762672 | \n",
      "Epoch: 4523 | train_loss: 117.4100265503 | test_loss: 6.0530304909 | \n",
      "Epoch: 4524 | train_loss: 117.4097671509 | test_loss: 6.0529894829 | \n",
      "Epoch: 4525 | train_loss: 117.4095306396 | test_loss: 6.0529422760 | \n",
      "Epoch: 4526 | train_loss: 117.4092941284 | test_loss: 6.0528998375 | \n",
      "Epoch: 4527 | train_loss: 117.4090499878 | test_loss: 6.0528459549 | \n",
      "Epoch: 4528 | train_loss: 117.4087829590 | test_loss: 6.0528025627 | \n",
      "Epoch: 4529 | train_loss: 117.4085388184 | test_loss: 6.0527553558 | \n",
      "Epoch: 4530 | train_loss: 117.4082946777 | test_loss: 6.0527129173 | \n",
      "Epoch: 4531 | train_loss: 117.4080734253 | test_loss: 6.0526647568 | \n",
      "Epoch: 4532 | train_loss: 117.4078521729 | test_loss: 6.0526161194 | \n",
      "Epoch: 4533 | train_loss: 117.4076156616 | test_loss: 6.0525751114 | \n",
      "Epoch: 4534 | train_loss: 117.4073715210 | test_loss: 6.0525298119 | \n",
      "Epoch: 4535 | train_loss: 117.4071121216 | test_loss: 6.0524773598 | \n",
      "Epoch: 4536 | train_loss: 117.4068832397 | test_loss: 6.0524330139 | \n",
      "Epoch: 4537 | train_loss: 117.4066314697 | test_loss: 6.0523858070 | \n",
      "Epoch: 4538 | train_loss: 117.4063644409 | test_loss: 6.0523419380 | \n",
      "Epoch: 4539 | train_loss: 117.4061508179 | test_loss: 6.0522990227 | \n",
      "Epoch: 4540 | train_loss: 117.4059371948 | test_loss: 6.0522484779 | \n",
      "Epoch: 4541 | train_loss: 117.4056930542 | test_loss: 6.0522074699 | \n",
      "Epoch: 4542 | train_loss: 117.4054565430 | test_loss: 6.0521583557 | \n",
      "Epoch: 4543 | train_loss: 117.4052352905 | test_loss: 6.0521149635 | \n",
      "Epoch: 4544 | train_loss: 117.4049606323 | test_loss: 6.0520677567 | \n",
      "Epoch: 4545 | train_loss: 117.4047241211 | test_loss: 6.0520195961 | \n",
      "Epoch: 4546 | train_loss: 117.4044723511 | test_loss: 6.0519738197 | \n",
      "Epoch: 4547 | train_loss: 117.4042587280 | test_loss: 6.0519304276 | \n",
      "Epoch: 4548 | train_loss: 117.4039764404 | test_loss: 6.0518836975 | \n",
      "Epoch: 4549 | train_loss: 117.4037399292 | test_loss: 6.0518407822 | \n",
      "Epoch: 4550 | train_loss: 117.4035034180 | test_loss: 6.0517888069 | \n",
      "Epoch: 4551 | train_loss: 117.4032516479 | test_loss: 6.0517463684 | \n",
      "Epoch: 4552 | train_loss: 117.4030456543 | test_loss: 6.0517015457 | \n",
      "Epoch: 4553 | train_loss: 117.4028015137 | test_loss: 6.0516548157 | \n",
      "Epoch: 4554 | train_loss: 117.4025497437 | test_loss: 6.0516114235 | \n",
      "Epoch: 4555 | train_loss: 117.4023056030 | test_loss: 6.0515637398 | \n",
      "Epoch: 4556 | train_loss: 117.4020767212 | test_loss: 6.0515246391 | \n",
      "Epoch: 4557 | train_loss: 117.4018554688 | test_loss: 6.0514736176 | \n",
      "Epoch: 4558 | train_loss: 117.4015884399 | test_loss: 6.0514283180 | \n",
      "Epoch: 4559 | train_loss: 117.4013595581 | test_loss: 6.0513844490 | \n",
      "Epoch: 4560 | train_loss: 117.4011230469 | test_loss: 6.0513391495 | \n",
      "Epoch: 4561 | train_loss: 117.4008560181 | test_loss: 6.0512943268 | \n",
      "Epoch: 4562 | train_loss: 117.4006042480 | test_loss: 6.0512480736 | \n",
      "Epoch: 4563 | train_loss: 117.4003601074 | test_loss: 6.0512003899 | \n",
      "Epoch: 4564 | train_loss: 117.4001235962 | test_loss: 6.0511589050 | \n",
      "Epoch: 4565 | train_loss: 117.3999252319 | test_loss: 6.0511150360 | \n",
      "Epoch: 4566 | train_loss: 117.3996887207 | test_loss: 6.0510692596 | \n",
      "Epoch: 4567 | train_loss: 117.3994598389 | test_loss: 6.0510282516 | \n",
      "Epoch: 4568 | train_loss: 117.3992156982 | test_loss: 6.0509791374 | \n",
      "Epoch: 4569 | train_loss: 117.3989486694 | test_loss: 6.0509257317 | \n",
      "Epoch: 4570 | train_loss: 117.3987121582 | test_loss: 6.0508832932 | \n",
      "Epoch: 4571 | train_loss: 117.3984603882 | test_loss: 6.0508403778 | \n",
      "Epoch: 4572 | train_loss: 117.3982391357 | test_loss: 6.0507917404 | \n",
      "Epoch: 4573 | train_loss: 117.3979797363 | test_loss: 6.0507493019 | \n",
      "Epoch: 4574 | train_loss: 117.3977279663 | test_loss: 6.0507011414 | \n",
      "Epoch: 4575 | train_loss: 117.3975296021 | test_loss: 6.0506572723 | \n",
      "Epoch: 4576 | train_loss: 117.3972549438 | test_loss: 6.0506114960 | \n",
      "Epoch: 4577 | train_loss: 117.3970336914 | test_loss: 6.0505652428 | \n",
      "Epoch: 4578 | train_loss: 117.3967666626 | test_loss: 6.0505218506 | \n",
      "Epoch: 4579 | train_loss: 117.3965148926 | test_loss: 6.0504765511 | \n",
      "Epoch: 4580 | train_loss: 117.3962783813 | test_loss: 6.0504302979 | \n",
      "Epoch: 4581 | train_loss: 117.3960266113 | test_loss: 6.0503821373 | \n",
      "Epoch: 4582 | train_loss: 117.3957595825 | test_loss: 6.0503358841 | \n",
      "Epoch: 4583 | train_loss: 117.3954849243 | test_loss: 6.0502905846 | \n",
      "Epoch: 4584 | train_loss: 117.3952865601 | test_loss: 6.0502429008 | \n",
      "Epoch: 4585 | train_loss: 117.3950271606 | test_loss: 6.0501999855 | \n",
      "Epoch: 4586 | train_loss: 117.3948135376 | test_loss: 6.0501508713 | \n",
      "Epoch: 4587 | train_loss: 117.3945770264 | test_loss: 6.0501031876 | \n",
      "Epoch: 4588 | train_loss: 117.3943176270 | test_loss: 6.0500555038 | \n",
      "Epoch: 4589 | train_loss: 117.3940429688 | test_loss: 6.0500016212 | \n",
      "Epoch: 4590 | train_loss: 117.3938217163 | test_loss: 6.0499577522 | \n",
      "Epoch: 4591 | train_loss: 117.3935852051 | test_loss: 6.0499129295 | \n",
      "Epoch: 4592 | train_loss: 117.3933410645 | test_loss: 6.0498681068 | \n",
      "Epoch: 4593 | train_loss: 117.3930435181 | test_loss: 6.0498194695 | \n",
      "Epoch: 4594 | train_loss: 117.3927993774 | test_loss: 6.0497722626 | \n",
      "Epoch: 4595 | train_loss: 117.3925781250 | test_loss: 6.0497350693 | \n",
      "Epoch: 4596 | train_loss: 117.3923797607 | test_loss: 6.0496888161 | \n",
      "Epoch: 4597 | train_loss: 117.3920822144 | test_loss: 6.0496463776 | \n",
      "Epoch: 4598 | train_loss: 117.3918533325 | test_loss: 6.0495967865 | \n",
      "Epoch: 4599 | train_loss: 117.3916091919 | test_loss: 6.0495548248 | \n",
      "Epoch: 4600 | train_loss: 117.3913803101 | test_loss: 6.0495076180 | \n",
      "Epoch: 4601 | train_loss: 117.3911361694 | test_loss: 6.0494651794 | \n",
      "Epoch: 4602 | train_loss: 117.3908996582 | test_loss: 6.0494174957 | \n",
      "Epoch: 4603 | train_loss: 117.3906478882 | test_loss: 6.0493712425 | \n",
      "Epoch: 4604 | train_loss: 117.3904113770 | test_loss: 6.0493292809 | \n",
      "Epoch: 4605 | train_loss: 117.3901824951 | test_loss: 6.0492811203 | \n",
      "Epoch: 4606 | train_loss: 117.3899154663 | test_loss: 6.0492334366 | \n",
      "Epoch: 4607 | train_loss: 117.3896789551 | test_loss: 6.0491924286 | \n",
      "Epoch: 4608 | train_loss: 117.3894653320 | test_loss: 6.0491461754 | \n",
      "Epoch: 4609 | train_loss: 117.3892288208 | test_loss: 6.0491056442 | \n",
      "Epoch: 4610 | train_loss: 117.3890075684 | test_loss: 6.0490565300 | \n",
      "Epoch: 4611 | train_loss: 117.3887481689 | test_loss: 6.0490040779 | \n",
      "Epoch: 4612 | train_loss: 117.3885040283 | test_loss: 6.0489563942 | \n",
      "Epoch: 4613 | train_loss: 117.3882598877 | test_loss: 6.0489134789 | \n",
      "Epoch: 4614 | train_loss: 117.3880386353 | test_loss: 6.0488753319 | \n",
      "Epoch: 4615 | train_loss: 117.3877716064 | test_loss: 6.0488295555 | \n",
      "Epoch: 4616 | train_loss: 117.3875198364 | test_loss: 6.0487847328 | \n",
      "Epoch: 4617 | train_loss: 117.3872833252 | test_loss: 6.0487475395 | \n",
      "Epoch: 4618 | train_loss: 117.3870544434 | test_loss: 6.0487017632 | \n",
      "Epoch: 4619 | train_loss: 117.3867797852 | test_loss: 6.0486521721 | \n",
      "Epoch: 4620 | train_loss: 117.3865356445 | test_loss: 6.0486121178 | \n",
      "Epoch: 4621 | train_loss: 117.3863220215 | test_loss: 6.0485668182 | \n",
      "Epoch: 4622 | train_loss: 117.3860473633 | test_loss: 6.0485162735 | \n",
      "Epoch: 4623 | train_loss: 117.3858337402 | test_loss: 6.0484709740 | \n",
      "Epoch: 4624 | train_loss: 117.3855972290 | test_loss: 6.0484228134 | \n",
      "Epoch: 4625 | train_loss: 117.3853530884 | test_loss: 6.0483751297 | \n",
      "Epoch: 4626 | train_loss: 117.3851013184 | test_loss: 6.0483264923 | \n",
      "Epoch: 4627 | train_loss: 117.3848571777 | test_loss: 6.0482816696 | \n",
      "Epoch: 4628 | train_loss: 117.3845977783 | test_loss: 6.0482349396 | \n",
      "Epoch: 4629 | train_loss: 117.3843231201 | test_loss: 6.0481934547 | \n",
      "Epoch: 4630 | train_loss: 117.3840637207 | test_loss: 6.0481462479 | \n",
      "Epoch: 4631 | train_loss: 117.3838653564 | test_loss: 6.0481023788 | \n",
      "Epoch: 4632 | train_loss: 117.3836364746 | test_loss: 6.0480513573 | \n",
      "Epoch: 4633 | train_loss: 117.3833770752 | test_loss: 6.0480089188 | \n",
      "Epoch: 4634 | train_loss: 117.3831176758 | test_loss: 6.0479679108 | \n",
      "Epoch: 4635 | train_loss: 117.3828887939 | test_loss: 6.0479240417 | \n",
      "Epoch: 4636 | train_loss: 117.3826599121 | test_loss: 6.0478773117 | \n",
      "Epoch: 4637 | train_loss: 117.3824157715 | test_loss: 6.0478296280 | \n",
      "Epoch: 4638 | train_loss: 117.3821792603 | test_loss: 6.0477948189 | \n",
      "Epoch: 4639 | train_loss: 117.3819503784 | test_loss: 6.0477428436 | \n",
      "Epoch: 4640 | train_loss: 117.3817214966 | test_loss: 6.0476975441 | \n",
      "Epoch: 4641 | train_loss: 117.3814849854 | test_loss: 6.0476484299 | \n",
      "Epoch: 4642 | train_loss: 117.3812332153 | test_loss: 6.0476069450 | \n",
      "Epoch: 4643 | train_loss: 117.3809890747 | test_loss: 6.0475616455 | \n",
      "Epoch: 4644 | train_loss: 117.3807144165 | test_loss: 6.0475172997 | \n",
      "Epoch: 4645 | train_loss: 117.3804855347 | test_loss: 6.0474705696 | \n",
      "Epoch: 4646 | train_loss: 117.3802413940 | test_loss: 6.0474228859 | \n",
      "Epoch: 4647 | train_loss: 117.3799896240 | test_loss: 6.0473790169 | \n",
      "Epoch: 4648 | train_loss: 117.3797531128 | test_loss: 6.0473356247 | \n",
      "Epoch: 4649 | train_loss: 117.3795013428 | test_loss: 6.0472888947 | \n",
      "Epoch: 4650 | train_loss: 117.3792648315 | test_loss: 6.0472455025 | \n",
      "Epoch: 4651 | train_loss: 117.3790130615 | test_loss: 6.0472035408 | \n",
      "Epoch: 4652 | train_loss: 117.3787918091 | test_loss: 6.0471558571 | \n",
      "Epoch: 4653 | train_loss: 117.3785476685 | test_loss: 6.0471067429 | \n",
      "Epoch: 4654 | train_loss: 117.3783187866 | test_loss: 6.0470643044 | \n",
      "Epoch: 4655 | train_loss: 117.3780670166 | test_loss: 6.0470194817 | \n",
      "Epoch: 4656 | train_loss: 117.3778305054 | test_loss: 6.0469751358 | \n",
      "Epoch: 4657 | train_loss: 117.3776092529 | test_loss: 6.0469293594 | \n",
      "Epoch: 4658 | train_loss: 117.3773651123 | test_loss: 6.0468831062 | \n",
      "Epoch: 4659 | train_loss: 117.3770675659 | test_loss: 6.0468425751 | \n",
      "Epoch: 4660 | train_loss: 117.3768539429 | test_loss: 6.0468010902 | \n",
      "Epoch: 4661 | train_loss: 117.3766174316 | test_loss: 6.0467567444 | \n",
      "Epoch: 4662 | train_loss: 117.3763885498 | test_loss: 6.0467090607 | \n",
      "Epoch: 4663 | train_loss: 117.3761444092 | test_loss: 6.0466656685 | \n",
      "Epoch: 4664 | train_loss: 117.3759384155 | test_loss: 6.0466213226 | \n",
      "Epoch: 4665 | train_loss: 117.3757019043 | test_loss: 6.0465769768 | \n",
      "Epoch: 4666 | train_loss: 117.3754577637 | test_loss: 6.0465307236 | \n",
      "Epoch: 4667 | train_loss: 117.3751983643 | test_loss: 6.0464887619 | \n",
      "Epoch: 4668 | train_loss: 117.3749313354 | test_loss: 6.0464434624 | \n",
      "Epoch: 4669 | train_loss: 117.3746795654 | test_loss: 6.0463886261 | \n",
      "Epoch: 4670 | train_loss: 117.3744354248 | test_loss: 6.0463495255 | \n",
      "Epoch: 4671 | train_loss: 117.3741989136 | test_loss: 6.0463023186 | \n",
      "Epoch: 4672 | train_loss: 117.3739852905 | test_loss: 6.0462541580 | \n",
      "Epoch: 4673 | train_loss: 117.3737030029 | test_loss: 6.0462102890 | \n",
      "Epoch: 4674 | train_loss: 117.3734436035 | test_loss: 6.0461649895 | \n",
      "Epoch: 4675 | train_loss: 117.3732223511 | test_loss: 6.0461177826 | \n",
      "Epoch: 4676 | train_loss: 117.3729782104 | test_loss: 6.0460705757 | \n",
      "Epoch: 4677 | train_loss: 117.3727416992 | test_loss: 6.0460262299 | \n",
      "Epoch: 4678 | train_loss: 117.3725128174 | test_loss: 6.0459837914 | \n",
      "Epoch: 4679 | train_loss: 117.3722381592 | test_loss: 6.0459375381 | \n",
      "Epoch: 4680 | train_loss: 117.3720016479 | test_loss: 6.0458917618 | \n",
      "Epoch: 4681 | train_loss: 117.3717956543 | test_loss: 6.0458455086 | \n",
      "Epoch: 4682 | train_loss: 117.3715591431 | test_loss: 6.0458021164 | \n",
      "Epoch: 4683 | train_loss: 117.3713302612 | test_loss: 6.0457596779 | \n",
      "Epoch: 4684 | train_loss: 117.3710937500 | test_loss: 6.0457172394 | \n",
      "Epoch: 4685 | train_loss: 117.3708267212 | test_loss: 6.0456719398 | \n",
      "Epoch: 4686 | train_loss: 117.3705978394 | test_loss: 6.0456266403 | \n",
      "Epoch: 4687 | train_loss: 117.3703536987 | test_loss: 6.0455822945 | \n",
      "Epoch: 4688 | train_loss: 117.3700866699 | test_loss: 6.0455365181 | \n",
      "Epoch: 4689 | train_loss: 117.3698959351 | test_loss: 6.0454893112 | \n",
      "Epoch: 4690 | train_loss: 117.3695907593 | test_loss: 6.0454435349 | \n",
      "Epoch: 4691 | train_loss: 117.3693923950 | test_loss: 6.0453948975 | \n",
      "Epoch: 4692 | train_loss: 117.3691482544 | test_loss: 6.0453476906 | \n",
      "Epoch: 4693 | train_loss: 117.3688964844 | test_loss: 6.0453071594 | \n",
      "Epoch: 4694 | train_loss: 117.3686599731 | test_loss: 6.0452637672 | \n",
      "Epoch: 4695 | train_loss: 117.3684158325 | test_loss: 6.0452198982 | \n",
      "Epoch: 4696 | train_loss: 117.3681869507 | test_loss: 6.0451741219 | \n",
      "Epoch: 4697 | train_loss: 117.3679580688 | test_loss: 6.0451297760 | \n",
      "Epoch: 4698 | train_loss: 117.3677139282 | test_loss: 6.0450878143 | \n",
      "Epoch: 4699 | train_loss: 117.3674926758 | test_loss: 6.0450444221 | \n",
      "Epoch: 4700 | train_loss: 117.3672637939 | test_loss: 6.0449981689 | \n",
      "Epoch: 4701 | train_loss: 117.3670120239 | test_loss: 6.0449519157 | \n",
      "Epoch: 4702 | train_loss: 117.3667984009 | test_loss: 6.0449080467 | \n",
      "Epoch: 4703 | train_loss: 117.3665313721 | test_loss: 6.0448651314 | \n",
      "Epoch: 4704 | train_loss: 117.3662872314 | test_loss: 6.0448155403 | \n",
      "Epoch: 4705 | train_loss: 117.3660430908 | test_loss: 6.0447783470 | \n",
      "Epoch: 4706 | train_loss: 117.3658523560 | test_loss: 6.0447363853 | \n",
      "Epoch: 4707 | train_loss: 117.3656311035 | test_loss: 6.0446977615 | \n",
      "Epoch: 4708 | train_loss: 117.3654098511 | test_loss: 6.0446510315 | \n",
      "Epoch: 4709 | train_loss: 117.3651733398 | test_loss: 6.0446076393 | \n",
      "Epoch: 4710 | train_loss: 117.3649291992 | test_loss: 6.0445623398 | \n",
      "Epoch: 4711 | train_loss: 117.3647079468 | test_loss: 6.0445184708 | \n",
      "Epoch: 4712 | train_loss: 117.3644638062 | test_loss: 6.0444765091 | \n",
      "Epoch: 4713 | train_loss: 117.3642196655 | test_loss: 6.0444307327 | \n",
      "Epoch: 4714 | train_loss: 117.3639907837 | test_loss: 6.0443797112 | \n",
      "Epoch: 4715 | train_loss: 117.3637313843 | test_loss: 6.0443363190 | \n",
      "Epoch: 4716 | train_loss: 117.3634948730 | test_loss: 6.0442924500 | \n",
      "Epoch: 4717 | train_loss: 117.3632507324 | test_loss: 6.0442466736 | \n",
      "Epoch: 4718 | train_loss: 117.3629913330 | test_loss: 6.0442032814 | \n",
      "Epoch: 4719 | train_loss: 117.3627548218 | test_loss: 6.0441551208 | \n",
      "Epoch: 4720 | train_loss: 117.3625183105 | test_loss: 6.0441126823 | \n",
      "Epoch: 4721 | train_loss: 117.3622665405 | test_loss: 6.0440654755 | \n",
      "Epoch: 4722 | train_loss: 117.3620147705 | test_loss: 6.0440211296 | \n",
      "Epoch: 4723 | train_loss: 117.3617858887 | test_loss: 6.0439743996 | \n",
      "Epoch: 4724 | train_loss: 117.3615722656 | test_loss: 6.0439319611 | \n",
      "Epoch: 4725 | train_loss: 117.3613052368 | test_loss: 6.0438861847 | \n",
      "Epoch: 4726 | train_loss: 117.3610534668 | test_loss: 6.0438451767 | \n",
      "Epoch: 4727 | train_loss: 117.3608398438 | test_loss: 6.0437984467 | \n",
      "Epoch: 4728 | train_loss: 117.3605728149 | test_loss: 6.0437541008 | \n",
      "Epoch: 4729 | train_loss: 117.3603286743 | test_loss: 6.0437126160 | \n",
      "Epoch: 4730 | train_loss: 117.3600997925 | test_loss: 6.0436649323 | \n",
      "Epoch: 4731 | train_loss: 117.3598632812 | test_loss: 6.0436244011 | \n",
      "Epoch: 4732 | train_loss: 117.3596115112 | test_loss: 6.0435771942 | \n",
      "Epoch: 4733 | train_loss: 117.3593444824 | test_loss: 6.0435371399 | \n",
      "Epoch: 4734 | train_loss: 117.3591537476 | test_loss: 6.0434937477 | \n",
      "Epoch: 4735 | train_loss: 117.3589096069 | test_loss: 6.0434460640 | \n",
      "Epoch: 4736 | train_loss: 117.3586349487 | test_loss: 6.0434045792 | \n",
      "Epoch: 4737 | train_loss: 117.3583984375 | test_loss: 6.0433564186 | \n",
      "Epoch: 4738 | train_loss: 117.3581542969 | test_loss: 6.0433111191 | \n",
      "Epoch: 4739 | train_loss: 117.3579330444 | test_loss: 6.0432658195 | \n",
      "Epoch: 4740 | train_loss: 117.3577041626 | test_loss: 6.0432243347 | \n",
      "Epoch: 4741 | train_loss: 117.3574523926 | test_loss: 6.0431785583 | \n",
      "Epoch: 4742 | train_loss: 117.3572158813 | test_loss: 6.0431327820 | \n",
      "Epoch: 4743 | train_loss: 117.3569793701 | test_loss: 6.0430836678 | \n",
      "Epoch: 4744 | train_loss: 117.3567276001 | test_loss: 6.0430388451 | \n",
      "Epoch: 4745 | train_loss: 117.3565063477 | test_loss: 6.0429954529 | \n",
      "Epoch: 4746 | train_loss: 117.3562622070 | test_loss: 6.0429530144 | \n",
      "Epoch: 4747 | train_loss: 117.3560028076 | test_loss: 6.0429062843 | \n",
      "Epoch: 4748 | train_loss: 117.3557586670 | test_loss: 6.0428605080 | \n",
      "Epoch: 4749 | train_loss: 117.3555297852 | test_loss: 6.0428252220 | \n",
      "Epoch: 4750 | train_loss: 117.3552703857 | test_loss: 6.0427770615 | \n",
      "Epoch: 4751 | train_loss: 117.3550491333 | test_loss: 6.0427403450 | \n",
      "Epoch: 4752 | train_loss: 117.3548126221 | test_loss: 6.0426936150 | \n",
      "Epoch: 4753 | train_loss: 117.3545684814 | test_loss: 6.0426540375 | \n",
      "Epoch: 4754 | train_loss: 117.3543395996 | test_loss: 6.0426039696 | \n",
      "Epoch: 4755 | train_loss: 117.3540649414 | test_loss: 6.0425553322 | \n",
      "Epoch: 4756 | train_loss: 117.3537902832 | test_loss: 6.0425128937 | \n",
      "Epoch: 4757 | train_loss: 117.3535614014 | test_loss: 6.0424623489 | \n",
      "Epoch: 4758 | train_loss: 117.3533401489 | test_loss: 6.0424170494 | \n",
      "Epoch: 4759 | train_loss: 117.3530883789 | test_loss: 6.0423731804 | \n",
      "Epoch: 4760 | train_loss: 117.3528594971 | test_loss: 6.0423240662 | \n",
      "Epoch: 4761 | train_loss: 117.3526306152 | test_loss: 6.0422806740 | \n",
      "Epoch: 4762 | train_loss: 117.3523941040 | test_loss: 6.0422377586 | \n",
      "Epoch: 4763 | train_loss: 117.3521881104 | test_loss: 6.0421919823 | \n",
      "Epoch: 4764 | train_loss: 117.3519439697 | test_loss: 6.0421476364 | \n",
      "Epoch: 4765 | train_loss: 117.3516998291 | test_loss: 6.0421075821 | \n",
      "Epoch: 4766 | train_loss: 117.3514633179 | test_loss: 6.0420660973 | \n",
      "Epoch: 4767 | train_loss: 117.3512725830 | test_loss: 6.0420227051 | \n",
      "Epoch: 4768 | train_loss: 117.3510055542 | test_loss: 6.0419764519 | \n",
      "Epoch: 4769 | train_loss: 117.3507690430 | test_loss: 6.0419373512 | \n",
      "Epoch: 4770 | train_loss: 117.3505630493 | test_loss: 6.0418944359 | \n",
      "Epoch: 4771 | train_loss: 117.3503341675 | test_loss: 6.0418519974 | \n",
      "Epoch: 4772 | train_loss: 117.3500671387 | test_loss: 6.0418071747 | \n",
      "Epoch: 4773 | train_loss: 117.3498458862 | test_loss: 6.0417675972 | \n",
      "Epoch: 4774 | train_loss: 117.3496322632 | test_loss: 6.0417175293 | \n",
      "Epoch: 4775 | train_loss: 117.3493576050 | test_loss: 6.0416717529 | \n",
      "Epoch: 4776 | train_loss: 117.3491592407 | test_loss: 6.0416283607 | \n",
      "Epoch: 4777 | train_loss: 117.3489074707 | test_loss: 6.0415935516 | \n",
      "Epoch: 4778 | train_loss: 117.3487091064 | test_loss: 6.0415477753 | \n",
      "Epoch: 4779 | train_loss: 117.3484649658 | test_loss: 6.0415101051 | \n",
      "Epoch: 4780 | train_loss: 117.3482131958 | test_loss: 6.0414662361 | \n",
      "Epoch: 4781 | train_loss: 117.3479766846 | test_loss: 6.0414180756 | \n",
      "Epoch: 4782 | train_loss: 117.3477478027 | test_loss: 6.0413746834 | \n",
      "Epoch: 4783 | train_loss: 117.3475418091 | test_loss: 6.0413317680 | \n",
      "Epoch: 4784 | train_loss: 117.3473129272 | test_loss: 6.0412888527 | \n",
      "Epoch: 4785 | train_loss: 117.3470916748 | test_loss: 6.0412464142 | \n",
      "Epoch: 4786 | train_loss: 117.3468551636 | test_loss: 6.0412049294 | \n",
      "Epoch: 4787 | train_loss: 117.3466110229 | test_loss: 6.0411601067 | \n",
      "Epoch: 4788 | train_loss: 117.3463897705 | test_loss: 6.0411162376 | \n",
      "Epoch: 4789 | train_loss: 117.3461456299 | test_loss: 6.0410718918 | \n",
      "Epoch: 4790 | train_loss: 117.3458709717 | test_loss: 6.0410270691 | \n",
      "Epoch: 4791 | train_loss: 117.3456344604 | test_loss: 6.0409870148 | \n",
      "Epoch: 4792 | train_loss: 117.3454208374 | test_loss: 6.0409412384 | \n",
      "Epoch: 4793 | train_loss: 117.3452148438 | test_loss: 6.0408954620 | \n",
      "Epoch: 4794 | train_loss: 117.3449325562 | test_loss: 6.0408520699 | \n",
      "Epoch: 4795 | train_loss: 117.3447265625 | test_loss: 6.0408129692 | \n",
      "Epoch: 4796 | train_loss: 117.3444747925 | test_loss: 6.0407671928 | \n",
      "Epoch: 4797 | train_loss: 117.3442459106 | test_loss: 6.0407218933 | \n",
      "Epoch: 4798 | train_loss: 117.3439941406 | test_loss: 6.0406761169 | \n",
      "Epoch: 4799 | train_loss: 117.3437805176 | test_loss: 6.0406365395 | \n",
      "Epoch: 4800 | train_loss: 117.3435440063 | test_loss: 6.0405950546 | \n",
      "Epoch: 4801 | train_loss: 117.3433074951 | test_loss: 6.0405468941 | \n",
      "Epoch: 4802 | train_loss: 117.3430557251 | test_loss: 6.0405015945 | \n",
      "Epoch: 4803 | train_loss: 117.3428115845 | test_loss: 6.0404615402 | \n",
      "Epoch: 4804 | train_loss: 117.3425903320 | test_loss: 6.0404148102 | \n",
      "Epoch: 4805 | train_loss: 117.3423461914 | test_loss: 6.0403656960 | \n",
      "Epoch: 4806 | train_loss: 117.3421173096 | test_loss: 6.0403227806 | \n",
      "Epoch: 4807 | train_loss: 117.3419342041 | test_loss: 6.0402817726 | \n",
      "Epoch: 4808 | train_loss: 117.3416824341 | test_loss: 6.0402402878 | \n",
      "Epoch: 4809 | train_loss: 117.3414230347 | test_loss: 6.0401959419 | \n",
      "Epoch: 4810 | train_loss: 117.3412017822 | test_loss: 6.0401515961 | \n",
      "Epoch: 4811 | train_loss: 117.3409500122 | test_loss: 6.0401039124 | \n",
      "Epoch: 4812 | train_loss: 117.3407440186 | test_loss: 6.0400581360 | \n",
      "Epoch: 4813 | train_loss: 117.3404846191 | test_loss: 6.0400166512 | \n",
      "Epoch: 4814 | train_loss: 117.3402709961 | test_loss: 6.0399675369 | \n",
      "Epoch: 4815 | train_loss: 117.3400192261 | test_loss: 6.0399327278 | \n",
      "Epoch: 4816 | train_loss: 117.3397827148 | test_loss: 6.0398893356 | \n",
      "Epoch: 4817 | train_loss: 117.3395919800 | test_loss: 6.0398464203 | \n",
      "Epoch: 4818 | train_loss: 117.3393478394 | test_loss: 6.0397987366 | \n",
      "Epoch: 4819 | train_loss: 117.3391189575 | test_loss: 6.0397586823 | \n",
      "Epoch: 4820 | train_loss: 117.3389053345 | test_loss: 6.0397162437 | \n",
      "Epoch: 4821 | train_loss: 117.3386840820 | test_loss: 6.0396704674 | \n",
      "Epoch: 4822 | train_loss: 117.3384475708 | test_loss: 6.0396285057 | \n",
      "Epoch: 4823 | train_loss: 117.3381729126 | test_loss: 6.0395879745 | \n",
      "Epoch: 4824 | train_loss: 117.3379592896 | test_loss: 6.0395441055 | \n",
      "Epoch: 4825 | train_loss: 117.3377380371 | test_loss: 6.0395002365 | \n",
      "Epoch: 4826 | train_loss: 117.3374938965 | test_loss: 6.0394549370 | \n",
      "Epoch: 4827 | train_loss: 117.3372955322 | test_loss: 6.0394163132 | \n",
      "Epoch: 4828 | train_loss: 117.3370513916 | test_loss: 6.0393681526 | \n",
      "Epoch: 4829 | train_loss: 117.3368072510 | test_loss: 6.0393238068 | \n",
      "Epoch: 4830 | train_loss: 117.3365707397 | test_loss: 6.0392785072 | \n",
      "Epoch: 4831 | train_loss: 117.3363189697 | test_loss: 6.0392322540 | \n",
      "Epoch: 4832 | train_loss: 117.3360824585 | test_loss: 6.0391874313 | \n",
      "Epoch: 4833 | train_loss: 117.3358459473 | test_loss: 6.0391449928 | \n",
      "Epoch: 4834 | train_loss: 117.3356246948 | test_loss: 6.0390954018 | \n",
      "Epoch: 4835 | train_loss: 117.3353881836 | test_loss: 6.0390605927 | \n",
      "Epoch: 4836 | train_loss: 117.3351669312 | test_loss: 6.0390191078 | \n",
      "Epoch: 4837 | train_loss: 117.3349609375 | test_loss: 6.0389695168 | \n",
      "Epoch: 4838 | train_loss: 117.3346939087 | test_loss: 6.0389275551 | \n",
      "Epoch: 4839 | train_loss: 117.3344650269 | test_loss: 6.0388836861 | \n",
      "Epoch: 4840 | train_loss: 117.3342208862 | test_loss: 6.0388360023 | \n",
      "Epoch: 4841 | train_loss: 117.3340072632 | test_loss: 6.0387988091 | \n",
      "Epoch: 4842 | train_loss: 117.3337402344 | test_loss: 6.0387611389 | \n",
      "Epoch: 4843 | train_loss: 117.3335189819 | test_loss: 6.0387153625 | \n",
      "Epoch: 4844 | train_loss: 117.3332977295 | test_loss: 6.0386767387 | \n",
      "Epoch: 4845 | train_loss: 117.3330535889 | test_loss: 6.0386290550 | \n",
      "Epoch: 4846 | train_loss: 117.3328323364 | test_loss: 6.0385875702 | \n",
      "Epoch: 4847 | train_loss: 117.3326187134 | test_loss: 6.0385422707 | \n",
      "Epoch: 4848 | train_loss: 117.3323669434 | test_loss: 6.0384941101 | \n",
      "Epoch: 4849 | train_loss: 117.3321304321 | test_loss: 6.0384492874 | \n",
      "Epoch: 4850 | train_loss: 117.3318862915 | test_loss: 6.0384101868 | \n",
      "Epoch: 4851 | train_loss: 117.3316574097 | test_loss: 6.0383667946 | \n",
      "Epoch: 4852 | train_loss: 117.3314437866 | test_loss: 6.0383300781 | \n",
      "Epoch: 4853 | train_loss: 117.3311691284 | test_loss: 6.0382828712 | \n",
      "Epoch: 4854 | train_loss: 117.3309631348 | test_loss: 6.0382413864 | \n",
      "Epoch: 4855 | train_loss: 117.3307418823 | test_loss: 6.0381999016 | \n",
      "Epoch: 4856 | train_loss: 117.3304977417 | test_loss: 6.0381584167 | \n",
      "Epoch: 4857 | train_loss: 117.3302764893 | test_loss: 6.0381097794 | \n",
      "Epoch: 4858 | train_loss: 117.3300170898 | test_loss: 6.0380644798 | \n",
      "Epoch: 4859 | train_loss: 117.3297882080 | test_loss: 6.0380201340 | \n",
      "Epoch: 4860 | train_loss: 117.3295288086 | test_loss: 6.0379815102 | \n",
      "Epoch: 4861 | train_loss: 117.3293151855 | test_loss: 6.0379352570 | \n",
      "Epoch: 4862 | train_loss: 117.3290939331 | test_loss: 6.0378971100 | \n",
      "Epoch: 4863 | train_loss: 117.3288574219 | test_loss: 6.0378513336 | \n",
      "Epoch: 4864 | train_loss: 117.3286209106 | test_loss: 6.0378098488 | \n",
      "Epoch: 4865 | train_loss: 117.3283996582 | test_loss: 6.0377678871 | \n",
      "Epoch: 4866 | train_loss: 117.3281631470 | test_loss: 6.0377235413 | \n",
      "Epoch: 4867 | train_loss: 117.3279418945 | test_loss: 6.0376796722 | \n",
      "Epoch: 4868 | train_loss: 117.3276977539 | test_loss: 6.0376353264 | \n",
      "Epoch: 4869 | train_loss: 117.3274612427 | test_loss: 6.0375905037 | \n",
      "Epoch: 4870 | train_loss: 117.3272247314 | test_loss: 6.0375447273 | \n",
      "Epoch: 4871 | train_loss: 117.3270034790 | test_loss: 6.0374999046 | \n",
      "Epoch: 4872 | train_loss: 117.3267745972 | test_loss: 6.0374498367 | \n",
      "Epoch: 4873 | train_loss: 117.3265151978 | test_loss: 6.0374116898 | \n",
      "Epoch: 4874 | train_loss: 117.3262863159 | test_loss: 6.0373725891 | \n",
      "Epoch: 4875 | train_loss: 117.3260803223 | test_loss: 6.0373287201 | \n",
      "Epoch: 4876 | train_loss: 117.3258514404 | test_loss: 6.0372838974 | \n",
      "Epoch: 4877 | train_loss: 117.3256759644 | test_loss: 6.0372385979 | \n",
      "Epoch: 4878 | train_loss: 117.3254013062 | test_loss: 6.0371947289 | \n",
      "Epoch: 4879 | train_loss: 117.3251953125 | test_loss: 6.0371551514 | \n",
      "Epoch: 4880 | train_loss: 117.3249588013 | test_loss: 6.0371079445 | \n",
      "Epoch: 4881 | train_loss: 117.3246841431 | test_loss: 6.0370631218 | \n",
      "Epoch: 4882 | train_loss: 117.3244323730 | test_loss: 6.0370202065 | \n",
      "Epoch: 4883 | train_loss: 117.3241882324 | test_loss: 6.0369782448 | \n",
      "Epoch: 4884 | train_loss: 117.3239364624 | test_loss: 6.0369329453 | \n",
      "Epoch: 4885 | train_loss: 117.3237075806 | test_loss: 6.0368862152 | \n",
      "Epoch: 4886 | train_loss: 117.3234634399 | test_loss: 6.0368461609 | \n",
      "Epoch: 4887 | train_loss: 117.3232421875 | test_loss: 6.0367980003 | \n",
      "Epoch: 4888 | train_loss: 117.3230056763 | test_loss: 6.0367569923 | \n",
      "Epoch: 4889 | train_loss: 117.3228302002 | test_loss: 6.0367102623 | \n",
      "Epoch: 4890 | train_loss: 117.3225555420 | test_loss: 6.0366649628 | \n",
      "Epoch: 4891 | train_loss: 117.3223266602 | test_loss: 6.0366215706 | \n",
      "Epoch: 4892 | train_loss: 117.3221206665 | test_loss: 6.0365777016 | \n",
      "Epoch: 4893 | train_loss: 117.3218460083 | test_loss: 6.0365362167 | \n",
      "Epoch: 4894 | train_loss: 117.3216323853 | test_loss: 6.0364904404 | \n",
      "Epoch: 4895 | train_loss: 117.3214035034 | test_loss: 6.0364494324 | \n",
      "Epoch: 4896 | train_loss: 117.3211593628 | test_loss: 6.0364112854 | \n",
      "Epoch: 4897 | train_loss: 117.3209457397 | test_loss: 6.0363674164 | \n",
      "Epoch: 4898 | train_loss: 117.3206939697 | test_loss: 6.0363326073 | \n",
      "Epoch: 4899 | train_loss: 117.3204956055 | test_loss: 6.0362939835 | \n",
      "Epoch: 4900 | train_loss: 117.3202667236 | test_loss: 6.0362534523 | \n",
      "Epoch: 4901 | train_loss: 117.3200073242 | test_loss: 6.0362138748 | \n",
      "Epoch: 4902 | train_loss: 117.3198013306 | test_loss: 6.0361676216 | \n",
      "Epoch: 4903 | train_loss: 117.3195495605 | test_loss: 6.0361213684 | \n",
      "Epoch: 4904 | train_loss: 117.3193283081 | test_loss: 6.0360774994 | \n",
      "Epoch: 4905 | train_loss: 117.3190841675 | test_loss: 6.0360302925 | \n",
      "Epoch: 4906 | train_loss: 117.3188552856 | test_loss: 6.0359888077 | \n",
      "Epoch: 4907 | train_loss: 117.3186340332 | test_loss: 6.0359435081 | \n",
      "Epoch: 4908 | train_loss: 117.3183364868 | test_loss: 6.0358996391 | \n",
      "Epoch: 4909 | train_loss: 117.3181457520 | test_loss: 6.0358600616 | \n",
      "Epoch: 4910 | train_loss: 117.3179168701 | test_loss: 6.0358166695 | \n",
      "Epoch: 4911 | train_loss: 117.3177261353 | test_loss: 6.0357775688 | \n",
      "Epoch: 4912 | train_loss: 117.3174743652 | test_loss: 6.0357303619 | \n",
      "Epoch: 4913 | train_loss: 117.3172531128 | test_loss: 6.0356922150 | \n",
      "Epoch: 4914 | train_loss: 117.3170318604 | test_loss: 6.0356450081 | \n",
      "Epoch: 4915 | train_loss: 117.3167953491 | test_loss: 6.0356035233 | \n",
      "Epoch: 4916 | train_loss: 117.3165359497 | test_loss: 6.0355615616 | \n",
      "Epoch: 4917 | train_loss: 117.3163070679 | test_loss: 6.0355162621 | \n",
      "Epoch: 4918 | train_loss: 117.3160476685 | test_loss: 6.0354762077 | \n",
      "Epoch: 4919 | train_loss: 117.3158264160 | test_loss: 6.0354356766 | \n",
      "Epoch: 4920 | train_loss: 117.3155975342 | test_loss: 6.0353932381 | \n",
      "Epoch: 4921 | train_loss: 117.3153610229 | test_loss: 6.0353484154 | \n",
      "Epoch: 4922 | train_loss: 117.3151397705 | test_loss: 6.0353069305 | \n",
      "Epoch: 4923 | train_loss: 117.3149032593 | test_loss: 6.0352668762 | \n",
      "Epoch: 4924 | train_loss: 117.3146591187 | test_loss: 6.0352230072 | \n",
      "Epoch: 4925 | train_loss: 117.3143844604 | test_loss: 6.0351843834 | \n",
      "Epoch: 4926 | train_loss: 117.3141479492 | test_loss: 6.0351381302 | \n",
      "Epoch: 4927 | train_loss: 117.3139419556 | test_loss: 6.0350966454 | \n",
      "Epoch: 4928 | train_loss: 117.3136978149 | test_loss: 6.0350499153 | \n",
      "Epoch: 4929 | train_loss: 117.3134460449 | test_loss: 6.0350084305 | \n",
      "Epoch: 4930 | train_loss: 117.3132400513 | test_loss: 6.0349650383 | \n",
      "Epoch: 4931 | train_loss: 117.3130187988 | test_loss: 6.0349245071 | \n",
      "Epoch: 4932 | train_loss: 117.3127822876 | test_loss: 6.0348787308 | \n",
      "Epoch: 4933 | train_loss: 117.3125076294 | test_loss: 6.0348420143 | \n",
      "Epoch: 4934 | train_loss: 117.3123474121 | test_loss: 6.0347962379 | \n",
      "Epoch: 4935 | train_loss: 117.3120803833 | test_loss: 6.0347552299 | \n",
      "Epoch: 4936 | train_loss: 117.3118362427 | test_loss: 6.0347137451 | \n",
      "Epoch: 4937 | train_loss: 117.3116302490 | test_loss: 6.0346679688 | \n",
      "Epoch: 4938 | train_loss: 117.3113708496 | test_loss: 6.0346240997 | \n",
      "Epoch: 4939 | train_loss: 117.3111495972 | test_loss: 6.0345835686 | \n",
      "Epoch: 4940 | train_loss: 117.3109207153 | test_loss: 6.0345430374 | \n",
      "Epoch: 4941 | train_loss: 117.3107299805 | test_loss: 6.0344986916 | \n",
      "Epoch: 4942 | train_loss: 117.3104782104 | test_loss: 6.0344572067 | \n",
      "Epoch: 4943 | train_loss: 117.3102645874 | test_loss: 6.0344123840 | \n",
      "Epoch: 4944 | train_loss: 117.3100128174 | test_loss: 6.0343694687 | \n",
      "Epoch: 4945 | train_loss: 117.3097763062 | test_loss: 6.0343255997 | \n",
      "Epoch: 4946 | train_loss: 117.3095092773 | test_loss: 6.0342864990 | \n",
      "Epoch: 4947 | train_loss: 117.3093032837 | test_loss: 6.0342388153 | \n",
      "Epoch: 4948 | train_loss: 117.3090744019 | test_loss: 6.0341963768 | \n",
      "Epoch: 4949 | train_loss: 117.3088607788 | test_loss: 6.0341534615 | \n",
      "Epoch: 4950 | train_loss: 117.3085937500 | test_loss: 6.0341105461 | \n",
      "Epoch: 4951 | train_loss: 117.3083724976 | test_loss: 6.0340700150 | \n",
      "Epoch: 4952 | train_loss: 117.3081741333 | test_loss: 6.0340275764 | \n",
      "Epoch: 4953 | train_loss: 117.3079299927 | test_loss: 6.0339818001 | \n",
      "Epoch: 4954 | train_loss: 117.3076858521 | test_loss: 6.0339417458 | \n",
      "Epoch: 4955 | train_loss: 117.3074188232 | test_loss: 6.0339026451 | \n",
      "Epoch: 4956 | train_loss: 117.3071975708 | test_loss: 6.0338616371 | \n",
      "Epoch: 4957 | train_loss: 117.3069839478 | test_loss: 6.0338153839 | \n",
      "Epoch: 4958 | train_loss: 117.3067398071 | test_loss: 6.0337781906 | \n",
      "Epoch: 4959 | train_loss: 117.3065338135 | test_loss: 6.0337333679 | \n",
      "Epoch: 4960 | train_loss: 117.3063201904 | test_loss: 6.0336894989 | \n",
      "Epoch: 4961 | train_loss: 117.3060913086 | test_loss: 6.0336480141 | \n",
      "Epoch: 4962 | train_loss: 117.3058624268 | test_loss: 6.0336046219 | \n",
      "Epoch: 4963 | train_loss: 117.3056182861 | test_loss: 6.0335564613 | \n",
      "Epoch: 4964 | train_loss: 117.3053741455 | test_loss: 6.0335173607 | \n",
      "Epoch: 4965 | train_loss: 117.3051528931 | test_loss: 6.0334773064 | \n",
      "Epoch: 4966 | train_loss: 117.3049011230 | test_loss: 6.0334324837 | \n",
      "Epoch: 4967 | train_loss: 117.3046951294 | test_loss: 6.0333876610 | \n",
      "Epoch: 4968 | train_loss: 117.3044509888 | test_loss: 6.0333471298 | \n",
      "Epoch: 4969 | train_loss: 117.3042373657 | test_loss: 6.0333118439 | \n",
      "Epoch: 4970 | train_loss: 117.3040466309 | test_loss: 6.0332736969 | \n",
      "Epoch: 4971 | train_loss: 117.3038101196 | test_loss: 6.0332293510 | \n",
      "Epoch: 4972 | train_loss: 117.3036193848 | test_loss: 6.0331883430 | \n",
      "Epoch: 4973 | train_loss: 117.3033523560 | test_loss: 6.0331511497 | \n",
      "Epoch: 4974 | train_loss: 117.3031463623 | test_loss: 6.0331110954 | \n",
      "Epoch: 4975 | train_loss: 117.3029174805 | test_loss: 6.0330681801 | \n",
      "Epoch: 4976 | train_loss: 117.3026733398 | test_loss: 6.0330271721 | \n",
      "Epoch: 4977 | train_loss: 117.3024826050 | test_loss: 6.0329833031 | \n",
      "Epoch: 4978 | train_loss: 117.3022460938 | test_loss: 6.0329389572 | \n",
      "Epoch: 4979 | train_loss: 117.3020095825 | test_loss: 6.0328917503 | \n",
      "Epoch: 4980 | train_loss: 117.3017654419 | test_loss: 6.0328493118 | \n",
      "Epoch: 4981 | train_loss: 117.3015747070 | test_loss: 6.0328044891 | \n",
      "Epoch: 4982 | train_loss: 117.3013076782 | test_loss: 6.0327606201 | \n",
      "Epoch: 4983 | train_loss: 117.3010864258 | test_loss: 6.0327243805 | \n",
      "Epoch: 4984 | train_loss: 117.3008499146 | test_loss: 6.0326828957 | \n",
      "Epoch: 4985 | train_loss: 117.3006134033 | test_loss: 6.0326366425 | \n",
      "Epoch: 4986 | train_loss: 117.3003692627 | test_loss: 6.0325946808 | \n",
      "Epoch: 4987 | train_loss: 117.3001480103 | test_loss: 6.0325469971 | \n",
      "Epoch: 4988 | train_loss: 117.2999038696 | test_loss: 6.0325074196 | \n",
      "Epoch: 4989 | train_loss: 117.2996749878 | test_loss: 6.0324678421 | \n",
      "Epoch: 4990 | train_loss: 117.2994613647 | test_loss: 6.0324211121 | \n",
      "Epoch: 4991 | train_loss: 117.2992401123 | test_loss: 6.0323772430 | \n",
      "Epoch: 4992 | train_loss: 117.2990112305 | test_loss: 6.0323395729 | \n",
      "Epoch: 4993 | train_loss: 117.2987670898 | test_loss: 6.0322990417 | \n",
      "Epoch: 4994 | train_loss: 117.2985382080 | test_loss: 6.0322542191 | \n",
      "Epoch: 4995 | train_loss: 117.2983093262 | test_loss: 6.0322175026 | \n",
      "Epoch: 4996 | train_loss: 117.2981185913 | test_loss: 6.0321712494 | \n",
      "Epoch: 4997 | train_loss: 117.2978744507 | test_loss: 6.0321254730 | \n",
      "Epoch: 4998 | train_loss: 117.2976074219 | test_loss: 6.0320863724 | \n",
      "Epoch: 4999 | train_loss: 117.2974014282 | test_loss: 6.0320448875 | \n",
      "Epoch: 5000 | train_loss: 117.2971572876 | test_loss: 6.0319995880 | \n",
      "Epoch: 5001 | train_loss: 117.2969360352 | test_loss: 6.0319585800 | \n",
      "Epoch: 5002 | train_loss: 117.2966766357 | test_loss: 6.0319185257 | \n",
      "Epoch: 5003 | train_loss: 117.2964782715 | test_loss: 6.0318756104 | \n",
      "Epoch: 5004 | train_loss: 117.2962341309 | test_loss: 6.0318346024 | \n",
      "Epoch: 5005 | train_loss: 117.2960281372 | test_loss: 6.0317897797 | \n",
      "Epoch: 5006 | train_loss: 117.2957839966 | test_loss: 6.0317449570 | \n",
      "Epoch: 5007 | train_loss: 117.2955780029 | test_loss: 6.0317077637 | \n",
      "Epoch: 5008 | train_loss: 117.2953414917 | test_loss: 6.0316624641 | \n",
      "Epoch: 5009 | train_loss: 117.2950744629 | test_loss: 6.0316200256 | \n",
      "Epoch: 5010 | train_loss: 117.2948760986 | test_loss: 6.0315790176 | \n",
      "Epoch: 5011 | train_loss: 117.2946319580 | test_loss: 6.0315413475 | \n",
      "Epoch: 5012 | train_loss: 117.2944030762 | test_loss: 6.0314960480 | \n",
      "Epoch: 5013 | train_loss: 117.2941589355 | test_loss: 6.0314545631 | \n",
      "Epoch: 5014 | train_loss: 117.2939224243 | test_loss: 6.0314121246 | \n",
      "Epoch: 5015 | train_loss: 117.2937088013 | test_loss: 6.0313758850 | \n",
      "Epoch: 5016 | train_loss: 117.2935028076 | test_loss: 6.0313334465 | \n",
      "Epoch: 5017 | train_loss: 117.2932739258 | test_loss: 6.0312886238 | \n",
      "Epoch: 5018 | train_loss: 117.2930145264 | test_loss: 6.0312452316 | \n",
      "Epoch: 5019 | train_loss: 117.2927932739 | test_loss: 6.0312061310 | \n",
      "Epoch: 5020 | train_loss: 117.2926101685 | test_loss: 6.0311632156 | \n",
      "Epoch: 5021 | train_loss: 117.2923812866 | test_loss: 6.0311250687 | \n",
      "Epoch: 5022 | train_loss: 117.2921295166 | test_loss: 6.0310821533 | \n",
      "Epoch: 5023 | train_loss: 117.2919464111 | test_loss: 6.0310368538 | \n",
      "Epoch: 5024 | train_loss: 117.2916870117 | test_loss: 6.0309953690 | \n",
      "Epoch: 5025 | train_loss: 117.2914428711 | test_loss: 6.0309514999 | \n",
      "Epoch: 5026 | train_loss: 117.2912368774 | test_loss: 6.0309128761 | \n",
      "Epoch: 5027 | train_loss: 117.2909927368 | test_loss: 6.0308642387 | \n",
      "Epoch: 5028 | train_loss: 117.2907638550 | test_loss: 6.0308246613 | \n",
      "Epoch: 5029 | train_loss: 117.2905273438 | test_loss: 6.0307826996 | \n",
      "Epoch: 5030 | train_loss: 117.2903213501 | test_loss: 6.0307435989 | \n",
      "Epoch: 5031 | train_loss: 117.2900848389 | test_loss: 6.0306992531 | \n",
      "Epoch: 5032 | train_loss: 117.2898559570 | test_loss: 6.0306577682 | \n",
      "Epoch: 5033 | train_loss: 117.2896270752 | test_loss: 6.0306177139 | \n",
      "Epoch: 5034 | train_loss: 117.2893829346 | test_loss: 6.0305762291 | \n",
      "Epoch: 5035 | train_loss: 117.2891921997 | test_loss: 6.0305356979 | \n",
      "Epoch: 5036 | train_loss: 117.2889480591 | test_loss: 6.0304932594 | \n",
      "Epoch: 5037 | train_loss: 117.2887039185 | test_loss: 6.0304541588 | \n",
      "Epoch: 5038 | train_loss: 117.2884674072 | test_loss: 6.0304083824 | \n",
      "Epoch: 5039 | train_loss: 117.2882614136 | test_loss: 6.0303678513 | \n",
      "Epoch: 5040 | train_loss: 117.2880401611 | test_loss: 6.0303277969 | \n",
      "Epoch: 5041 | train_loss: 117.2877807617 | test_loss: 6.0302858353 | \n",
      "Epoch: 5042 | train_loss: 117.2875900269 | test_loss: 6.0302410126 | \n",
      "Epoch: 5043 | train_loss: 117.2873306274 | test_loss: 6.0301952362 | \n",
      "Epoch: 5044 | train_loss: 117.2871170044 | test_loss: 6.0301556587 | \n",
      "Epoch: 5045 | train_loss: 117.2868881226 | test_loss: 6.0301160812 | \n",
      "Epoch: 5046 | train_loss: 117.2866592407 | test_loss: 6.0300793648 | \n",
      "Epoch: 5047 | train_loss: 117.2864227295 | test_loss: 6.0300374031 | \n",
      "Epoch: 5048 | train_loss: 117.2861938477 | test_loss: 6.0299949646 | \n",
      "Epoch: 5049 | train_loss: 117.2859573364 | test_loss: 6.0299544334 | \n",
      "Epoch: 5050 | train_loss: 117.2857360840 | test_loss: 6.0299086571 | \n",
      "Epoch: 5051 | train_loss: 117.2855072021 | test_loss: 6.0298643112 | \n",
      "Epoch: 5052 | train_loss: 117.2852478027 | test_loss: 6.0298204422 | \n",
      "Epoch: 5053 | train_loss: 117.2850494385 | test_loss: 6.0297780037 | \n",
      "Epoch: 5054 | train_loss: 117.2847976685 | test_loss: 6.0297393799 | \n",
      "Epoch: 5055 | train_loss: 117.2846069336 | test_loss: 6.0296950340 | \n",
      "Epoch: 5056 | train_loss: 117.2843780518 | test_loss: 6.0296525955 | \n",
      "Epoch: 5057 | train_loss: 117.2841186523 | test_loss: 6.0296111107 | \n",
      "Epoch: 5058 | train_loss: 117.2839126587 | test_loss: 6.0295739174 | \n",
      "Epoch: 5059 | train_loss: 117.2836685181 | test_loss: 6.0295319557 | \n",
      "Epoch: 5060 | train_loss: 117.2834472656 | test_loss: 6.0294885635 | \n",
      "Epoch: 5061 | train_loss: 117.2832031250 | test_loss: 6.0294480324 | \n",
      "Epoch: 5062 | train_loss: 117.2829666138 | test_loss: 6.0294079781 | \n",
      "Epoch: 5063 | train_loss: 117.2827224731 | test_loss: 6.0293688774 | \n",
      "Epoch: 5064 | train_loss: 117.2824707031 | test_loss: 6.0293250084 | \n",
      "Epoch: 5065 | train_loss: 117.2822494507 | test_loss: 6.0292797089 | \n",
      "Epoch: 5066 | train_loss: 117.2820053101 | test_loss: 6.0292396545 | \n",
      "Epoch: 5067 | train_loss: 117.2818298340 | test_loss: 6.0292010307 | \n",
      "Epoch: 5068 | train_loss: 117.2815551758 | test_loss: 6.0291619301 | \n",
      "Epoch: 5069 | train_loss: 117.2813491821 | test_loss: 6.0291194916 | \n",
      "Epoch: 5070 | train_loss: 117.2811203003 | test_loss: 6.0290751457 | \n",
      "Epoch: 5071 | train_loss: 117.2808761597 | test_loss: 6.0290350914 | \n",
      "Epoch: 5072 | train_loss: 117.2807006836 | test_loss: 6.0289959908 | \n",
      "Epoch: 5073 | train_loss: 117.2804489136 | test_loss: 6.0289540291 | \n",
      "Epoch: 5074 | train_loss: 117.2801818848 | test_loss: 6.0289154053 | \n",
      "Epoch: 5075 | train_loss: 117.2799911499 | test_loss: 6.0288691521 | \n",
      "Epoch: 5076 | train_loss: 117.2797622681 | test_loss: 6.0288310051 | \n",
      "Epoch: 5077 | train_loss: 117.2795486450 | test_loss: 6.0287876129 | \n",
      "Epoch: 5078 | train_loss: 117.2792968750 | test_loss: 6.0287446976 | \n",
      "Epoch: 5079 | train_loss: 117.2790985107 | test_loss: 6.0287084579 | \n",
      "Epoch: 5080 | train_loss: 117.2789001465 | test_loss: 6.0286626816 | \n",
      "Epoch: 5081 | train_loss: 117.2786712646 | test_loss: 6.0286188126 | \n",
      "Epoch: 5082 | train_loss: 117.2784118652 | test_loss: 6.0285773277 | \n",
      "Epoch: 5083 | train_loss: 117.2781448364 | test_loss: 6.0285325050 | \n",
      "Epoch: 5084 | train_loss: 117.2779464722 | test_loss: 6.0284905434 | \n",
      "Epoch: 5085 | train_loss: 117.2777252197 | test_loss: 6.0284514427 | \n",
      "Epoch: 5086 | train_loss: 117.2774887085 | test_loss: 6.0284142494 | \n",
      "Epoch: 5087 | train_loss: 117.2772979736 | test_loss: 6.0283756256 | \n",
      "Epoch: 5088 | train_loss: 117.2770690918 | test_loss: 6.0283384323 | \n",
      "Epoch: 5089 | train_loss: 117.2768325806 | test_loss: 6.0282979012 | \n",
      "Epoch: 5090 | train_loss: 117.2765884399 | test_loss: 6.0282573700 | \n",
      "Epoch: 5091 | train_loss: 117.2763748169 | test_loss: 6.0282135010 | \n",
      "Epoch: 5092 | train_loss: 117.2761230469 | test_loss: 6.0281791687 | \n",
      "Epoch: 5093 | train_loss: 117.2759170532 | test_loss: 6.0281367302 | \n",
      "Epoch: 5094 | train_loss: 117.2757263184 | test_loss: 6.0280952454 | \n",
      "Epoch: 5095 | train_loss: 117.2754974365 | test_loss: 6.0280523300 | \n",
      "Epoch: 5096 | train_loss: 117.2752761841 | test_loss: 6.0280103683 | \n",
      "Epoch: 5097 | train_loss: 117.2750396729 | test_loss: 6.0279622078 | \n",
      "Epoch: 5098 | train_loss: 117.2748260498 | test_loss: 6.0279178619 | \n",
      "Epoch: 5099 | train_loss: 117.2745971680 | test_loss: 6.0278806686 | \n",
      "Epoch: 5100 | train_loss: 117.2743682861 | test_loss: 6.0278377533 | \n",
      "Epoch: 5101 | train_loss: 117.2741088867 | test_loss: 6.0277996063 | \n",
      "Epoch: 5102 | train_loss: 117.2738876343 | test_loss: 6.0277600288 | \n",
      "Epoch: 5103 | train_loss: 117.2736434937 | test_loss: 6.0277166367 | \n",
      "Epoch: 5104 | train_loss: 117.2733993530 | test_loss: 6.0276770592 | \n",
      "Epoch: 5105 | train_loss: 117.2732238770 | test_loss: 6.0276312828 | \n",
      "Epoch: 5106 | train_loss: 117.2729873657 | test_loss: 6.0275907516 | \n",
      "Epoch: 5107 | train_loss: 117.2727508545 | test_loss: 6.0275502205 | \n",
      "Epoch: 5108 | train_loss: 117.2725219727 | test_loss: 6.0275115967 | \n",
      "Epoch: 5109 | train_loss: 117.2722854614 | test_loss: 6.0274658203 | \n",
      "Epoch: 5110 | train_loss: 117.2720642090 | test_loss: 6.0274262428 | \n",
      "Epoch: 5111 | train_loss: 117.2718353271 | test_loss: 6.0273866653 | \n",
      "Epoch: 5112 | train_loss: 117.2716369629 | test_loss: 6.0273461342 | \n",
      "Epoch: 5113 | train_loss: 117.2714080811 | test_loss: 6.0272979736 | \n",
      "Epoch: 5114 | train_loss: 117.2711715698 | test_loss: 6.0272521973 | \n",
      "Epoch: 5115 | train_loss: 117.2709732056 | test_loss: 6.0272154808 | \n",
      "Epoch: 5116 | train_loss: 117.2707748413 | test_loss: 6.0271735191 | \n",
      "Epoch: 5117 | train_loss: 117.2705154419 | test_loss: 6.0271348953 | \n",
      "Epoch: 5118 | train_loss: 117.2702865601 | test_loss: 6.0270910263 | \n",
      "Epoch: 5119 | train_loss: 117.2700653076 | test_loss: 6.0270533562 | \n",
      "Epoch: 5120 | train_loss: 117.2698440552 | test_loss: 6.0270152092 | \n",
      "Epoch: 5121 | train_loss: 117.2696075439 | test_loss: 6.0269751549 | \n",
      "Epoch: 5122 | train_loss: 117.2693786621 | test_loss: 6.0269360542 | \n",
      "Epoch: 5123 | train_loss: 117.2691192627 | test_loss: 6.0268974304 | \n",
      "Epoch: 5124 | train_loss: 117.2689056396 | test_loss: 6.0268568993 | \n",
      "Epoch: 5125 | train_loss: 117.2686538696 | test_loss: 6.0268139839 | \n",
      "Epoch: 5126 | train_loss: 117.2684173584 | test_loss: 6.0267734528 | \n",
      "Epoch: 5127 | train_loss: 117.2682418823 | test_loss: 6.0267348289 | \n",
      "Epoch: 5128 | train_loss: 117.2679977417 | test_loss: 6.0266938210 | \n",
      "Epoch: 5129 | train_loss: 117.2677612305 | test_loss: 6.0266575813 | \n",
      "Epoch: 5130 | train_loss: 117.2675781250 | test_loss: 6.0266118050 | \n",
      "Epoch: 5131 | train_loss: 117.2673339844 | test_loss: 6.0265703201 | \n",
      "Epoch: 5132 | train_loss: 117.2671127319 | test_loss: 6.0265302658 | \n",
      "Epoch: 5133 | train_loss: 117.2668914795 | test_loss: 6.0264844894 | \n",
      "Epoch: 5134 | train_loss: 117.2666625977 | test_loss: 6.0264396667 | \n",
      "Epoch: 5135 | train_loss: 117.2664337158 | test_loss: 6.0263996124 | \n",
      "Epoch: 5136 | train_loss: 117.2662048340 | test_loss: 6.0263557434 | \n",
      "Epoch: 5137 | train_loss: 117.2660064697 | test_loss: 6.0263166428 | \n",
      "Epoch: 5138 | train_loss: 117.2657699585 | test_loss: 6.0262718201 | \n",
      "Epoch: 5139 | train_loss: 117.2655105591 | test_loss: 6.0262322426 | \n",
      "Epoch: 5140 | train_loss: 117.2652664185 | test_loss: 6.0261931419 | \n",
      "Epoch: 5141 | train_loss: 117.2650299072 | test_loss: 6.0261507034 | \n",
      "Epoch: 5142 | train_loss: 117.2648010254 | test_loss: 6.0261092186 | \n",
      "Epoch: 5143 | train_loss: 117.2646179199 | test_loss: 6.0260701180 | \n",
      "Epoch: 5144 | train_loss: 117.2643585205 | test_loss: 6.0260243416 | \n",
      "Epoch: 5145 | train_loss: 117.2641372681 | test_loss: 6.0259838104 | \n",
      "Epoch: 5146 | train_loss: 117.2639465332 | test_loss: 6.0259466171 | \n",
      "Epoch: 5147 | train_loss: 117.2637176514 | test_loss: 6.0259094238 | \n",
      "Epoch: 5148 | train_loss: 117.2634582520 | test_loss: 6.0258679390 | \n",
      "Epoch: 5149 | train_loss: 117.2632751465 | test_loss: 6.0258283615 | \n",
      "Epoch: 5150 | train_loss: 117.2630462646 | test_loss: 6.0257816315 | \n",
      "Epoch: 5151 | train_loss: 117.2628250122 | test_loss: 6.0257487297 | \n",
      "Epoch: 5152 | train_loss: 117.2625808716 | test_loss: 6.0257043839 | \n",
      "Epoch: 5153 | train_loss: 117.2623443604 | test_loss: 6.0256648064 | \n",
      "Epoch: 5154 | train_loss: 117.2621154785 | test_loss: 6.0256299973 | \n",
      "Epoch: 5155 | train_loss: 117.2619171143 | test_loss: 6.0255889893 | \n",
      "Epoch: 5156 | train_loss: 117.2616729736 | test_loss: 6.0255436897 | \n",
      "Epoch: 5157 | train_loss: 117.2614974976 | test_loss: 6.0254969597 | \n",
      "Epoch: 5158 | train_loss: 117.2612075806 | test_loss: 6.0254545212 | \n",
      "Epoch: 5159 | train_loss: 117.2609634399 | test_loss: 6.0254144669 | \n",
      "Epoch: 5160 | train_loss: 117.2607345581 | test_loss: 6.0253782272 | \n",
      "Epoch: 5161 | train_loss: 117.2604904175 | test_loss: 6.0253386497 | \n",
      "Epoch: 5162 | train_loss: 117.2602996826 | test_loss: 6.0252933502 | \n",
      "Epoch: 5163 | train_loss: 117.2600479126 | test_loss: 6.0252571106 | \n",
      "Epoch: 5164 | train_loss: 117.2598419189 | test_loss: 6.0252180099 | \n",
      "Epoch: 5165 | train_loss: 117.2596130371 | test_loss: 6.0251774788 | \n",
      "Epoch: 5166 | train_loss: 117.2593917847 | test_loss: 6.0251383781 | \n",
      "Epoch: 5167 | train_loss: 117.2591476440 | test_loss: 6.0250926018 | \n",
      "Epoch: 5168 | train_loss: 117.2589340210 | test_loss: 6.0250501633 | \n",
      "Epoch: 5169 | train_loss: 117.2587280273 | test_loss: 6.0250091553 | \n",
      "Epoch: 5170 | train_loss: 117.2584838867 | test_loss: 6.0249695778 | \n",
      "Epoch: 5171 | train_loss: 117.2582626343 | test_loss: 6.0249323845 | \n",
      "Epoch: 5172 | train_loss: 117.2580490112 | test_loss: 6.0248875618 | \n",
      "Epoch: 5173 | train_loss: 117.2578125000 | test_loss: 6.0248484612 | \n",
      "Epoch: 5174 | train_loss: 117.2576065063 | test_loss: 6.0248107910 | \n",
      "Epoch: 5175 | train_loss: 117.2573699951 | test_loss: 6.0247678757 | \n",
      "Epoch: 5176 | train_loss: 117.2571182251 | test_loss: 6.0247368813 | \n",
      "Epoch: 5177 | train_loss: 117.2569122314 | test_loss: 6.0246973038 | \n",
      "Epoch: 5178 | train_loss: 117.2566833496 | test_loss: 6.0246553421 | \n",
      "Epoch: 5179 | train_loss: 117.2564468384 | test_loss: 6.0246129036 | \n",
      "Epoch: 5180 | train_loss: 117.2562179565 | test_loss: 6.0245738029 | \n",
      "Epoch: 5181 | train_loss: 117.2560043335 | test_loss: 6.0245280266 | \n",
      "Epoch: 5182 | train_loss: 117.2557983398 | test_loss: 6.0244908333 | \n",
      "Epoch: 5183 | train_loss: 117.2555847168 | test_loss: 6.0244493484 | \n",
      "Epoch: 5184 | train_loss: 117.2553482056 | test_loss: 6.0244140625 | \n",
      "Epoch: 5185 | train_loss: 117.2551193237 | test_loss: 6.0243697166 | \n",
      "Epoch: 5186 | train_loss: 117.2548828125 | test_loss: 6.0243296623 | \n",
      "Epoch: 5187 | train_loss: 117.2546997070 | test_loss: 6.0242919922 | \n",
      "Epoch: 5188 | train_loss: 117.2544631958 | test_loss: 6.0242505074 | \n",
      "Epoch: 5189 | train_loss: 117.2542266846 | test_loss: 6.0242094994 | \n",
      "Epoch: 5190 | train_loss: 117.2539520264 | test_loss: 6.0241651535 | \n",
      "Epoch: 5191 | train_loss: 117.2537765503 | test_loss: 6.0241270065 | \n",
      "Epoch: 5192 | train_loss: 117.2535324097 | test_loss: 6.0240869522 | \n",
      "Epoch: 5193 | train_loss: 117.2533035278 | test_loss: 6.0240464211 | \n",
      "Epoch: 5194 | train_loss: 117.2530670166 | test_loss: 6.0240044594 | \n",
      "Epoch: 5195 | train_loss: 117.2528533936 | test_loss: 6.0239624977 | \n",
      "Epoch: 5196 | train_loss: 117.2526168823 | test_loss: 6.0239219666 | \n",
      "Epoch: 5197 | train_loss: 117.2523956299 | test_loss: 6.0238771439 | \n",
      "Epoch: 5198 | train_loss: 117.2521591187 | test_loss: 6.0238413811 | \n",
      "Epoch: 5199 | train_loss: 117.2519149780 | test_loss: 6.0238008499 | \n",
      "Epoch: 5200 | train_loss: 117.2516860962 | test_loss: 6.0237674713 | \n",
      "Epoch: 5201 | train_loss: 117.2514724731 | test_loss: 6.0237216949 | \n",
      "Epoch: 5202 | train_loss: 117.2512435913 | test_loss: 6.0236802101 | \n",
      "Epoch: 5203 | train_loss: 117.2510375977 | test_loss: 6.0236430168 | \n",
      "Epoch: 5204 | train_loss: 117.2507781982 | test_loss: 6.0236053467 | \n",
      "Epoch: 5205 | train_loss: 117.2505950928 | test_loss: 6.0235652924 | \n",
      "Epoch: 5206 | train_loss: 117.2503585815 | test_loss: 6.0235228539 | \n",
      "Epoch: 5207 | train_loss: 117.2501296997 | test_loss: 6.0234794617 | \n",
      "Epoch: 5208 | train_loss: 117.2499313354 | test_loss: 6.0234336853 | \n",
      "Epoch: 5209 | train_loss: 117.2496795654 | test_loss: 6.0233917236 | \n",
      "Epoch: 5210 | train_loss: 117.2494506836 | test_loss: 6.0233573914 | \n",
      "Epoch: 5211 | train_loss: 117.2492294312 | test_loss: 6.0233139992 | \n",
      "Epoch: 5212 | train_loss: 117.2490234375 | test_loss: 6.0232768059 | \n",
      "Epoch: 5213 | train_loss: 117.2487869263 | test_loss: 6.0232343674 | \n",
      "Epoch: 5214 | train_loss: 117.2485046387 | test_loss: 6.0231962204 | \n",
      "Epoch: 5215 | train_loss: 117.2482833862 | test_loss: 6.0231604576 | \n",
      "Epoch: 5216 | train_loss: 117.2480697632 | test_loss: 6.0231208801 | \n",
      "Epoch: 5217 | train_loss: 117.2478332520 | test_loss: 6.0230774879 | \n",
      "Epoch: 5218 | train_loss: 117.2476272583 | test_loss: 6.0230326653 | \n",
      "Epoch: 5219 | train_loss: 117.2474060059 | test_loss: 6.0229930878 | \n",
      "Epoch: 5220 | train_loss: 117.2471847534 | test_loss: 6.0229535103 | \n",
      "Epoch: 5221 | train_loss: 117.2469711304 | test_loss: 6.0229167938 | \n",
      "Epoch: 5222 | train_loss: 117.2467498779 | test_loss: 6.0228767395 | \n",
      "Epoch: 5223 | train_loss: 117.2465057373 | test_loss: 6.0228343010 | \n",
      "Epoch: 5224 | train_loss: 117.2462768555 | test_loss: 6.0227952003 | \n",
      "Epoch: 5225 | train_loss: 117.2460556030 | test_loss: 6.0227508545 | \n",
      "Epoch: 5226 | train_loss: 117.2458267212 | test_loss: 6.0227141380 | \n",
      "Epoch: 5227 | train_loss: 117.2456054688 | test_loss: 6.0226745605 | \n",
      "Epoch: 5228 | train_loss: 117.2453918457 | test_loss: 6.0226345062 | \n",
      "Epoch: 5229 | train_loss: 117.2451248169 | test_loss: 6.0225911140 | \n",
      "Epoch: 5230 | train_loss: 117.2449340820 | test_loss: 6.0225501060 | \n",
      "Epoch: 5231 | train_loss: 117.2447280884 | test_loss: 6.0225114822 | \n",
      "Epoch: 5232 | train_loss: 117.2444992065 | test_loss: 6.0224733353 | \n",
      "Epoch: 5233 | train_loss: 117.2442703247 | test_loss: 6.0224318504 | \n",
      "Epoch: 5234 | train_loss: 117.2440490723 | test_loss: 6.0223894119 | \n",
      "Epoch: 5235 | train_loss: 117.2438354492 | test_loss: 6.0223455429 | \n",
      "Epoch: 5236 | train_loss: 117.2436294556 | test_loss: 6.0223064423 | \n",
      "Epoch: 5237 | train_loss: 117.2433624268 | test_loss: 6.0222682953 | \n",
      "Epoch: 5238 | train_loss: 117.2431564331 | test_loss: 6.0222239494 | \n",
      "Epoch: 5239 | train_loss: 117.2428970337 | test_loss: 6.0221886635 | \n",
      "Epoch: 5240 | train_loss: 117.2427062988 | test_loss: 6.0221495628 | \n",
      "Epoch: 5241 | train_loss: 117.2425231934 | test_loss: 6.0221061707 | \n",
      "Epoch: 5242 | train_loss: 117.2422637939 | test_loss: 6.0220670700 | \n",
      "Epoch: 5243 | train_loss: 117.2420578003 | test_loss: 6.0220298767 | \n",
      "Epoch: 5244 | train_loss: 117.2418060303 | test_loss: 6.0219879150 | \n",
      "Epoch: 5245 | train_loss: 117.2416000366 | test_loss: 6.0219459534 | \n",
      "Epoch: 5246 | train_loss: 117.2413558960 | test_loss: 6.0219054222 | \n",
      "Epoch: 5247 | train_loss: 117.2411499023 | test_loss: 6.0218658447 | \n",
      "Epoch: 5248 | train_loss: 117.2409362793 | test_loss: 6.0218315125 | \n",
      "Epoch: 5249 | train_loss: 117.2406921387 | test_loss: 6.0217938423 | \n",
      "Epoch: 5250 | train_loss: 117.2404708862 | test_loss: 6.0217523575 | \n",
      "Epoch: 5251 | train_loss: 117.2402648926 | test_loss: 6.0217132568 | \n",
      "Epoch: 5252 | train_loss: 117.2400512695 | test_loss: 6.0216765404 | \n",
      "Epoch: 5253 | train_loss: 117.2398223877 | test_loss: 6.0216364861 | \n",
      "Epoch: 5254 | train_loss: 117.2395935059 | test_loss: 6.0215930939 | \n",
      "Epoch: 5255 | train_loss: 117.2393875122 | test_loss: 6.0215559006 | \n",
      "Epoch: 5256 | train_loss: 117.2391815186 | test_loss: 6.0215177536 | \n",
      "Epoch: 5257 | train_loss: 117.2389373779 | test_loss: 6.0214772224 | \n",
      "Epoch: 5258 | train_loss: 117.2387237549 | test_loss: 6.0214319229 | \n",
      "Epoch: 5259 | train_loss: 117.2384948730 | test_loss: 6.0213932991 | \n",
      "Epoch: 5260 | train_loss: 117.2382431030 | test_loss: 6.0213527679 | \n",
      "Epoch: 5261 | train_loss: 117.2380142212 | test_loss: 6.0213165283 | \n",
      "Epoch: 5262 | train_loss: 117.2378387451 | test_loss: 6.0212802887 | \n",
      "Epoch: 5263 | train_loss: 117.2376174927 | test_loss: 6.0212383270 | \n",
      "Epoch: 5264 | train_loss: 117.2373733521 | test_loss: 6.0212016106 | \n",
      "Epoch: 5265 | train_loss: 117.2371749878 | test_loss: 6.0211625099 | \n",
      "Epoch: 5266 | train_loss: 117.2369232178 | test_loss: 6.0211195946 | \n",
      "Epoch: 5267 | train_loss: 117.2367172241 | test_loss: 6.0210847855 | \n",
      "Epoch: 5268 | train_loss: 117.2365112305 | test_loss: 6.0210399628 | \n",
      "Epoch: 5269 | train_loss: 117.2362518311 | test_loss: 6.0209989548 | \n",
      "Epoch: 5270 | train_loss: 117.2360458374 | test_loss: 6.0209565163 | \n",
      "Epoch: 5271 | train_loss: 117.2358093262 | test_loss: 6.0209183693 | \n",
      "Epoch: 5272 | train_loss: 117.2356185913 | test_loss: 6.0208778381 | \n",
      "Epoch: 5273 | train_loss: 117.2353591919 | test_loss: 6.0208363533 | \n",
      "Epoch: 5274 | train_loss: 117.2351379395 | test_loss: 6.0207977295 | \n",
      "Epoch: 5275 | train_loss: 117.2349395752 | test_loss: 6.0207600594 | \n",
      "Epoch: 5276 | train_loss: 117.2347106934 | test_loss: 6.0207176208 | \n",
      "Epoch: 5277 | train_loss: 117.2345046997 | test_loss: 6.0206775665 | \n",
      "Epoch: 5278 | train_loss: 117.2342681885 | test_loss: 6.0206379890 | \n",
      "Epoch: 5279 | train_loss: 117.2340316772 | test_loss: 6.0205993652 | \n",
      "Epoch: 5280 | train_loss: 117.2338409424 | test_loss: 6.0205593109 | \n",
      "Epoch: 5281 | train_loss: 117.2336730957 | test_loss: 6.0205211639 | \n",
      "Epoch: 5282 | train_loss: 117.2334442139 | test_loss: 6.0204777718 | \n",
      "Epoch: 5283 | train_loss: 117.2332305908 | test_loss: 6.0204358101 | \n",
      "Epoch: 5284 | train_loss: 117.2330169678 | test_loss: 6.0204024315 | \n",
      "Epoch: 5285 | train_loss: 117.2327804565 | test_loss: 6.0203571320 | \n",
      "Epoch: 5286 | train_loss: 117.2325363159 | test_loss: 6.0203204155 | \n",
      "Epoch: 5287 | train_loss: 117.2323226929 | test_loss: 6.0202808380 | \n",
      "Epoch: 5288 | train_loss: 117.2320480347 | test_loss: 6.0202379227 | \n",
      "Epoch: 5289 | train_loss: 117.2318344116 | test_loss: 6.0201983452 | \n",
      "Epoch: 5290 | train_loss: 117.2315902710 | test_loss: 6.0201525688 | \n",
      "Epoch: 5291 | train_loss: 117.2313537598 | test_loss: 6.0201225281 | \n",
      "Epoch: 5292 | train_loss: 117.2311706543 | test_loss: 6.0200824738 | \n",
      "Epoch: 5293 | train_loss: 117.2309265137 | test_loss: 6.0200414658 | \n",
      "Epoch: 5294 | train_loss: 117.2307052612 | test_loss: 6.0200047493 | \n",
      "Epoch: 5295 | train_loss: 117.2304382324 | test_loss: 6.0199689865 | \n",
      "Epoch: 5296 | train_loss: 117.2302627563 | test_loss: 6.0199303627 | \n",
      "Epoch: 5297 | train_loss: 117.2300186157 | test_loss: 6.0198917389 | \n",
      "Epoch: 5298 | train_loss: 117.2298278809 | test_loss: 6.0198469162 | \n",
      "Epoch: 5299 | train_loss: 117.2296066284 | test_loss: 6.0198121071 | \n",
      "Epoch: 5300 | train_loss: 117.2293853760 | test_loss: 6.0197758675 | \n",
      "Epoch: 5301 | train_loss: 117.2291793823 | test_loss: 6.0197381973 | \n",
      "Epoch: 5302 | train_loss: 117.2289428711 | test_loss: 6.0196943283 | \n",
      "Epoch: 5303 | train_loss: 117.2287368774 | test_loss: 6.0196599960 | \n",
      "Epoch: 5304 | train_loss: 117.2285156250 | test_loss: 6.0196166039 | \n",
      "Epoch: 5305 | train_loss: 117.2282943726 | test_loss: 6.0195765495 | \n",
      "Epoch: 5306 | train_loss: 117.2280578613 | test_loss: 6.0195379257 | \n",
      "Epoch: 5307 | train_loss: 117.2278442383 | test_loss: 6.0194926262 | \n",
      "Epoch: 5308 | train_loss: 117.2276382446 | test_loss: 6.0194573402 | \n",
      "Epoch: 5309 | train_loss: 117.2274398804 | test_loss: 6.0194158554 | \n",
      "Epoch: 5310 | train_loss: 117.2271804810 | test_loss: 6.0193762779 | \n",
      "Epoch: 5311 | train_loss: 117.2269592285 | test_loss: 6.0193347931 | \n",
      "Epoch: 5312 | train_loss: 117.2267074585 | test_loss: 6.0192952156 | \n",
      "Epoch: 5313 | train_loss: 117.2264709473 | test_loss: 6.0192561150 | \n",
      "Epoch: 5314 | train_loss: 117.2262802124 | test_loss: 6.0192189217 | \n",
      "Epoch: 5315 | train_loss: 117.2260437012 | test_loss: 6.0191745758 | \n",
      "Epoch: 5316 | train_loss: 117.2257995605 | test_loss: 6.0191411972 | \n",
      "Epoch: 5317 | train_loss: 117.2256164551 | test_loss: 6.0191020966 | \n",
      "Epoch: 5318 | train_loss: 117.2253723145 | test_loss: 6.0190629959 | \n",
      "Epoch: 5319 | train_loss: 117.2251358032 | test_loss: 6.0190196037 | \n",
      "Epoch: 5320 | train_loss: 117.2248840332 | test_loss: 6.0189862251 | \n",
      "Epoch: 5321 | train_loss: 117.2246780396 | test_loss: 6.0189490318 | \n",
      "Epoch: 5322 | train_loss: 117.2244720459 | test_loss: 6.0189085007 | \n",
      "Epoch: 5323 | train_loss: 117.2242584229 | test_loss: 6.0188732147 | \n",
      "Epoch: 5324 | train_loss: 117.2240066528 | test_loss: 6.0188331604 | \n",
      "Epoch: 5325 | train_loss: 117.2237472534 | test_loss: 6.0187907219 | \n",
      "Epoch: 5326 | train_loss: 117.2235412598 | test_loss: 6.0187506676 | \n",
      "Epoch: 5327 | train_loss: 117.2233428955 | test_loss: 6.0187077522 | \n",
      "Epoch: 5328 | train_loss: 117.2230987549 | test_loss: 6.0186700821 | \n",
      "Epoch: 5329 | train_loss: 117.2228927612 | test_loss: 6.0186324120 | \n",
      "Epoch: 5330 | train_loss: 117.2227020264 | test_loss: 6.0185937881 | \n",
      "Epoch: 5331 | train_loss: 117.2224578857 | test_loss: 6.0185523033 | \n",
      "Epoch: 5332 | train_loss: 117.2222442627 | test_loss: 6.0185127258 | \n",
      "Epoch: 5333 | train_loss: 117.2220306396 | test_loss: 6.0184745789 | \n",
      "Epoch: 5334 | train_loss: 117.2217864990 | test_loss: 6.0184383392 | \n",
      "Epoch: 5335 | train_loss: 117.2215652466 | test_loss: 6.0183911324 | \n",
      "Epoch: 5336 | train_loss: 117.2213668823 | test_loss: 6.0183539391 | \n",
      "Epoch: 5337 | train_loss: 117.2211608887 | test_loss: 6.0183191299 | \n",
      "Epoch: 5338 | train_loss: 117.2209167480 | test_loss: 6.0182771683 | \n",
      "Epoch: 5339 | train_loss: 117.2207031250 | test_loss: 6.0182409286 | \n",
      "Epoch: 5340 | train_loss: 117.2204818726 | test_loss: 6.0182008743 | \n",
      "Epoch: 5341 | train_loss: 117.2202453613 | test_loss: 6.0181641579 | \n",
      "Epoch: 5342 | train_loss: 117.2200012207 | test_loss: 6.0181279182 | \n",
      "Epoch: 5343 | train_loss: 117.2198028564 | test_loss: 6.0180864334 | \n",
      "Epoch: 5344 | train_loss: 117.2196121216 | test_loss: 6.0180463791 | \n",
      "Epoch: 5345 | train_loss: 117.2193374634 | test_loss: 6.0180091858 | \n",
      "Epoch: 5346 | train_loss: 117.2191314697 | test_loss: 6.0179700851 | \n",
      "Epoch: 5347 | train_loss: 117.2188949585 | test_loss: 6.0179333687 | \n",
      "Epoch: 5348 | train_loss: 117.2186355591 | test_loss: 6.0178928375 | \n",
      "Epoch: 5349 | train_loss: 117.2184066772 | test_loss: 6.0178523064 | \n",
      "Epoch: 5350 | train_loss: 117.2181854248 | test_loss: 6.0178151131 | \n",
      "Epoch: 5351 | train_loss: 117.2179946899 | test_loss: 6.0177779198 | \n",
      "Epoch: 5352 | train_loss: 117.2177658081 | test_loss: 6.0177378654 | \n",
      "Epoch: 5353 | train_loss: 117.2175216675 | test_loss: 6.0176982880 | \n",
      "Epoch: 5354 | train_loss: 117.2173080444 | test_loss: 6.0176601410 | \n",
      "Epoch: 5355 | train_loss: 117.2170715332 | test_loss: 6.0176234245 | \n",
      "Epoch: 5356 | train_loss: 117.2168426514 | test_loss: 6.0175819397 | \n",
      "Epoch: 5357 | train_loss: 117.2166595459 | test_loss: 6.0175409317 | \n",
      "Epoch: 5358 | train_loss: 117.2164077759 | test_loss: 6.0174970627 | \n",
      "Epoch: 5359 | train_loss: 117.2161712646 | test_loss: 6.0174641609 | \n",
      "Epoch: 5360 | train_loss: 117.2159347534 | test_loss: 6.0174236298 | \n",
      "Epoch: 5361 | train_loss: 117.2157440186 | test_loss: 6.0173840523 | \n",
      "Epoch: 5362 | train_loss: 117.2154769897 | test_loss: 6.0173454285 | \n",
      "Epoch: 5363 | train_loss: 117.2152709961 | test_loss: 6.0173101425 | \n",
      "Epoch: 5364 | train_loss: 117.2150497437 | test_loss: 6.0172734261 | \n",
      "Epoch: 5365 | train_loss: 117.2148437500 | test_loss: 6.0172276497 | \n",
      "Epoch: 5366 | train_loss: 117.2145767212 | test_loss: 6.0171904564 | \n",
      "Epoch: 5367 | train_loss: 117.2143630981 | test_loss: 6.0171513557 | \n",
      "Epoch: 5368 | train_loss: 117.2141189575 | test_loss: 6.0171189308 | \n",
      "Epoch: 5369 | train_loss: 117.2139282227 | test_loss: 6.0170722008 | \n",
      "Epoch: 5370 | train_loss: 117.2137145996 | test_loss: 6.0170373917 | \n",
      "Epoch: 5371 | train_loss: 117.2134780884 | test_loss: 6.0169982910 | \n",
      "Epoch: 5372 | train_loss: 117.2132492065 | test_loss: 6.0169630051 | \n",
      "Epoch: 5373 | train_loss: 117.2130432129 | test_loss: 6.0169219971 | \n",
      "Epoch: 5374 | train_loss: 117.2127990723 | test_loss: 6.0168824196 | \n",
      "Epoch: 5375 | train_loss: 117.2126235962 | test_loss: 6.0168390274 | \n",
      "Epoch: 5376 | train_loss: 117.2123565674 | test_loss: 6.0167980194 | \n",
      "Epoch: 5377 | train_loss: 117.2121582031 | test_loss: 6.0167598724 | \n",
      "Epoch: 5378 | train_loss: 117.2119369507 | test_loss: 6.0167241096 | \n",
      "Epoch: 5379 | train_loss: 117.2117080688 | test_loss: 6.0166821480 | \n",
      "Epoch: 5380 | train_loss: 117.2115020752 | test_loss: 6.0166416168 | \n",
      "Epoch: 5381 | train_loss: 117.2112960815 | test_loss: 6.0166001320 | \n",
      "Epoch: 5382 | train_loss: 117.2110443115 | test_loss: 6.0165615082 | \n",
      "Epoch: 5383 | train_loss: 117.2108230591 | test_loss: 6.0165214539 | \n",
      "Epoch: 5384 | train_loss: 117.2105941772 | test_loss: 6.0164842606 | \n",
      "Epoch: 5385 | train_loss: 117.2103881836 | test_loss: 6.0164451599 | \n",
      "Epoch: 5386 | train_loss: 117.2101364136 | test_loss: 6.0164103508 | \n",
      "Epoch: 5387 | train_loss: 117.2098922729 | test_loss: 6.0163683891 | \n",
      "Epoch: 5388 | train_loss: 117.2096710205 | test_loss: 6.0163331032 | \n",
      "Epoch: 5389 | train_loss: 117.2094345093 | test_loss: 6.0162935257 | \n",
      "Epoch: 5390 | train_loss: 117.2091903687 | test_loss: 6.0162529945 | \n",
      "Epoch: 5391 | train_loss: 117.2089767456 | test_loss: 6.0162138939 | \n",
      "Epoch: 5392 | train_loss: 117.2087478638 | test_loss: 6.0161757469 | \n",
      "Epoch: 5393 | train_loss: 117.2085342407 | test_loss: 6.0161390305 | \n",
      "Epoch: 5394 | train_loss: 117.2083129883 | test_loss: 6.0161008835 | \n",
      "Epoch: 5395 | train_loss: 117.2081375122 | test_loss: 6.0160627365 | \n",
      "Epoch: 5396 | train_loss: 117.2079010010 | test_loss: 6.0160179138 | \n",
      "Epoch: 5397 | train_loss: 117.2076721191 | test_loss: 6.0159840584 | \n",
      "Epoch: 5398 | train_loss: 117.2074203491 | test_loss: 6.0159397125 | \n",
      "Epoch: 5399 | train_loss: 117.2071838379 | test_loss: 6.0159044266 | \n",
      "Epoch: 5400 | train_loss: 117.2070007324 | test_loss: 6.0158624649 | \n",
      "Epoch: 5401 | train_loss: 117.2067565918 | test_loss: 6.0158247948 | \n",
      "Epoch: 5402 | train_loss: 117.2065200806 | test_loss: 6.0157890320 | \n",
      "Epoch: 5403 | train_loss: 117.2062988281 | test_loss: 6.0157527924 | \n",
      "Epoch: 5404 | train_loss: 117.2060546875 | test_loss: 6.0157160759 | \n",
      "Epoch: 5405 | train_loss: 117.2058410645 | test_loss: 6.0156712532 | \n",
      "Epoch: 5406 | train_loss: 117.2056427002 | test_loss: 6.0156331062 | \n",
      "Epoch: 5407 | train_loss: 117.2053909302 | test_loss: 6.0155959129 | \n",
      "Epoch: 5408 | train_loss: 117.2051925659 | test_loss: 6.0155553818 | \n",
      "Epoch: 5409 | train_loss: 117.2049865723 | test_loss: 6.0155186653 | \n",
      "Epoch: 5410 | train_loss: 117.2047653198 | test_loss: 6.0154786110 | \n",
      "Epoch: 5411 | train_loss: 117.2045288086 | test_loss: 6.0154433250 | \n",
      "Epoch: 5412 | train_loss: 117.2043151855 | test_loss: 6.0154070854 | \n",
      "Epoch: 5413 | train_loss: 117.2040939331 | test_loss: 6.0153627396 | \n",
      "Epoch: 5414 | train_loss: 117.2038879395 | test_loss: 6.0153279305 | \n",
      "Epoch: 5415 | train_loss: 117.2036514282 | test_loss: 6.0152916908 | \n",
      "Epoch: 5416 | train_loss: 117.2034454346 | test_loss: 6.0152606964 | \n",
      "Epoch: 5417 | train_loss: 117.2031936646 | test_loss: 6.0152201653 | \n",
      "Epoch: 5418 | train_loss: 117.2029190063 | test_loss: 6.0151886940 | \n",
      "Epoch: 5419 | train_loss: 117.2027282715 | test_loss: 6.0151433945 | \n",
      "Epoch: 5420 | train_loss: 117.2024841309 | test_loss: 6.0151038170 | \n",
      "Epoch: 5421 | train_loss: 117.2022399902 | test_loss: 6.0150642395 | \n",
      "Epoch: 5422 | train_loss: 117.2020339966 | test_loss: 6.0150313377 | \n",
      "Epoch: 5423 | train_loss: 117.2017669678 | test_loss: 6.0149898529 | \n",
      "Epoch: 5424 | train_loss: 117.2015609741 | test_loss: 6.0149536133 | \n",
      "Epoch: 5425 | train_loss: 117.2013473511 | test_loss: 6.0149149895 | \n",
      "Epoch: 5426 | train_loss: 117.2011413574 | test_loss: 6.0148701668 | \n",
      "Epoch: 5427 | train_loss: 117.2008972168 | test_loss: 6.0148353577 | \n",
      "Epoch: 5428 | train_loss: 117.2006912231 | test_loss: 6.0147986412 | \n",
      "Epoch: 5429 | train_loss: 117.2004776001 | test_loss: 6.0147590637 | \n",
      "Epoch: 5430 | train_loss: 117.2002258301 | test_loss: 6.0147218704 | \n",
      "Epoch: 5431 | train_loss: 117.1999893188 | test_loss: 6.0146818161 | \n",
      "Epoch: 5432 | train_loss: 117.1997680664 | test_loss: 6.0146417618 | \n",
      "Epoch: 5433 | train_loss: 117.1995620728 | test_loss: 6.0146045685 | \n",
      "Epoch: 5434 | train_loss: 117.1993255615 | test_loss: 6.0145692825 | \n",
      "Epoch: 5435 | train_loss: 117.1990890503 | test_loss: 6.0145320892 | \n",
      "Epoch: 5436 | train_loss: 117.1988830566 | test_loss: 6.0144920349 | \n",
      "Epoch: 5437 | train_loss: 117.1986846924 | test_loss: 6.0144543648 | \n",
      "Epoch: 5438 | train_loss: 117.1984786987 | test_loss: 6.0144109726 | \n",
      "Epoch: 5439 | train_loss: 117.1982727051 | test_loss: 6.0143718719 | \n",
      "Epoch: 5440 | train_loss: 117.1980361938 | test_loss: 6.0143303871 | \n",
      "Epoch: 5441 | train_loss: 117.1978607178 | test_loss: 6.0142970085 | \n",
      "Epoch: 5442 | train_loss: 117.1976165771 | test_loss: 6.0142588615 | \n",
      "Epoch: 5443 | train_loss: 117.1974334717 | test_loss: 6.0142183304 | \n",
      "Epoch: 5444 | train_loss: 117.1971893311 | test_loss: 6.0141797066 | \n",
      "Epoch: 5445 | train_loss: 117.1969757080 | test_loss: 6.0141429901 | \n",
      "Epoch: 5446 | train_loss: 117.1967697144 | test_loss: 6.0140991211 | \n",
      "Epoch: 5447 | train_loss: 117.1965255737 | test_loss: 6.0140681267 | \n",
      "Epoch: 5448 | train_loss: 117.1963119507 | test_loss: 6.0140333176 | \n",
      "Epoch: 5449 | train_loss: 117.1960830688 | test_loss: 6.0139966011 | \n",
      "Epoch: 5450 | train_loss: 117.1958694458 | test_loss: 6.0139589310 | \n",
      "Epoch: 5451 | train_loss: 117.1956176758 | test_loss: 6.0139245987 | \n",
      "Epoch: 5452 | train_loss: 117.1954193115 | test_loss: 6.0138878822 | \n",
      "Epoch: 5453 | train_loss: 117.1952056885 | test_loss: 6.0138454437 | \n",
      "Epoch: 5454 | train_loss: 117.1949996948 | test_loss: 6.0138072968 | \n",
      "Epoch: 5455 | train_loss: 117.1947555542 | test_loss: 6.0137724876 | \n",
      "Epoch: 5456 | train_loss: 117.1945419312 | test_loss: 6.0137376785 | \n",
      "Epoch: 5457 | train_loss: 117.1943283081 | test_loss: 6.0136961937 | \n",
      "Epoch: 5458 | train_loss: 117.1940917969 | test_loss: 6.0136580467 | \n",
      "Epoch: 5459 | train_loss: 117.1938781738 | test_loss: 6.0136213303 | \n",
      "Epoch: 5460 | train_loss: 117.1936492920 | test_loss: 6.0135865211 | \n",
      "Epoch: 5461 | train_loss: 117.1934356689 | test_loss: 6.0135517120 | \n",
      "Epoch: 5462 | train_loss: 117.1932449341 | test_loss: 6.0135107040 | \n",
      "Epoch: 5463 | train_loss: 117.1930007935 | test_loss: 6.0134716034 | \n",
      "Epoch: 5464 | train_loss: 117.1927566528 | test_loss: 6.0134344101 | \n",
      "Epoch: 5465 | train_loss: 117.1925277710 | test_loss: 6.0134010315 | \n",
      "Epoch: 5466 | train_loss: 117.1923217773 | test_loss: 6.0133609772 | \n",
      "Epoch: 5467 | train_loss: 117.1921081543 | test_loss: 6.0133233070 | \n",
      "Epoch: 5468 | train_loss: 117.1918945312 | test_loss: 6.0132818222 | \n",
      "Epoch: 5469 | train_loss: 117.1916885376 | test_loss: 6.0132408142 | \n",
      "Epoch: 5470 | train_loss: 117.1914291382 | test_loss: 6.0132017136 | \n",
      "Epoch: 5471 | train_loss: 117.1912307739 | test_loss: 6.0131669044 | \n",
      "Epoch: 5472 | train_loss: 117.1910247803 | test_loss: 6.0131225586 | \n",
      "Epoch: 5473 | train_loss: 117.1907882690 | test_loss: 6.0130891800 | \n",
      "Epoch: 5474 | train_loss: 117.1905746460 | test_loss: 6.0130510330 | \n",
      "Epoch: 5475 | train_loss: 117.1903533936 | test_loss: 6.0130109787 | \n",
      "Epoch: 5476 | train_loss: 117.1901473999 | test_loss: 6.0129742622 | \n",
      "Epoch: 5477 | train_loss: 117.1898956299 | test_loss: 6.0129356384 | \n",
      "Epoch: 5478 | train_loss: 117.1896743774 | test_loss: 6.0128951073 | \n",
      "Epoch: 5479 | train_loss: 117.1894531250 | test_loss: 6.0128536224 | \n",
      "Epoch: 5480 | train_loss: 117.1892395020 | test_loss: 6.0128173828 | \n",
      "Epoch: 5481 | train_loss: 117.1890335083 | test_loss: 6.0127830505 | \n",
      "Epoch: 5482 | train_loss: 117.1888122559 | test_loss: 6.0127458572 | \n",
      "Epoch: 5483 | train_loss: 117.1885910034 | test_loss: 6.0127124786 | \n",
      "Epoch: 5484 | train_loss: 117.1883621216 | test_loss: 6.0126776695 | \n",
      "Epoch: 5485 | train_loss: 117.1881866455 | test_loss: 6.0126347542 | \n",
      "Epoch: 5486 | train_loss: 117.1879577637 | test_loss: 6.0125989914 | \n",
      "Epoch: 5487 | train_loss: 117.1877441406 | test_loss: 6.0125575066 | \n",
      "Epoch: 5488 | train_loss: 117.1875076294 | test_loss: 6.0125236511 | \n",
      "Epoch: 5489 | train_loss: 117.1872634888 | test_loss: 6.0124821663 | \n",
      "Epoch: 5490 | train_loss: 117.1870422363 | test_loss: 6.0124421120 | \n",
      "Epoch: 5491 | train_loss: 117.1868286133 | test_loss: 6.0124068260 | \n",
      "Epoch: 5492 | train_loss: 117.1866378784 | test_loss: 6.0123734474 | \n",
      "Epoch: 5493 | train_loss: 117.1864166260 | test_loss: 6.0123348236 | \n",
      "Epoch: 5494 | train_loss: 117.1861801147 | test_loss: 6.0122928619 | \n",
      "Epoch: 5495 | train_loss: 117.1859893799 | test_loss: 6.0122542381 | \n",
      "Epoch: 5496 | train_loss: 117.1857604980 | test_loss: 6.0122151375 | \n",
      "Epoch: 5497 | train_loss: 117.1855468750 | test_loss: 6.0121769905 | \n",
      "Epoch: 5498 | train_loss: 117.1853256226 | test_loss: 6.0121417046 | \n",
      "Epoch: 5499 | train_loss: 117.1851272583 | test_loss: 6.0121083260 | \n",
      "Epoch: 5500 | train_loss: 117.1849212646 | test_loss: 6.0120682716 | \n",
      "Epoch: 5501 | train_loss: 117.1846771240 | test_loss: 6.0120301247 | \n",
      "Epoch: 5502 | train_loss: 117.1844406128 | test_loss: 6.0119919777 | \n",
      "Epoch: 5503 | train_loss: 117.1842269897 | test_loss: 6.0119543076 | \n",
      "Epoch: 5504 | train_loss: 117.1840133667 | test_loss: 6.0119209290 | \n",
      "Epoch: 5505 | train_loss: 117.1838073730 | test_loss: 6.0118827820 | \n",
      "Epoch: 5506 | train_loss: 117.1836013794 | test_loss: 6.0118503571 | \n",
      "Epoch: 5507 | train_loss: 117.1834106445 | test_loss: 6.0118088722 | \n",
      "Epoch: 5508 | train_loss: 117.1831970215 | test_loss: 6.0117669106 | \n",
      "Epoch: 5509 | train_loss: 117.1829681396 | test_loss: 6.0117363930 | \n",
      "Epoch: 5510 | train_loss: 117.1827239990 | test_loss: 6.0117006302 | \n",
      "Epoch: 5511 | train_loss: 117.1825408936 | test_loss: 6.0116605759 | \n",
      "Epoch: 5512 | train_loss: 117.1823120117 | test_loss: 6.0116214752 | \n",
      "Epoch: 5513 | train_loss: 117.1821212769 | test_loss: 6.0115818977 | \n",
      "Epoch: 5514 | train_loss: 117.1818542480 | test_loss: 6.0115418434 | \n",
      "Epoch: 5515 | train_loss: 117.1816482544 | test_loss: 6.0115094185 | \n",
      "Epoch: 5516 | train_loss: 117.1814651489 | test_loss: 6.0114750862 | \n",
      "Epoch: 5517 | train_loss: 117.1812210083 | test_loss: 6.0114412308 | \n",
      "Epoch: 5518 | train_loss: 117.1810226440 | test_loss: 6.0114030838 | \n",
      "Epoch: 5519 | train_loss: 117.1808013916 | test_loss: 6.0113635063 | \n",
      "Epoch: 5520 | train_loss: 117.1805648804 | test_loss: 6.0113177299 | \n",
      "Epoch: 5521 | train_loss: 117.1803283691 | test_loss: 6.0112833977 | \n",
      "Epoch: 5522 | train_loss: 117.1801300049 | test_loss: 6.0112481117 | \n",
      "Epoch: 5523 | train_loss: 117.1798858643 | test_loss: 6.0112171173 | \n",
      "Epoch: 5524 | train_loss: 117.1796875000 | test_loss: 6.0111799240 | \n",
      "Epoch: 5525 | train_loss: 117.1794509888 | test_loss: 6.0111432076 | \n",
      "Epoch: 5526 | train_loss: 117.1791915894 | test_loss: 6.0111036301 | \n",
      "Epoch: 5527 | train_loss: 117.1789932251 | test_loss: 6.0110669136 | \n",
      "Epoch: 5528 | train_loss: 117.1787796021 | test_loss: 6.0110340118 | \n",
      "Epoch: 5529 | train_loss: 117.1785659790 | test_loss: 6.0109972954 | \n",
      "Epoch: 5530 | train_loss: 117.1783370972 | test_loss: 6.0109605789 | \n",
      "Epoch: 5531 | train_loss: 117.1781311035 | test_loss: 6.0109257698 | \n",
      "Epoch: 5532 | train_loss: 117.1779479980 | test_loss: 6.0108842850 | \n",
      "Epoch: 5533 | train_loss: 117.1776962280 | test_loss: 6.0108475685 | \n",
      "Epoch: 5534 | train_loss: 117.1774673462 | test_loss: 6.0108089447 | \n",
      "Epoch: 5535 | train_loss: 117.1772918701 | test_loss: 6.0107746124 | \n",
      "Epoch: 5536 | train_loss: 117.1770782471 | test_loss: 6.0107336044 | \n",
      "Epoch: 5537 | train_loss: 117.1768646240 | test_loss: 6.0106935501 | \n",
      "Epoch: 5538 | train_loss: 117.1766510010 | test_loss: 6.0106587410 | \n",
      "Epoch: 5539 | train_loss: 117.1764373779 | test_loss: 6.0106215477 | \n",
      "Epoch: 5540 | train_loss: 117.1762084961 | test_loss: 6.0105829239 | \n",
      "Epoch: 5541 | train_loss: 117.1760177612 | test_loss: 6.0105466843 | \n",
      "Epoch: 5542 | train_loss: 117.1757507324 | test_loss: 6.0105051994 | \n",
      "Epoch: 5543 | train_loss: 117.1755752563 | test_loss: 6.0104689598 | \n",
      "Epoch: 5544 | train_loss: 117.1753845215 | test_loss: 6.0104284286 | \n",
      "Epoch: 5545 | train_loss: 117.1751403809 | test_loss: 6.0103917122 | \n",
      "Epoch: 5546 | train_loss: 117.1749343872 | test_loss: 6.0103569031 | \n",
      "Epoch: 5547 | train_loss: 117.1746978760 | test_loss: 6.0103187561 | \n",
      "Epoch: 5548 | train_loss: 117.1744918823 | test_loss: 6.0102825165 | \n",
      "Epoch: 5549 | train_loss: 117.1742782593 | test_loss: 6.0102453232 | \n",
      "Epoch: 5550 | train_loss: 117.1740112305 | test_loss: 6.0102062225 | \n",
      "Epoch: 5551 | train_loss: 117.1738204956 | test_loss: 6.0101728439 | \n",
      "Epoch: 5552 | train_loss: 117.1735839844 | test_loss: 6.0101346970 | \n",
      "Epoch: 5553 | train_loss: 117.1734085083 | test_loss: 6.0100975037 | \n",
      "Epoch: 5554 | train_loss: 117.1732101440 | test_loss: 6.0100626945 | \n",
      "Epoch: 5555 | train_loss: 117.1729583740 | test_loss: 6.0100231171 | \n",
      "Epoch: 5556 | train_loss: 117.1727600098 | test_loss: 6.0099878311 | \n",
      "Epoch: 5557 | train_loss: 117.1725387573 | test_loss: 6.0099558830 | \n",
      "Epoch: 5558 | train_loss: 117.1723403931 | test_loss: 6.0099225044 | \n",
      "Epoch: 5559 | train_loss: 117.1721572876 | test_loss: 6.0098848343 | \n",
      "Epoch: 5560 | train_loss: 117.1719055176 | test_loss: 6.0098433495 | \n",
      "Epoch: 5561 | train_loss: 117.1716995239 | test_loss: 6.0098042488 | \n",
      "Epoch: 5562 | train_loss: 117.1714553833 | test_loss: 6.0097656250 | \n",
      "Epoch: 5563 | train_loss: 117.1712341309 | test_loss: 6.0097284317 | \n",
      "Epoch: 5564 | train_loss: 117.1710510254 | test_loss: 6.0096917152 | \n",
      "Epoch: 5565 | train_loss: 117.1708068848 | test_loss: 6.0096521378 | \n",
      "Epoch: 5566 | train_loss: 117.1705856323 | test_loss: 6.0096201897 | \n",
      "Epoch: 5567 | train_loss: 117.1703796387 | test_loss: 6.0095820427 | \n",
      "Epoch: 5568 | train_loss: 117.1701889038 | test_loss: 6.0095462799 | \n",
      "Epoch: 5569 | train_loss: 117.1699295044 | test_loss: 6.0095090866 | \n",
      "Epoch: 5570 | train_loss: 117.1697158813 | test_loss: 6.0094699860 | \n",
      "Epoch: 5571 | train_loss: 117.1695098877 | test_loss: 6.0094337463 | \n",
      "Epoch: 5572 | train_loss: 117.1692962646 | test_loss: 6.0093965530 | \n",
      "Epoch: 5573 | train_loss: 117.1690902710 | test_loss: 6.0093598366 | \n",
      "Epoch: 5574 | train_loss: 117.1688461304 | test_loss: 6.0093255043 | \n",
      "Epoch: 5575 | train_loss: 117.1686630249 | test_loss: 6.0092902184 | \n",
      "Epoch: 5576 | train_loss: 117.1684112549 | test_loss: 6.0092525482 | \n",
      "Epoch: 5577 | train_loss: 117.1682128906 | test_loss: 6.0092105865 | \n",
      "Epoch: 5578 | train_loss: 117.1679916382 | test_loss: 6.0091767311 | \n",
      "Epoch: 5579 | train_loss: 117.1677551270 | test_loss: 6.0091376305 | \n",
      "Epoch: 5580 | train_loss: 117.1675720215 | test_loss: 6.0091013908 | \n",
      "Epoch: 5581 | train_loss: 117.1673126221 | test_loss: 6.0090651512 | \n",
      "Epoch: 5582 | train_loss: 117.1670837402 | test_loss: 6.0090289116 | \n",
      "Epoch: 5583 | train_loss: 117.1668930054 | test_loss: 6.0089936256 | \n",
      "Epoch: 5584 | train_loss: 117.1666564941 | test_loss: 6.0089550018 | \n",
      "Epoch: 5585 | train_loss: 117.1664352417 | test_loss: 6.0089192390 | \n",
      "Epoch: 5586 | train_loss: 117.1662292480 | test_loss: 6.0088844299 | \n",
      "Epoch: 5587 | train_loss: 117.1659927368 | test_loss: 6.0088496208 | \n",
      "Epoch: 5588 | train_loss: 117.1657943726 | test_loss: 6.0088195801 | \n",
      "Epoch: 5589 | train_loss: 117.1655578613 | test_loss: 6.0087752342 | \n",
      "Epoch: 5590 | train_loss: 117.1653518677 | test_loss: 6.0087404251 | \n",
      "Epoch: 5591 | train_loss: 117.1651306152 | test_loss: 6.0087018013 | \n",
      "Epoch: 5592 | train_loss: 117.1648864746 | test_loss: 6.0086650848 | \n",
      "Epoch: 5593 | train_loss: 117.1646575928 | test_loss: 6.0086255074 | \n",
      "Epoch: 5594 | train_loss: 117.1644592285 | test_loss: 6.0085878372 | \n",
      "Epoch: 5595 | train_loss: 117.1642379761 | test_loss: 6.0085520744 | \n",
      "Epoch: 5596 | train_loss: 117.1640243530 | test_loss: 6.0085158348 | \n",
      "Epoch: 5597 | train_loss: 117.1637802124 | test_loss: 6.0084724426 | \n",
      "Epoch: 5598 | train_loss: 117.1635971069 | test_loss: 6.0084404945 | \n",
      "Epoch: 5599 | train_loss: 117.1633605957 | test_loss: 6.0084013939 | \n",
      "Epoch: 5600 | train_loss: 117.1631774902 | test_loss: 6.0083632469 | \n",
      "Epoch: 5601 | train_loss: 117.1629486084 | test_loss: 6.0083284378 | \n",
      "Epoch: 5602 | train_loss: 117.1627273560 | test_loss: 6.0082893372 | \n",
      "Epoch: 5603 | train_loss: 117.1624908447 | test_loss: 6.0082502365 | \n",
      "Epoch: 5604 | train_loss: 117.1622390747 | test_loss: 6.0082135201 | \n",
      "Epoch: 5605 | train_loss: 117.1620330811 | test_loss: 6.0081763268 | \n",
      "Epoch: 5606 | train_loss: 117.1617736816 | test_loss: 6.0081415176 | \n",
      "Epoch: 5607 | train_loss: 117.1615600586 | test_loss: 6.0081052780 | \n",
      "Epoch: 5608 | train_loss: 117.1613388062 | test_loss: 6.0080704689 | \n",
      "Epoch: 5609 | train_loss: 117.1611251831 | test_loss: 6.0080342293 | \n",
      "Epoch: 5610 | train_loss: 117.1609115601 | test_loss: 6.0080003738 | \n",
      "Epoch: 5611 | train_loss: 117.1607055664 | test_loss: 6.0079631805 | \n",
      "Epoch: 5612 | train_loss: 117.1604919434 | test_loss: 6.0079274178 | \n",
      "Epoch: 5613 | train_loss: 117.1602554321 | test_loss: 6.0078911781 | \n",
      "Epoch: 5614 | train_loss: 117.1600646973 | test_loss: 6.0078525543 | \n",
      "Epoch: 5615 | train_loss: 117.1598434448 | test_loss: 6.0078182220 | \n",
      "Epoch: 5616 | train_loss: 117.1596298218 | test_loss: 6.0077838898 | \n",
      "Epoch: 5617 | train_loss: 117.1594238281 | test_loss: 6.0077486038 | \n",
      "Epoch: 5618 | train_loss: 117.1592025757 | test_loss: 6.0077114105 | \n",
      "Epoch: 5619 | train_loss: 117.1589355469 | test_loss: 6.0076723099 | \n",
      "Epoch: 5620 | train_loss: 117.1587219238 | test_loss: 6.0076384544 | \n",
      "Epoch: 5621 | train_loss: 117.1584930420 | test_loss: 6.0075974464 | \n",
      "Epoch: 5622 | train_loss: 117.1582794189 | test_loss: 6.0075597763 | \n",
      "Epoch: 5623 | train_loss: 117.1580657959 | test_loss: 6.0075297356 | \n",
      "Epoch: 5624 | train_loss: 117.1578292847 | test_loss: 6.0074987411 | \n",
      "Epoch: 5625 | train_loss: 117.1576385498 | test_loss: 6.0074615479 | \n",
      "Epoch: 5626 | train_loss: 117.1574096680 | test_loss: 6.0074243546 | \n",
      "Epoch: 5627 | train_loss: 117.1571960449 | test_loss: 6.0073871613 | \n",
      "Epoch: 5628 | train_loss: 117.1569747925 | test_loss: 6.0073442459 | \n",
      "Epoch: 5629 | train_loss: 117.1567306519 | test_loss: 6.0073108673 | \n",
      "Epoch: 5630 | train_loss: 117.1565246582 | test_loss: 6.0072736740 | \n",
      "Epoch: 5631 | train_loss: 117.1563262939 | test_loss: 6.0072383881 | \n",
      "Epoch: 5632 | train_loss: 117.1560974121 | test_loss: 6.0072021484 | \n",
      "Epoch: 5633 | train_loss: 117.1558914185 | test_loss: 6.0071625710 | \n",
      "Epoch: 5634 | train_loss: 117.1556854248 | test_loss: 6.0071282387 | \n",
      "Epoch: 5635 | train_loss: 117.1555099487 | test_loss: 6.0070867538 | \n",
      "Epoch: 5636 | train_loss: 117.1552810669 | test_loss: 6.0070533752 | \n",
      "Epoch: 5637 | train_loss: 117.1550369263 | test_loss: 6.0070161819 | \n",
      "Epoch: 5638 | train_loss: 117.1548309326 | test_loss: 6.0069837570 | \n",
      "Epoch: 5639 | train_loss: 117.1546020508 | test_loss: 6.0069451332 | \n",
      "Epoch: 5640 | train_loss: 117.1543960571 | test_loss: 6.0069103241 | \n",
      "Epoch: 5641 | train_loss: 117.1542205811 | test_loss: 6.0068745613 | \n",
      "Epoch: 5642 | train_loss: 117.1539611816 | test_loss: 6.0068383217 | \n",
      "Epoch: 5643 | train_loss: 117.1537780762 | test_loss: 6.0068011284 | \n",
      "Epoch: 5644 | train_loss: 117.1535339355 | test_loss: 6.0067634583 | \n",
      "Epoch: 5645 | train_loss: 117.1533203125 | test_loss: 6.0067286491 | \n",
      "Epoch: 5646 | train_loss: 117.1531066895 | test_loss: 6.0066928864 | \n",
      "Epoch: 5647 | train_loss: 117.1528625488 | test_loss: 6.0066595078 | \n",
      "Epoch: 5648 | train_loss: 117.1526641846 | test_loss: 6.0066175461 | \n",
      "Epoch: 5649 | train_loss: 117.1524734497 | test_loss: 6.0065836906 | \n",
      "Epoch: 5650 | train_loss: 117.1522369385 | test_loss: 6.0065498352 | \n",
      "Epoch: 5651 | train_loss: 117.1519851685 | test_loss: 6.0065197945 | \n",
      "Epoch: 5652 | train_loss: 117.1517868042 | test_loss: 6.0064806938 | \n",
      "Epoch: 5653 | train_loss: 117.1515808105 | test_loss: 6.0064454079 | \n",
      "Epoch: 5654 | train_loss: 117.1513595581 | test_loss: 6.0064072609 | \n",
      "Epoch: 5655 | train_loss: 117.1511001587 | test_loss: 6.0063738823 | \n",
      "Epoch: 5656 | train_loss: 117.1508865356 | test_loss: 6.0063309669 | \n",
      "Epoch: 5657 | train_loss: 117.1506576538 | test_loss: 6.0062980652 | \n",
      "Epoch: 5658 | train_loss: 117.1504211426 | test_loss: 6.0062632561 | \n",
      "Epoch: 5659 | train_loss: 117.1501770020 | test_loss: 6.0062260628 | \n",
      "Epoch: 5660 | train_loss: 117.1499481201 | test_loss: 6.0061945915 | \n",
      "Epoch: 5661 | train_loss: 117.1497650146 | test_loss: 6.0061588287 | \n",
      "Epoch: 5662 | train_loss: 117.1495437622 | test_loss: 6.0061225891 | \n",
      "Epoch: 5663 | train_loss: 117.1493225098 | test_loss: 6.0060815811 | \n",
      "Epoch: 5664 | train_loss: 117.1490783691 | test_loss: 6.0060424805 | \n",
      "Epoch: 5665 | train_loss: 117.1489028931 | test_loss: 6.0060100555 | \n",
      "Epoch: 5666 | train_loss: 117.1487197876 | test_loss: 6.0059719086 | \n",
      "Epoch: 5667 | train_loss: 117.1484985352 | test_loss: 6.0059370995 | \n",
      "Epoch: 5668 | train_loss: 117.1482543945 | test_loss: 6.0058989525 | \n",
      "Epoch: 5669 | train_loss: 117.1480407715 | test_loss: 6.0058674812 | \n",
      "Epoch: 5670 | train_loss: 117.1478576660 | test_loss: 6.0058350563 | \n",
      "Epoch: 5671 | train_loss: 117.1476135254 | test_loss: 6.0057969093 | \n",
      "Epoch: 5672 | train_loss: 117.1473846436 | test_loss: 6.0057587624 | \n",
      "Epoch: 5673 | train_loss: 117.1471481323 | test_loss: 6.0057220459 | \n",
      "Epoch: 5674 | train_loss: 117.1469421387 | test_loss: 6.0056896210 | \n",
      "Epoch: 5675 | train_loss: 117.1467056274 | test_loss: 6.0056514740 | \n",
      "Epoch: 5676 | train_loss: 117.1465225220 | test_loss: 6.0056128502 | \n",
      "Epoch: 5677 | train_loss: 117.1462860107 | test_loss: 6.0055766106 | \n",
      "Epoch: 5678 | train_loss: 117.1460418701 | test_loss: 6.0055403709 | \n",
      "Epoch: 5679 | train_loss: 117.1458435059 | test_loss: 6.0055065155 | \n",
      "Epoch: 5680 | train_loss: 117.1456375122 | test_loss: 6.0054731369 | \n",
      "Epoch: 5681 | train_loss: 117.1454315186 | test_loss: 6.0054383278 | \n",
      "Epoch: 5682 | train_loss: 117.1452026367 | test_loss: 6.0053977966 | \n",
      "Epoch: 5683 | train_loss: 117.1449890137 | test_loss: 6.0053639412 | \n",
      "Epoch: 5684 | train_loss: 117.1447753906 | test_loss: 6.0053248405 | \n",
      "Epoch: 5685 | train_loss: 117.1445541382 | test_loss: 6.0052886009 | \n",
      "Epoch: 5686 | train_loss: 117.1443328857 | test_loss: 6.0052561760 | \n",
      "Epoch: 5687 | train_loss: 117.1441040039 | test_loss: 6.0052232742 | \n",
      "Epoch: 5688 | train_loss: 117.1438980103 | test_loss: 6.0051898956 | \n",
      "Epoch: 5689 | train_loss: 117.1436843872 | test_loss: 6.0051555634 | \n",
      "Epoch: 5690 | train_loss: 117.1434631348 | test_loss: 6.0051212311 | \n",
      "Epoch: 5691 | train_loss: 117.1432723999 | test_loss: 6.0050868988 | \n",
      "Epoch: 5692 | train_loss: 117.1430282593 | test_loss: 6.0050506592 | \n",
      "Epoch: 5693 | train_loss: 117.1427993774 | test_loss: 6.0050158501 | \n",
      "Epoch: 5694 | train_loss: 117.1425704956 | test_loss: 6.0049810410 | \n",
      "Epoch: 5695 | train_loss: 117.1423568726 | test_loss: 6.0049414635 | \n",
      "Epoch: 5696 | train_loss: 117.1421585083 | test_loss: 6.0049028397 | \n",
      "Epoch: 5697 | train_loss: 117.1419143677 | test_loss: 6.0048704147 | \n",
      "Epoch: 5698 | train_loss: 117.1417083740 | test_loss: 6.0048341751 | \n",
      "Epoch: 5699 | train_loss: 117.1414871216 | test_loss: 6.0048041344 | \n",
      "Epoch: 5700 | train_loss: 117.1412658691 | test_loss: 6.0047631264 | \n",
      "Epoch: 5701 | train_loss: 117.1410369873 | test_loss: 6.0047273636 | \n",
      "Epoch: 5702 | train_loss: 117.1408462524 | test_loss: 6.0046916008 | \n",
      "Epoch: 5703 | train_loss: 117.1406021118 | test_loss: 6.0046534538 | \n",
      "Epoch: 5704 | train_loss: 117.1404190063 | test_loss: 6.0046195984 | \n",
      "Epoch: 5705 | train_loss: 117.1402053833 | test_loss: 6.0045790672 | \n",
      "Epoch: 5706 | train_loss: 117.1399917603 | test_loss: 6.0045485497 | \n",
      "Epoch: 5707 | train_loss: 117.1397399902 | test_loss: 6.0045137405 | \n",
      "Epoch: 5708 | train_loss: 117.1395339966 | test_loss: 6.0044751167 | \n",
      "Epoch: 5709 | train_loss: 117.1392898560 | test_loss: 6.0044403076 | \n",
      "Epoch: 5710 | train_loss: 117.1390686035 | test_loss: 6.0044069290 | \n",
      "Epoch: 5711 | train_loss: 117.1388473511 | test_loss: 6.0043730736 | \n",
      "Epoch: 5712 | train_loss: 117.1386413574 | test_loss: 6.0043392181 | \n",
      "Epoch: 5713 | train_loss: 117.1384048462 | test_loss: 6.0043025017 | \n",
      "Epoch: 5714 | train_loss: 117.1381607056 | test_loss: 6.0042662621 | \n",
      "Epoch: 5715 | train_loss: 117.1379776001 | test_loss: 6.0042338371 | \n",
      "Epoch: 5716 | train_loss: 117.1377639771 | test_loss: 6.0041999817 | \n",
      "Epoch: 5717 | train_loss: 117.1375503540 | test_loss: 6.0041594505 | \n",
      "Epoch: 5718 | train_loss: 117.1373062134 | test_loss: 6.0041217804 | \n",
      "Epoch: 5719 | train_loss: 117.1370925903 | test_loss: 6.0040879250 | \n",
      "Epoch: 5720 | train_loss: 117.1368560791 | test_loss: 6.0040559769 | \n",
      "Epoch: 5721 | train_loss: 117.1366500854 | test_loss: 6.0040216446 | \n",
      "Epoch: 5722 | train_loss: 117.1364364624 | test_loss: 6.0039892197 | \n",
      "Epoch: 5723 | train_loss: 117.1361694336 | test_loss: 6.0039520264 | \n",
      "Epoch: 5724 | train_loss: 117.1359481812 | test_loss: 6.0039119720 | \n",
      "Epoch: 5725 | train_loss: 117.1357116699 | test_loss: 6.0038776398 | \n",
      "Epoch: 5726 | train_loss: 117.1355285645 | test_loss: 6.0038409233 | \n",
      "Epoch: 5727 | train_loss: 117.1352920532 | test_loss: 6.0038042068 | \n",
      "Epoch: 5728 | train_loss: 117.1350860596 | test_loss: 6.0037646294 | \n",
      "Epoch: 5729 | train_loss: 117.1349029541 | test_loss: 6.0037345886 | \n",
      "Epoch: 5730 | train_loss: 117.1346511841 | test_loss: 6.0036997795 | \n",
      "Epoch: 5731 | train_loss: 117.1344299316 | test_loss: 6.0036649704 | \n",
      "Epoch: 5732 | train_loss: 117.1342315674 | test_loss: 6.0036230087 | \n",
      "Epoch: 5733 | train_loss: 117.1340255737 | test_loss: 6.0035953522 | \n",
      "Epoch: 5734 | train_loss: 117.1337890625 | test_loss: 6.0035567284 | \n",
      "Epoch: 5735 | train_loss: 117.1335906982 | test_loss: 6.0035243034 | \n",
      "Epoch: 5736 | train_loss: 117.1333694458 | test_loss: 6.0034880638 | \n",
      "Epoch: 5737 | train_loss: 117.1331634521 | test_loss: 6.0034565926 | \n",
      "Epoch: 5738 | train_loss: 117.1329803467 | test_loss: 6.0034251213 | \n",
      "Epoch: 5739 | train_loss: 117.1327209473 | test_loss: 6.0033888817 | \n",
      "Epoch: 5740 | train_loss: 117.1325225830 | test_loss: 6.0033555031 | \n",
      "Epoch: 5741 | train_loss: 117.1322937012 | test_loss: 6.0033154488 | \n",
      "Epoch: 5742 | train_loss: 117.1320419312 | test_loss: 6.0032768250 | \n",
      "Epoch: 5743 | train_loss: 117.1318664551 | test_loss: 6.0032415390 | \n",
      "Epoch: 5744 | train_loss: 117.1316223145 | test_loss: 6.0032067299 | \n",
      "Epoch: 5745 | train_loss: 117.1314086914 | test_loss: 6.0031733513 | \n",
      "Epoch: 5746 | train_loss: 117.1312026978 | test_loss: 6.0031347275 | \n",
      "Epoch: 5747 | train_loss: 117.1309585571 | test_loss: 6.0031042099 | \n",
      "Epoch: 5748 | train_loss: 117.1307601929 | test_loss: 6.0030674934 | \n",
      "Epoch: 5749 | train_loss: 117.1305541992 | test_loss: 6.0030345917 | \n",
      "Epoch: 5750 | train_loss: 117.1303405762 | test_loss: 6.0029911995 | \n",
      "Epoch: 5751 | train_loss: 117.1301040649 | test_loss: 6.0029578209 | \n",
      "Epoch: 5752 | train_loss: 117.1299285889 | test_loss: 6.0029201508 | \n",
      "Epoch: 5753 | train_loss: 117.1297302246 | test_loss: 6.0028882027 | \n",
      "Epoch: 5754 | train_loss: 117.1294860840 | test_loss: 6.0028519630 | \n",
      "Epoch: 5755 | train_loss: 117.1292800903 | test_loss: 6.0028162003 | \n",
      "Epoch: 5756 | train_loss: 117.1291046143 | test_loss: 6.0027785301 | \n",
      "Epoch: 5757 | train_loss: 117.1288986206 | test_loss: 6.0027480125 | \n",
      "Epoch: 5758 | train_loss: 117.1287231445 | test_loss: 6.0027155876 | \n",
      "Epoch: 5759 | train_loss: 117.1284790039 | test_loss: 6.0026774406 | \n",
      "Epoch: 5760 | train_loss: 117.1282348633 | test_loss: 6.0026464462 | \n",
      "Epoch: 5761 | train_loss: 117.1280364990 | test_loss: 6.0026121140 | \n",
      "Epoch: 5762 | train_loss: 117.1277770996 | test_loss: 6.0025792122 | \n",
      "Epoch: 5763 | train_loss: 117.1275558472 | test_loss: 6.0025458336 | \n",
      "Epoch: 5764 | train_loss: 117.1273498535 | test_loss: 6.0025105476 | \n",
      "Epoch: 5765 | train_loss: 117.1271514893 | test_loss: 6.0024757385 | \n",
      "Epoch: 5766 | train_loss: 117.1269226074 | test_loss: 6.0024428368 | \n",
      "Epoch: 5767 | train_loss: 117.1267089844 | test_loss: 6.0024065971 | \n",
      "Epoch: 5768 | train_loss: 117.1264877319 | test_loss: 6.0023765564 | \n",
      "Epoch: 5769 | train_loss: 117.1262664795 | test_loss: 6.0023374557 | \n",
      "Epoch: 5770 | train_loss: 117.1260375977 | test_loss: 6.0023055077 | \n",
      "Epoch: 5771 | train_loss: 117.1258239746 | test_loss: 6.0022678375 | \n",
      "Epoch: 5772 | train_loss: 117.1255950928 | test_loss: 6.0022320747 | \n",
      "Epoch: 5773 | train_loss: 117.1253509521 | test_loss: 6.0021944046 | \n",
      "Epoch: 5774 | train_loss: 117.1251754761 | test_loss: 6.0021619797 | \n",
      "Epoch: 5775 | train_loss: 117.1249694824 | test_loss: 6.0021238327 | \n",
      "Epoch: 5776 | train_loss: 117.1247558594 | test_loss: 6.0020928383 | \n",
      "Epoch: 5777 | train_loss: 117.1245040894 | test_loss: 6.0020551682 | \n",
      "Epoch: 5778 | train_loss: 117.1243057251 | test_loss: 6.0020184517 | \n",
      "Epoch: 5779 | train_loss: 117.1240844727 | test_loss: 6.0019874573 | \n",
      "Epoch: 5780 | train_loss: 117.1238937378 | test_loss: 6.0019555092 | \n",
      "Epoch: 5781 | train_loss: 117.1237030029 | test_loss: 6.0019221306 | \n",
      "Epoch: 5782 | train_loss: 117.1234893799 | test_loss: 6.0018882751 | \n",
      "Epoch: 5783 | train_loss: 117.1232833862 | test_loss: 6.0018501282 | \n",
      "Epoch: 5784 | train_loss: 117.1230392456 | test_loss: 6.0018172264 | \n",
      "Epoch: 5785 | train_loss: 117.1228103638 | test_loss: 6.0017805099 | \n",
      "Epoch: 5786 | train_loss: 117.1225814819 | test_loss: 6.0017499924 | \n",
      "Epoch: 5787 | train_loss: 117.1223526001 | test_loss: 6.0017175674 | \n",
      "Epoch: 5788 | train_loss: 117.1221389771 | test_loss: 6.0016846657 | \n",
      "Epoch: 5789 | train_loss: 117.1219406128 | test_loss: 6.0016508102 | \n",
      "Epoch: 5790 | train_loss: 117.1217117310 | test_loss: 6.0016193390 | \n",
      "Epoch: 5791 | train_loss: 117.1215057373 | test_loss: 6.0015830994 | \n",
      "Epoch: 5792 | train_loss: 117.1212615967 | test_loss: 6.0015435219 | \n",
      "Epoch: 5793 | train_loss: 117.1210250854 | test_loss: 6.0015087128 | \n",
      "Epoch: 5794 | train_loss: 117.1208114624 | test_loss: 6.0014772415 | \n",
      "Epoch: 5795 | train_loss: 117.1206130981 | test_loss: 6.0014424324 | \n",
      "Epoch: 5796 | train_loss: 117.1203918457 | test_loss: 6.0014076233 | \n",
      "Epoch: 5797 | train_loss: 117.1202087402 | test_loss: 6.0013747215 | \n",
      "Epoch: 5798 | train_loss: 117.1199722290 | test_loss: 6.0013465881 | \n",
      "Epoch: 5799 | train_loss: 117.1197662354 | test_loss: 6.0013055801 | \n",
      "Epoch: 5800 | train_loss: 117.1195297241 | test_loss: 6.0012726784 | \n",
      "Epoch: 5801 | train_loss: 117.1193389893 | test_loss: 6.0012316704 | \n",
      "Epoch: 5802 | train_loss: 117.1191177368 | test_loss: 6.0012006760 | \n",
      "Epoch: 5803 | train_loss: 117.1188735962 | test_loss: 6.0011639595 | \n",
      "Epoch: 5804 | train_loss: 117.1186828613 | test_loss: 6.0011272430 | \n",
      "Epoch: 5805 | train_loss: 117.1185073853 | test_loss: 6.0011005402 | \n",
      "Epoch: 5806 | train_loss: 117.1182556152 | test_loss: 6.0010643005 | \n",
      "Epoch: 5807 | train_loss: 117.1180496216 | test_loss: 6.0010294914 | \n",
      "Epoch: 5808 | train_loss: 117.1178512573 | test_loss: 6.0009961128 | \n",
      "Epoch: 5809 | train_loss: 117.1175994873 | test_loss: 6.0009598732 | \n",
      "Epoch: 5810 | train_loss: 117.1173934937 | test_loss: 6.0009293556 | \n",
      "Epoch: 5811 | train_loss: 117.1171875000 | test_loss: 6.0008931160 | \n",
      "Epoch: 5812 | train_loss: 117.1169662476 | test_loss: 6.0008568764 | \n",
      "Epoch: 5813 | train_loss: 117.1167221069 | test_loss: 6.0008196831 | \n",
      "Epoch: 5814 | train_loss: 117.1165313721 | test_loss: 6.0007848740 | \n",
      "Epoch: 5815 | train_loss: 117.1162796021 | test_loss: 6.0007448196 | \n",
      "Epoch: 5816 | train_loss: 117.1160736084 | test_loss: 6.0007114410 | \n",
      "Epoch: 5817 | train_loss: 117.1158752441 | test_loss: 6.0006742477 | \n",
      "Epoch: 5818 | train_loss: 117.1156768799 | test_loss: 6.0006403923 | \n",
      "Epoch: 5819 | train_loss: 117.1154327393 | test_loss: 6.0006022453 | \n",
      "Epoch: 5820 | train_loss: 117.1152114868 | test_loss: 6.0005707741 | \n",
      "Epoch: 5821 | train_loss: 117.1149902344 | test_loss: 6.0005397797 | \n",
      "Epoch: 5822 | train_loss: 117.1147842407 | test_loss: 6.0005097389 | \n",
      "Epoch: 5823 | train_loss: 117.1145477295 | test_loss: 6.0004754066 | \n",
      "Epoch: 5824 | train_loss: 117.1143493652 | test_loss: 6.0004396439 | \n",
      "Epoch: 5825 | train_loss: 117.1141204834 | test_loss: 6.0004057884 | \n",
      "Epoch: 5826 | train_loss: 117.1138992310 | test_loss: 6.0003719330 | \n",
      "Epoch: 5827 | train_loss: 117.1136932373 | test_loss: 6.0003395081 | \n",
      "Epoch: 5828 | train_loss: 117.1134567261 | test_loss: 6.0003075600 | \n",
      "Epoch: 5829 | train_loss: 117.1132507324 | test_loss: 6.0002722740 | \n",
      "Epoch: 5830 | train_loss: 117.1130599976 | test_loss: 6.0002360344 | \n",
      "Epoch: 5831 | train_loss: 117.1128234863 | test_loss: 6.0002002716 | \n",
      "Epoch: 5832 | train_loss: 117.1126174927 | test_loss: 6.0001640320 | \n",
      "Epoch: 5833 | train_loss: 117.1123809814 | test_loss: 6.0001325607 | \n",
      "Epoch: 5834 | train_loss: 117.1121902466 | test_loss: 6.0000996590 | \n",
      "Epoch: 5835 | train_loss: 117.1119537354 | test_loss: 6.0000667572 | \n",
      "Epoch: 5836 | train_loss: 117.1117172241 | test_loss: 6.0000324249 | \n",
      "Epoch: 5837 | train_loss: 117.1115036011 | test_loss: 5.9999933243 | \n",
      "Epoch: 5838 | train_loss: 117.1112518311 | test_loss: 5.9999632835 | \n",
      "Epoch: 5839 | train_loss: 117.1110382080 | test_loss: 5.9999256134 | \n",
      "Epoch: 5840 | train_loss: 117.1107940674 | test_loss: 5.9998955727 | \n",
      "Epoch: 5841 | train_loss: 117.1105880737 | test_loss: 5.9998607635 | \n",
      "Epoch: 5842 | train_loss: 117.1103897095 | test_loss: 5.9998264313 | \n",
      "Epoch: 5843 | train_loss: 117.1101684570 | test_loss: 5.9997920990 | \n",
      "Epoch: 5844 | train_loss: 117.1099395752 | test_loss: 5.9997587204 | \n",
      "Epoch: 5845 | train_loss: 117.1097488403 | test_loss: 5.9997224808 | \n",
      "Epoch: 5846 | train_loss: 117.1095046997 | test_loss: 5.9996838570 | \n",
      "Epoch: 5847 | train_loss: 117.1092681885 | test_loss: 5.9996485710 | \n",
      "Epoch: 5848 | train_loss: 117.1090927124 | test_loss: 5.9996190071 | \n",
      "Epoch: 5849 | train_loss: 117.1088638306 | test_loss: 5.9995861053 | \n",
      "Epoch: 5850 | train_loss: 117.1086349487 | test_loss: 5.9995446205 | \n",
      "Epoch: 5851 | train_loss: 117.1084289551 | test_loss: 5.9995117188 | \n",
      "Epoch: 5852 | train_loss: 117.1082305908 | test_loss: 5.9994769096 | \n",
      "Epoch: 5853 | train_loss: 117.1079864502 | test_loss: 5.9994425774 | \n",
      "Epoch: 5854 | train_loss: 117.1077423096 | test_loss: 5.9994120598 | \n",
      "Epoch: 5855 | train_loss: 117.1075515747 | test_loss: 5.9993801117 | \n",
      "Epoch: 5856 | train_loss: 117.1073303223 | test_loss: 5.9993481636 | \n",
      "Epoch: 5857 | train_loss: 117.1071166992 | test_loss: 5.9993114471 | \n",
      "Epoch: 5858 | train_loss: 117.1068878174 | test_loss: 5.9992747307 | \n",
      "Epoch: 5859 | train_loss: 117.1066818237 | test_loss: 5.9992456436 | \n",
      "Epoch: 5860 | train_loss: 117.1064834595 | test_loss: 5.9992122650 | \n",
      "Epoch: 5861 | train_loss: 117.1062698364 | test_loss: 5.9991788864 | \n",
      "Epoch: 5862 | train_loss: 117.1060256958 | test_loss: 5.9991431236 | \n",
      "Epoch: 5863 | train_loss: 117.1058044434 | test_loss: 5.9991092682 | \n",
      "Epoch: 5864 | train_loss: 117.1055984497 | test_loss: 5.9990758896 | \n",
      "Epoch: 5865 | train_loss: 117.1053619385 | test_loss: 5.9990491867 | \n",
      "Epoch: 5866 | train_loss: 117.1051635742 | test_loss: 5.9990153313 | \n",
      "Epoch: 5867 | train_loss: 117.1049423218 | test_loss: 5.9989833832 | \n",
      "Epoch: 5868 | train_loss: 117.1047134399 | test_loss: 5.9989528656 | \n",
      "Epoch: 5869 | train_loss: 117.1044921875 | test_loss: 5.9989190102 | \n",
      "Epoch: 5870 | train_loss: 117.1043167114 | test_loss: 5.9988842010 | \n",
      "Epoch: 5871 | train_loss: 117.1040496826 | test_loss: 5.9988451004 | \n",
      "Epoch: 5872 | train_loss: 117.1038208008 | test_loss: 5.9988160133 | \n",
      "Epoch: 5873 | train_loss: 117.1036453247 | test_loss: 5.9987816811 | \n",
      "Epoch: 5874 | train_loss: 117.1034240723 | test_loss: 5.9987483025 | \n",
      "Epoch: 5875 | train_loss: 117.1032257080 | test_loss: 5.9987125397 | \n",
      "Epoch: 5876 | train_loss: 117.1030197144 | test_loss: 5.9986820221 | \n",
      "Epoch: 5877 | train_loss: 117.1027755737 | test_loss: 5.9986457825 | \n",
      "Epoch: 5878 | train_loss: 117.1025619507 | test_loss: 5.9986138344 | \n",
      "Epoch: 5879 | train_loss: 117.1023559570 | test_loss: 5.9985780716 | \n",
      "Epoch: 5880 | train_loss: 117.1021118164 | test_loss: 5.9985394478 | \n",
      "Epoch: 5881 | train_loss: 117.1018829346 | test_loss: 5.9985094070 | \n",
      "Epoch: 5882 | train_loss: 117.1016921997 | test_loss: 5.9984736443 | \n",
      "Epoch: 5883 | train_loss: 117.1014709473 | test_loss: 5.9984412193 | \n",
      "Epoch: 5884 | train_loss: 117.1012725830 | test_loss: 5.9984059334 | \n",
      "Epoch: 5885 | train_loss: 117.1010437012 | test_loss: 5.9983758926 | \n",
      "Epoch: 5886 | train_loss: 117.1007995605 | test_loss: 5.9983453751 | \n",
      "Epoch: 5887 | train_loss: 117.1005630493 | test_loss: 5.9983129501 | \n",
      "Epoch: 5888 | train_loss: 117.1003875732 | test_loss: 5.9982781410 | \n",
      "Epoch: 5889 | train_loss: 117.1001663208 | test_loss: 5.9982428551 | \n",
      "Epoch: 5890 | train_loss: 117.0999679565 | test_loss: 5.9982089996 | \n",
      "Epoch: 5891 | train_loss: 117.0997467041 | test_loss: 5.9981737137 | \n",
      "Epoch: 5892 | train_loss: 117.0995330811 | test_loss: 5.9981431961 | \n",
      "Epoch: 5893 | train_loss: 117.0993347168 | test_loss: 5.9981074333 | \n",
      "Epoch: 5894 | train_loss: 117.0990982056 | test_loss: 5.9980759621 | \n",
      "Epoch: 5895 | train_loss: 117.0988693237 | test_loss: 5.9980401993 | \n",
      "Epoch: 5896 | train_loss: 117.0986633301 | test_loss: 5.9980072975 | \n",
      "Epoch: 5897 | train_loss: 117.0984268188 | test_loss: 5.9979724884 | \n",
      "Epoch: 5898 | train_loss: 117.0982131958 | test_loss: 5.9979362488 | \n",
      "Epoch: 5899 | train_loss: 117.0980072021 | test_loss: 5.9978990555 | \n",
      "Epoch: 5900 | train_loss: 117.0978012085 | test_loss: 5.9978661537 | \n",
      "Epoch: 5901 | train_loss: 117.0975952148 | test_loss: 5.9978318214 | \n",
      "Epoch: 5902 | train_loss: 117.0973663330 | test_loss: 5.9977984428 | \n",
      "Epoch: 5903 | train_loss: 117.0971298218 | test_loss: 5.9977631569 | \n",
      "Epoch: 5904 | train_loss: 117.0969161987 | test_loss: 5.9977288246 | \n",
      "Epoch: 5905 | train_loss: 117.0967254639 | test_loss: 5.9976940155 | \n",
      "Epoch: 5906 | train_loss: 117.0965042114 | test_loss: 5.9976582527 | \n",
      "Epoch: 5907 | train_loss: 117.0962829590 | test_loss: 5.9976243973 | \n",
      "Epoch: 5908 | train_loss: 117.0960540771 | test_loss: 5.9975934029 | \n",
      "Epoch: 5909 | train_loss: 117.0958404541 | test_loss: 5.9975614548 | \n",
      "Epoch: 5910 | train_loss: 117.0956649780 | test_loss: 5.9975299835 | \n",
      "Epoch: 5911 | train_loss: 117.0954284668 | test_loss: 5.9974918365 | \n",
      "Epoch: 5912 | train_loss: 117.0951843262 | test_loss: 5.9974646568 | \n",
      "Epoch: 5913 | train_loss: 117.0950012207 | test_loss: 5.9974317551 | \n",
      "Epoch: 5914 | train_loss: 117.0947875977 | test_loss: 5.9973993301 | \n",
      "Epoch: 5915 | train_loss: 117.0945739746 | test_loss: 5.9973673820 | \n",
      "Epoch: 5916 | train_loss: 117.0943679810 | test_loss: 5.9973325729 | \n",
      "Epoch: 5917 | train_loss: 117.0941619873 | test_loss: 5.9973034859 | \n",
      "Epoch: 5918 | train_loss: 117.0939025879 | test_loss: 5.9972681999 | \n",
      "Epoch: 5919 | train_loss: 117.0937042236 | test_loss: 5.9972324371 | \n",
      "Epoch: 5920 | train_loss: 117.0934753418 | test_loss: 5.9971966743 | \n",
      "Epoch: 5921 | train_loss: 117.0932693481 | test_loss: 5.9971618652 | \n",
      "Epoch: 5922 | train_loss: 117.0930480957 | test_loss: 5.9971270561 | \n",
      "Epoch: 5923 | train_loss: 117.0928497314 | test_loss: 5.9970960617 | \n",
      "Epoch: 5924 | train_loss: 117.0926437378 | test_loss: 5.9970660210 | \n",
      "Epoch: 5925 | train_loss: 117.0924301147 | test_loss: 5.9970331192 | \n",
      "Epoch: 5926 | train_loss: 117.0922164917 | test_loss: 5.9969968796 | \n",
      "Epoch: 5927 | train_loss: 117.0920333862 | test_loss: 5.9969663620 | \n",
      "Epoch: 5928 | train_loss: 117.0918045044 | test_loss: 5.9969272614 | \n",
      "Epoch: 5929 | train_loss: 117.0915985107 | test_loss: 5.9968962669 | \n",
      "Epoch: 5930 | train_loss: 117.0913467407 | test_loss: 5.9968614578 | \n",
      "Epoch: 5931 | train_loss: 117.0911636353 | test_loss: 5.9968285561 | \n",
      "Epoch: 5932 | train_loss: 117.0909194946 | test_loss: 5.9967975616 | \n",
      "Epoch: 5933 | train_loss: 117.0907058716 | test_loss: 5.9967665672 | \n",
      "Epoch: 5934 | train_loss: 117.0905380249 | test_loss: 5.9967370033 | \n",
      "Epoch: 5935 | train_loss: 117.0903091431 | test_loss: 5.9967055321 | \n",
      "Epoch: 5936 | train_loss: 117.0900802612 | test_loss: 5.9966716766 | \n",
      "Epoch: 5937 | train_loss: 117.0898666382 | test_loss: 5.9966359138 | \n",
      "Epoch: 5938 | train_loss: 117.0896377563 | test_loss: 5.9966030121 | \n",
      "Epoch: 5939 | train_loss: 117.0894012451 | test_loss: 5.9965724945 | \n",
      "Epoch: 5940 | train_loss: 117.0891571045 | test_loss: 5.9965333939 | \n",
      "Epoch: 5941 | train_loss: 117.0889663696 | test_loss: 5.9965009689 | \n",
      "Epoch: 5942 | train_loss: 117.0887374878 | test_loss: 5.9964690208 | \n",
      "Epoch: 5943 | train_loss: 117.0885238647 | test_loss: 5.9964318275 | \n",
      "Epoch: 5944 | train_loss: 117.0882873535 | test_loss: 5.9963974953 | \n",
      "Epoch: 5945 | train_loss: 117.0880889893 | test_loss: 5.9963617325 | \n",
      "Epoch: 5946 | train_loss: 117.0878524780 | test_loss: 5.9963274002 | \n",
      "Epoch: 5947 | train_loss: 117.0876388550 | test_loss: 5.9962944984 | \n",
      "Epoch: 5948 | train_loss: 117.0874099731 | test_loss: 5.9962673187 | \n",
      "Epoch: 5949 | train_loss: 117.0872268677 | test_loss: 5.9962315559 | \n",
      "Epoch: 5950 | train_loss: 117.0869445801 | test_loss: 5.9962019920 | \n",
      "Epoch: 5951 | train_loss: 117.0867462158 | test_loss: 5.9961700439 | \n",
      "Epoch: 5952 | train_loss: 117.0865173340 | test_loss: 5.9961376190 | \n",
      "Epoch: 5953 | train_loss: 117.0862808228 | test_loss: 5.9961047173 | \n",
      "Epoch: 5954 | train_loss: 117.0860824585 | test_loss: 5.9960694313 | \n",
      "Epoch: 5955 | train_loss: 117.0858764648 | test_loss: 5.9960331917 | \n",
      "Epoch: 5956 | train_loss: 117.0856704712 | test_loss: 5.9959974289 | \n",
      "Epoch: 5957 | train_loss: 117.0854263306 | test_loss: 5.9959645271 | \n",
      "Epoch: 5958 | train_loss: 117.0852127075 | test_loss: 5.9959301949 | \n",
      "Epoch: 5959 | train_loss: 117.0850067139 | test_loss: 5.9959025383 | \n",
      "Epoch: 5960 | train_loss: 117.0847625732 | test_loss: 5.9958744049 | \n",
      "Epoch: 5961 | train_loss: 117.0845642090 | test_loss: 5.9958434105 | \n",
      "Epoch: 5962 | train_loss: 117.0843582153 | test_loss: 5.9958043098 | \n",
      "Epoch: 5963 | train_loss: 117.0841293335 | test_loss: 5.9957714081 | \n",
      "Epoch: 5964 | train_loss: 117.0839157104 | test_loss: 5.9957370758 | \n",
      "Epoch: 5965 | train_loss: 117.0837097168 | test_loss: 5.9957041740 | \n",
      "Epoch: 5966 | train_loss: 117.0834884644 | test_loss: 5.9956660271 | \n",
      "Epoch: 5967 | train_loss: 117.0832977295 | test_loss: 5.9956383705 | \n",
      "Epoch: 5968 | train_loss: 117.0830535889 | test_loss: 5.9956040382 | \n",
      "Epoch: 5969 | train_loss: 117.0828628540 | test_loss: 5.9955763817 | \n",
      "Epoch: 5970 | train_loss: 117.0826721191 | test_loss: 5.9955425262 | \n",
      "Epoch: 5971 | train_loss: 117.0824203491 | test_loss: 5.9955153465 | \n",
      "Epoch: 5972 | train_loss: 117.0821990967 | test_loss: 5.9954814911 | \n",
      "Epoch: 5973 | train_loss: 117.0819625854 | test_loss: 5.9954504967 | \n",
      "Epoch: 5974 | train_loss: 117.0817565918 | test_loss: 5.9954152107 | \n",
      "Epoch: 5975 | train_loss: 117.0815429688 | test_loss: 5.9953813553 | \n",
      "Epoch: 5976 | train_loss: 117.0813293457 | test_loss: 5.9953479767 | \n",
      "Epoch: 5977 | train_loss: 117.0811233521 | test_loss: 5.9953117371 | \n",
      "Epoch: 5978 | train_loss: 117.0809020996 | test_loss: 5.9952778816 | \n",
      "Epoch: 5979 | train_loss: 117.0807113647 | test_loss: 5.9952464104 | \n",
      "Epoch: 5980 | train_loss: 117.0805282593 | test_loss: 5.9952111244 | \n",
      "Epoch: 5981 | train_loss: 117.0802764893 | test_loss: 5.9951796532 | \n",
      "Epoch: 5982 | train_loss: 117.0800857544 | test_loss: 5.9951477051 | \n",
      "Epoch: 5983 | train_loss: 117.0798721313 | test_loss: 5.9951114655 | \n",
      "Epoch: 5984 | train_loss: 117.0796356201 | test_loss: 5.9950809479 | \n",
      "Epoch: 5985 | train_loss: 117.0794143677 | test_loss: 5.9950470924 | \n",
      "Epoch: 5986 | train_loss: 117.0791778564 | test_loss: 5.9950141907 | \n",
      "Epoch: 5987 | train_loss: 117.0789871216 | test_loss: 5.9949822426 | \n",
      "Epoch: 5988 | train_loss: 117.0787658691 | test_loss: 5.9949517250 | \n",
      "Epoch: 5989 | train_loss: 117.0785522461 | test_loss: 5.9949202538 | \n",
      "Epoch: 5990 | train_loss: 117.0783615112 | test_loss: 5.9948902130 | \n",
      "Epoch: 5991 | train_loss: 117.0781173706 | test_loss: 5.9948549271 | \n",
      "Epoch: 5992 | train_loss: 117.0779266357 | test_loss: 5.9948310852 | \n",
      "Epoch: 5993 | train_loss: 117.0777130127 | test_loss: 5.9947972298 | \n",
      "Epoch: 5994 | train_loss: 117.0775299072 | test_loss: 5.9947614670 | \n",
      "Epoch: 5995 | train_loss: 117.0773162842 | test_loss: 5.9947261810 | \n",
      "Epoch: 5996 | train_loss: 117.0770797729 | test_loss: 5.9946942329 | \n",
      "Epoch: 5997 | train_loss: 117.0768585205 | test_loss: 5.9946632385 | \n",
      "Epoch: 5998 | train_loss: 117.0766296387 | test_loss: 5.9946293831 | \n",
      "Epoch: 5999 | train_loss: 117.0764236450 | test_loss: 5.9946002960 | \n",
      "Epoch: 6000 | train_loss: 117.0762023926 | test_loss: 5.9945678711 | \n",
      "Epoch: 6001 | train_loss: 117.0759735107 | test_loss: 5.9945373535 | \n",
      "Epoch: 6002 | train_loss: 117.0757293701 | test_loss: 5.9945054054 | \n",
      "Epoch: 6003 | train_loss: 117.0755462646 | test_loss: 5.9944715500 | \n",
      "Epoch: 6004 | train_loss: 117.0753097534 | test_loss: 5.9944362640 | \n",
      "Epoch: 6005 | train_loss: 117.0751190186 | test_loss: 5.9944047928 | \n",
      "Epoch: 6006 | train_loss: 117.0749130249 | test_loss: 5.9943742752 | \n",
      "Epoch: 6007 | train_loss: 117.0746994019 | test_loss: 5.9943389893 | \n",
      "Epoch: 6008 | train_loss: 117.0745010376 | test_loss: 5.9943089485 | \n",
      "Epoch: 6009 | train_loss: 117.0742645264 | test_loss: 5.9942779541 | \n",
      "Epoch: 6010 | train_loss: 117.0740661621 | test_loss: 5.9942469597 | \n",
      "Epoch: 6011 | train_loss: 117.0738601685 | test_loss: 5.9942188263 | \n",
      "Epoch: 6012 | train_loss: 117.0736694336 | test_loss: 5.9941906929 | \n",
      "Epoch: 6013 | train_loss: 117.0734405518 | test_loss: 5.9941558838 | \n",
      "Epoch: 6014 | train_loss: 117.0732116699 | test_loss: 5.9941258430 | \n",
      "Epoch: 6015 | train_loss: 117.0729522705 | test_loss: 5.9940953255 | \n",
      "Epoch: 6016 | train_loss: 117.0727767944 | test_loss: 5.9940638542 | \n",
      "Epoch: 6017 | train_loss: 117.0725555420 | test_loss: 5.9940314293 | \n",
      "Epoch: 6018 | train_loss: 117.0723419189 | test_loss: 5.9939937592 | \n",
      "Epoch: 6019 | train_loss: 117.0721206665 | test_loss: 5.9939599037 | \n",
      "Epoch: 6020 | train_loss: 117.0719299316 | test_loss: 5.9939246178 | \n",
      "Epoch: 6021 | train_loss: 117.0717086792 | test_loss: 5.9938945770 | \n",
      "Epoch: 6022 | train_loss: 117.0715026855 | test_loss: 5.9938645363 | \n",
      "Epoch: 6023 | train_loss: 117.0712890625 | test_loss: 5.9938340187 | \n",
      "Epoch: 6024 | train_loss: 117.0711059570 | test_loss: 5.9938001633 | \n",
      "Epoch: 6025 | train_loss: 117.0708465576 | test_loss: 5.9937696457 | \n",
      "Epoch: 6026 | train_loss: 117.0706481934 | test_loss: 5.9937372208 | \n",
      "Epoch: 6027 | train_loss: 117.0704421997 | test_loss: 5.9937086105 | \n",
      "Epoch: 6028 | train_loss: 117.0702209473 | test_loss: 5.9936761856 | \n",
      "Epoch: 6029 | train_loss: 117.0700302124 | test_loss: 5.9936394691 | \n",
      "Epoch: 6030 | train_loss: 117.0697937012 | test_loss: 5.9936132431 | \n",
      "Epoch: 6031 | train_loss: 117.0695800781 | test_loss: 5.9935817719 | \n",
      "Epoch: 6032 | train_loss: 117.0693740845 | test_loss: 5.9935498238 | \n",
      "Epoch: 6033 | train_loss: 117.0691986084 | test_loss: 5.9935135841 | \n",
      "Epoch: 6034 | train_loss: 117.0689620972 | test_loss: 5.9934787750 | \n",
      "Epoch: 6035 | train_loss: 117.0687789917 | test_loss: 5.9934492111 | \n",
      "Epoch: 6036 | train_loss: 117.0685424805 | test_loss: 5.9934163094 | \n",
      "Epoch: 6037 | train_loss: 117.0682983398 | test_loss: 5.9933834076 | \n",
      "Epoch: 6038 | train_loss: 117.0680847168 | test_loss: 5.9933509827 | \n",
      "Epoch: 6039 | train_loss: 117.0678634644 | test_loss: 5.9933152199 | \n",
      "Epoch: 6040 | train_loss: 117.0676574707 | test_loss: 5.9932832718 | \n",
      "Epoch: 6041 | train_loss: 117.0674514771 | test_loss: 5.9932489395 | \n",
      "Epoch: 6042 | train_loss: 117.0672302246 | test_loss: 5.9932193756 | \n",
      "Epoch: 6043 | train_loss: 117.0670242310 | test_loss: 5.9931874275 | \n",
      "Epoch: 6044 | train_loss: 117.0667953491 | test_loss: 5.9931573868 | \n",
      "Epoch: 6045 | train_loss: 117.0665740967 | test_loss: 5.9931201935 | \n",
      "Epoch: 6046 | train_loss: 117.0663681030 | test_loss: 5.9930872917 | \n",
      "Epoch: 6047 | train_loss: 117.0661239624 | test_loss: 5.9930558205 | \n",
      "Epoch: 6048 | train_loss: 117.0659408569 | test_loss: 5.9930262566 | \n",
      "Epoch: 6049 | train_loss: 117.0657348633 | test_loss: 5.9929895401 | \n",
      "Epoch: 6050 | train_loss: 117.0655212402 | test_loss: 5.9929575920 | \n",
      "Epoch: 6051 | train_loss: 117.0652770996 | test_loss: 5.9929270744 | \n",
      "Epoch: 6052 | train_loss: 117.0650787354 | test_loss: 5.9928960800 | \n",
      "Epoch: 6053 | train_loss: 117.0648727417 | test_loss: 5.9928655624 | \n",
      "Epoch: 6054 | train_loss: 117.0646667480 | test_loss: 5.9928355217 | \n",
      "Epoch: 6055 | train_loss: 117.0644454956 | test_loss: 5.9928016663 | \n",
      "Epoch: 6056 | train_loss: 117.0642089844 | test_loss: 5.9927678108 | \n",
      "Epoch: 6057 | train_loss: 117.0640029907 | test_loss: 5.9927353859 | \n",
      "Epoch: 6058 | train_loss: 117.0637893677 | test_loss: 5.9927020073 | \n",
      "Epoch: 6059 | train_loss: 117.0636062622 | test_loss: 5.9926729202 | \n",
      "Epoch: 6060 | train_loss: 117.0633850098 | test_loss: 5.9926357269 | \n",
      "Epoch: 6061 | train_loss: 117.0631866455 | test_loss: 5.9926033020 | \n",
      "Epoch: 6062 | train_loss: 117.0629501343 | test_loss: 5.9925689697 | \n",
      "Epoch: 6063 | train_loss: 117.0627212524 | test_loss: 5.9925379753 | \n",
      "Epoch: 6064 | train_loss: 117.0625000000 | test_loss: 5.9925065041 | \n",
      "Epoch: 6065 | train_loss: 117.0622787476 | test_loss: 5.9924745560 | \n",
      "Epoch: 6066 | train_loss: 117.0620574951 | test_loss: 5.9924407005 | \n",
      "Epoch: 6067 | train_loss: 117.0618286133 | test_loss: 5.9924092293 | \n",
      "Epoch: 6068 | train_loss: 117.0616302490 | test_loss: 5.9923787117 | \n",
      "Epoch: 6069 | train_loss: 117.0613937378 | test_loss: 5.9923458099 | \n",
      "Epoch: 6070 | train_loss: 117.0612106323 | test_loss: 5.9923119545 | \n",
      "Epoch: 6071 | train_loss: 117.0610275269 | test_loss: 5.9922847748 | \n",
      "Epoch: 6072 | train_loss: 117.0608215332 | test_loss: 5.9922513962 | \n",
      "Epoch: 6073 | train_loss: 117.0605850220 | test_loss: 5.9922242165 | \n",
      "Epoch: 6074 | train_loss: 117.0603942871 | test_loss: 5.9921956062 | \n",
      "Epoch: 6075 | train_loss: 117.0601577759 | test_loss: 5.9921588898 | \n",
      "Epoch: 6076 | train_loss: 117.0599822998 | test_loss: 5.9921307564 | \n",
      "Epoch: 6077 | train_loss: 117.0597381592 | test_loss: 5.9920988083 | \n",
      "Epoch: 6078 | train_loss: 117.0595397949 | test_loss: 5.9920654297 | \n",
      "Epoch: 6079 | train_loss: 117.0593185425 | test_loss: 5.9920334816 | \n",
      "Epoch: 6080 | train_loss: 117.0590744019 | test_loss: 5.9920048714 | \n",
      "Epoch: 6081 | train_loss: 117.0588607788 | test_loss: 5.9919753075 | \n",
      "Epoch: 6082 | train_loss: 117.0586624146 | test_loss: 5.9919390678 | \n",
      "Epoch: 6083 | train_loss: 117.0584411621 | test_loss: 5.9919018745 | \n",
      "Epoch: 6084 | train_loss: 117.0582199097 | test_loss: 5.9918708801 | \n",
      "Epoch: 6085 | train_loss: 117.0580062866 | test_loss: 5.9918389320 | \n",
      "Epoch: 6086 | train_loss: 117.0578231812 | test_loss: 5.9918055534 | \n",
      "Epoch: 6087 | train_loss: 117.0576095581 | test_loss: 5.9917736053 | \n",
      "Epoch: 6088 | train_loss: 117.0573730469 | test_loss: 5.9917430878 | \n",
      "Epoch: 6089 | train_loss: 117.0571594238 | test_loss: 5.9917120934 | \n",
      "Epoch: 6090 | train_loss: 117.0569534302 | test_loss: 5.9916796684 | \n",
      "Epoch: 6091 | train_loss: 117.0567321777 | test_loss: 5.9916453362 | \n",
      "Epoch: 6092 | train_loss: 117.0564727783 | test_loss: 5.9916143417 | \n",
      "Epoch: 6093 | train_loss: 117.0562820435 | test_loss: 5.9915866852 | \n",
      "Epoch: 6094 | train_loss: 117.0561065674 | test_loss: 5.9915513992 | \n",
      "Epoch: 6095 | train_loss: 117.0558471680 | test_loss: 5.9915199280 | \n",
      "Epoch: 6096 | train_loss: 117.0556411743 | test_loss: 5.9914903641 | \n",
      "Epoch: 6097 | train_loss: 117.0554199219 | test_loss: 5.9914593697 | \n",
      "Epoch: 6098 | train_loss: 117.0551834106 | test_loss: 5.9914293289 | \n",
      "Epoch: 6099 | train_loss: 117.0549392700 | test_loss: 5.9913997650 | \n",
      "Epoch: 6100 | train_loss: 117.0547561646 | test_loss: 5.9913659096 | \n",
      "Epoch: 6101 | train_loss: 117.0545425415 | test_loss: 5.9913282394 | \n",
      "Epoch: 6102 | train_loss: 117.0542907715 | test_loss: 5.9912986755 | \n",
      "Epoch: 6103 | train_loss: 117.0540847778 | test_loss: 5.9912691116 | \n",
      "Epoch: 6104 | train_loss: 117.0538558960 | test_loss: 5.9912376404 | \n",
      "Epoch: 6105 | train_loss: 117.0536193848 | test_loss: 5.9912061691 | \n",
      "Epoch: 6106 | train_loss: 117.0534057617 | test_loss: 5.9911694527 | \n",
      "Epoch: 6107 | train_loss: 117.0532226562 | test_loss: 5.9911351204 | \n",
      "Epoch: 6108 | train_loss: 117.0530090332 | test_loss: 5.9911022186 | \n",
      "Epoch: 6109 | train_loss: 117.0527725220 | test_loss: 5.9910759926 | \n",
      "Epoch: 6110 | train_loss: 117.0525360107 | test_loss: 5.9910445213 | \n",
      "Epoch: 6111 | train_loss: 117.0523223877 | test_loss: 5.9910140038 | \n",
      "Epoch: 6112 | train_loss: 117.0521163940 | test_loss: 5.9909834862 | \n",
      "Epoch: 6113 | train_loss: 117.0518951416 | test_loss: 5.9909539223 | \n",
      "Epoch: 6114 | train_loss: 117.0517120361 | test_loss: 5.9909191132 | \n",
      "Epoch: 6115 | train_loss: 117.0515136719 | test_loss: 5.9908919334 | \n",
      "Epoch: 6116 | train_loss: 117.0512771606 | test_loss: 5.9908628464 | \n",
      "Epoch: 6117 | train_loss: 117.0510635376 | test_loss: 5.9908318520 | \n",
      "Epoch: 6118 | train_loss: 117.0508422852 | test_loss: 5.9907989502 | \n",
      "Epoch: 6119 | train_loss: 117.0506515503 | test_loss: 5.9907722473 | \n",
      "Epoch: 6120 | train_loss: 117.0504379272 | test_loss: 5.9907445908 | \n",
      "Epoch: 6121 | train_loss: 117.0502471924 | test_loss: 5.9907097816 | \n",
      "Epoch: 6122 | train_loss: 117.0500259399 | test_loss: 5.9906792641 | \n",
      "Epoch: 6123 | train_loss: 117.0498123169 | test_loss: 5.9906468391 | \n",
      "Epoch: 6124 | train_loss: 117.0495986938 | test_loss: 5.9906153679 | \n",
      "Epoch: 6125 | train_loss: 117.0493927002 | test_loss: 5.9905848503 | \n",
      "Epoch: 6126 | train_loss: 117.0491638184 | test_loss: 5.9905524254 | \n",
      "Epoch: 6127 | train_loss: 117.0489578247 | test_loss: 5.9905257225 | \n",
      "Epoch: 6128 | train_loss: 117.0487365723 | test_loss: 5.9904923439 | \n",
      "Epoch: 6129 | train_loss: 117.0485382080 | test_loss: 5.9904618263 | \n",
      "Epoch: 6130 | train_loss: 117.0483016968 | test_loss: 5.9904260635 | \n",
      "Epoch: 6131 | train_loss: 117.0480728149 | test_loss: 5.9903974533 | \n",
      "Epoch: 6132 | train_loss: 117.0478515625 | test_loss: 5.9903631210 | \n",
      "Epoch: 6133 | train_loss: 117.0476684570 | test_loss: 5.9903321266 | \n",
      "Epoch: 6134 | train_loss: 117.0474548340 | test_loss: 5.9903025627 | \n",
      "Epoch: 6135 | train_loss: 117.0472640991 | test_loss: 5.9902706146 | \n",
      "Epoch: 6136 | train_loss: 117.0470581055 | test_loss: 5.9902348518 | \n",
      "Epoch: 6137 | train_loss: 117.0468673706 | test_loss: 5.9902076721 | \n",
      "Epoch: 6138 | train_loss: 117.0466308594 | test_loss: 5.9901719093 | \n",
      "Epoch: 6139 | train_loss: 117.0463867188 | test_loss: 5.9901471138 | \n",
      "Epoch: 6140 | train_loss: 117.0461730957 | test_loss: 5.9901108742 | \n",
      "Epoch: 6141 | train_loss: 117.0459213257 | test_loss: 5.9900813103 | \n",
      "Epoch: 6142 | train_loss: 117.0457000732 | test_loss: 5.9900493622 | \n",
      "Epoch: 6143 | train_loss: 117.0455017090 | test_loss: 5.9900159836 | \n",
      "Epoch: 6144 | train_loss: 117.0452728271 | test_loss: 5.9899878502 | \n",
      "Epoch: 6145 | train_loss: 117.0450363159 | test_loss: 5.9899573326 | \n",
      "Epoch: 6146 | train_loss: 117.0448303223 | test_loss: 5.9899234772 | \n",
      "Epoch: 6147 | train_loss: 117.0445861816 | test_loss: 5.9898962975 | \n",
      "Epoch: 6148 | train_loss: 117.0443954468 | test_loss: 5.9898610115 | \n",
      "Epoch: 6149 | train_loss: 117.0441589355 | test_loss: 5.9898281097 | \n",
      "Epoch: 6150 | train_loss: 117.0439224243 | test_loss: 5.9897942543 | \n",
      "Epoch: 6151 | train_loss: 117.0437469482 | test_loss: 5.9897651672 | \n",
      "Epoch: 6152 | train_loss: 117.0435180664 | test_loss: 5.9897356033 | \n",
      "Epoch: 6153 | train_loss: 117.0433044434 | test_loss: 5.9897060394 | \n",
      "Epoch: 6154 | train_loss: 117.0430374146 | test_loss: 5.9896745682 | \n",
      "Epoch: 6155 | train_loss: 117.0428237915 | test_loss: 5.9896473885 | \n",
      "Epoch: 6156 | train_loss: 117.0426330566 | test_loss: 5.9896101952 | \n",
      "Epoch: 6157 | train_loss: 117.0424346924 | test_loss: 5.9895825386 | \n",
      "Epoch: 6158 | train_loss: 117.0422134399 | test_loss: 5.9895482063 | \n",
      "Epoch: 6159 | train_loss: 117.0419921875 | test_loss: 5.9895114899 | \n",
      "Epoch: 6160 | train_loss: 117.0417709351 | test_loss: 5.9894843102 | \n",
      "Epoch: 6161 | train_loss: 117.0415649414 | test_loss: 5.9894523621 | \n",
      "Epoch: 6162 | train_loss: 117.0413208008 | test_loss: 5.9894208908 | \n",
      "Epoch: 6163 | train_loss: 117.0411224365 | test_loss: 5.9893922806 | \n",
      "Epoch: 6164 | train_loss: 117.0409240723 | test_loss: 5.9893589020 | \n",
      "Epoch: 6165 | train_loss: 117.0407104492 | test_loss: 5.9893298149 | \n",
      "Epoch: 6166 | train_loss: 117.0405044556 | test_loss: 5.9892973900 | \n",
      "Epoch: 6167 | train_loss: 117.0402526855 | test_loss: 5.9892640114 | \n",
      "Epoch: 6168 | train_loss: 117.0400085449 | test_loss: 5.9892330170 | \n",
      "Epoch: 6169 | train_loss: 117.0398178101 | test_loss: 5.9892053604 | \n",
      "Epoch: 6170 | train_loss: 117.0395736694 | test_loss: 5.9891686440 | \n",
      "Epoch: 6171 | train_loss: 117.0393371582 | test_loss: 5.9891400337 | \n",
      "Epoch: 6172 | train_loss: 117.0391540527 | test_loss: 5.9891066551 | \n",
      "Epoch: 6173 | train_loss: 117.0389480591 | test_loss: 5.9890770912 | \n",
      "Epoch: 6174 | train_loss: 117.0387115479 | test_loss: 5.9890460968 | \n",
      "Epoch: 6175 | train_loss: 117.0384979248 | test_loss: 5.9890198708 | \n",
      "Epoch: 6176 | train_loss: 117.0382308960 | test_loss: 5.9889836311 | \n",
      "Epoch: 6177 | train_loss: 117.0380401611 | test_loss: 5.9889526367 | \n",
      "Epoch: 6178 | train_loss: 117.0378189087 | test_loss: 5.9889240265 | \n",
      "Epoch: 6179 | train_loss: 117.0376281738 | test_loss: 5.9888973236 | \n",
      "Epoch: 6180 | train_loss: 117.0374145508 | test_loss: 5.9888668060 | \n",
      "Epoch: 6181 | train_loss: 117.0371704102 | test_loss: 5.9888362885 | \n",
      "Epoch: 6182 | train_loss: 117.0369796753 | test_loss: 5.9888024330 | \n",
      "Epoch: 6183 | train_loss: 117.0367660522 | test_loss: 5.9887733459 | \n",
      "Epoch: 6184 | train_loss: 117.0365600586 | test_loss: 5.9887366295 | \n",
      "Epoch: 6185 | train_loss: 117.0363845825 | test_loss: 5.9887132645 | \n",
      "Epoch: 6186 | train_loss: 117.0361557007 | test_loss: 5.9886798859 | \n",
      "Epoch: 6187 | train_loss: 117.0359573364 | test_loss: 5.9886469841 | \n",
      "Epoch: 6188 | train_loss: 117.0357360840 | test_loss: 5.9886212349 | \n",
      "Epoch: 6189 | train_loss: 117.0355377197 | test_loss: 5.9885945320 | \n",
      "Epoch: 6190 | train_loss: 117.0353012085 | test_loss: 5.9885630608 | \n",
      "Epoch: 6191 | train_loss: 117.0351104736 | test_loss: 5.9885301590 | \n",
      "Epoch: 6192 | train_loss: 117.0348815918 | test_loss: 5.9885034561 | \n",
      "Epoch: 6193 | train_loss: 117.0346679688 | test_loss: 5.9884767532 | \n",
      "Epoch: 6194 | train_loss: 117.0344543457 | test_loss: 5.9884386063 | \n",
      "Epoch: 6195 | train_loss: 117.0342254639 | test_loss: 5.9884071350 | \n",
      "Epoch: 6196 | train_loss: 117.0340423584 | test_loss: 5.9883742332 | \n",
      "Epoch: 6197 | train_loss: 117.0338287354 | test_loss: 5.9883451462 | \n",
      "Epoch: 6198 | train_loss: 117.0335998535 | test_loss: 5.9883122444 | \n",
      "Epoch: 6199 | train_loss: 117.0334167480 | test_loss: 5.9882812500 | \n",
      "Epoch: 6200 | train_loss: 117.0331954956 | test_loss: 5.9882488251 | \n",
      "Epoch: 6201 | train_loss: 117.0329818726 | test_loss: 5.9882187843 | \n",
      "Epoch: 6202 | train_loss: 117.0327835083 | test_loss: 5.9881849289 | \n",
      "Epoch: 6203 | train_loss: 117.0325622559 | test_loss: 5.9881525040 | \n",
      "Epoch: 6204 | train_loss: 117.0323410034 | test_loss: 5.9881267548 | \n",
      "Epoch: 6205 | train_loss: 117.0321350098 | test_loss: 5.9880976677 | \n",
      "Epoch: 6206 | train_loss: 117.0319213867 | test_loss: 5.9880714417 | \n",
      "Epoch: 6207 | train_loss: 117.0317077637 | test_loss: 5.9880394936 | \n",
      "Epoch: 6208 | train_loss: 117.0314712524 | test_loss: 5.9880051613 | \n",
      "Epoch: 6209 | train_loss: 117.0312805176 | test_loss: 5.9879775047 | \n",
      "Epoch: 6210 | train_loss: 117.0310516357 | test_loss: 5.9879493713 | \n",
      "Epoch: 6211 | train_loss: 117.0308532715 | test_loss: 5.9879217148 | \n",
      "Epoch: 6212 | train_loss: 117.0306472778 | test_loss: 5.9878911972 | \n",
      "Epoch: 6213 | train_loss: 117.0304183960 | test_loss: 5.9878625870 | \n",
      "Epoch: 6214 | train_loss: 117.0302124023 | test_loss: 5.9878325462 | \n",
      "Epoch: 6215 | train_loss: 117.0300216675 | test_loss: 5.9878010750 | \n",
      "Epoch: 6216 | train_loss: 117.0297927856 | test_loss: 5.9877638817 | \n",
      "Epoch: 6217 | train_loss: 117.0295867920 | test_loss: 5.9877386093 | \n",
      "Epoch: 6218 | train_loss: 117.0293731689 | test_loss: 5.9877061844 | \n",
      "Epoch: 6219 | train_loss: 117.0291519165 | test_loss: 5.9876790047 | \n",
      "Epoch: 6220 | train_loss: 117.0289382935 | test_loss: 5.9876475334 | \n",
      "Epoch: 6221 | train_loss: 117.0287170410 | test_loss: 5.9876165390 | \n",
      "Epoch: 6222 | train_loss: 117.0285186768 | test_loss: 5.9875888824 | \n",
      "Epoch: 6223 | train_loss: 117.0283126831 | test_loss: 5.9875569344 | \n",
      "Epoch: 6224 | train_loss: 117.0280838013 | test_loss: 5.9875245094 | \n",
      "Epoch: 6225 | train_loss: 117.0278701782 | test_loss: 5.9874949455 | \n",
      "Epoch: 6226 | train_loss: 117.0276947021 | test_loss: 5.9874591827 | \n",
      "Epoch: 6227 | train_loss: 117.0274734497 | test_loss: 5.9874291420 | \n",
      "Epoch: 6228 | train_loss: 117.0272369385 | test_loss: 5.9873976707 | \n",
      "Epoch: 6229 | train_loss: 117.0270309448 | test_loss: 5.9873704910 | \n",
      "Epoch: 6230 | train_loss: 117.0268325806 | test_loss: 5.9873409271 | \n",
      "Epoch: 6231 | train_loss: 117.0266113281 | test_loss: 5.9873065948 | \n",
      "Epoch: 6232 | train_loss: 117.0264053345 | test_loss: 5.9872779846 | \n",
      "Epoch: 6233 | train_loss: 117.0261993408 | test_loss: 5.9872422218 | \n",
      "Epoch: 6234 | train_loss: 117.0259857178 | test_loss: 5.9872117043 | \n",
      "Epoch: 6235 | train_loss: 117.0257415771 | test_loss: 5.9871788025 | \n",
      "Epoch: 6236 | train_loss: 117.0255432129 | test_loss: 5.9871497154 | \n",
      "Epoch: 6237 | train_loss: 117.0253524780 | test_loss: 5.9871201515 | \n",
      "Epoch: 6238 | train_loss: 117.0251159668 | test_loss: 5.9870996475 | \n",
      "Epoch: 6239 | train_loss: 117.0248794556 | test_loss: 5.9870667458 | \n",
      "Epoch: 6240 | train_loss: 117.0246582031 | test_loss: 5.9870347977 | \n",
      "Epoch: 6241 | train_loss: 117.0244598389 | test_loss: 5.9870057106 | \n",
      "Epoch: 6242 | train_loss: 117.0242004395 | test_loss: 5.9869737625 | \n",
      "Epoch: 6243 | train_loss: 117.0239944458 | test_loss: 5.9869427681 | \n",
      "Epoch: 6244 | train_loss: 117.0237731934 | test_loss: 5.9869136810 | \n",
      "Epoch: 6245 | train_loss: 117.0235824585 | test_loss: 5.9868822098 | \n",
      "Epoch: 6246 | train_loss: 117.0233612061 | test_loss: 5.9868531227 | \n",
      "Epoch: 6247 | train_loss: 117.0231628418 | test_loss: 5.9868288040 | \n",
      "Epoch: 6248 | train_loss: 117.0229492188 | test_loss: 5.9867954254 | \n",
      "Epoch: 6249 | train_loss: 117.0227279663 | test_loss: 5.9867610931 | \n",
      "Epoch: 6250 | train_loss: 117.0224914551 | test_loss: 5.9867248535 | \n",
      "Epoch: 6251 | train_loss: 117.0223083496 | test_loss: 5.9866943359 | \n",
      "Epoch: 6252 | train_loss: 117.0220870972 | test_loss: 5.9866652489 | \n",
      "Epoch: 6253 | train_loss: 117.0218429565 | test_loss: 5.9866328239 | \n",
      "Epoch: 6254 | train_loss: 117.0216598511 | test_loss: 5.9866042137 | \n",
      "Epoch: 6255 | train_loss: 117.0214157104 | test_loss: 5.9865784645 | \n",
      "Epoch: 6256 | train_loss: 117.0212020874 | test_loss: 5.9865489006 | \n",
      "Epoch: 6257 | train_loss: 117.0210037231 | test_loss: 5.9865226746 | \n",
      "Epoch: 6258 | train_loss: 117.0207977295 | test_loss: 5.9864940643 | \n",
      "Epoch: 6259 | train_loss: 117.0205841064 | test_loss: 5.9864611626 | \n",
      "Epoch: 6260 | train_loss: 117.0203781128 | test_loss: 5.9864320755 | \n",
      "Epoch: 6261 | train_loss: 117.0201873779 | test_loss: 5.9864029884 | \n",
      "Epoch: 6262 | train_loss: 117.0199737549 | test_loss: 5.9863715172 | \n",
      "Epoch: 6263 | train_loss: 117.0197525024 | test_loss: 5.9863376617 | \n",
      "Epoch: 6264 | train_loss: 117.0195388794 | test_loss: 5.9863090515 | \n",
      "Epoch: 6265 | train_loss: 117.0193481445 | test_loss: 5.9862809181 | \n",
      "Epoch: 6266 | train_loss: 117.0191345215 | test_loss: 5.9862523079 | \n",
      "Epoch: 6267 | train_loss: 117.0189285278 | test_loss: 5.9862170219 | \n",
      "Epoch: 6268 | train_loss: 117.0186996460 | test_loss: 5.9861869812 | \n",
      "Epoch: 6269 | train_loss: 117.0184860229 | test_loss: 5.9861545563 | \n",
      "Epoch: 6270 | train_loss: 117.0183029175 | test_loss: 5.9861273766 | \n",
      "Epoch: 6271 | train_loss: 117.0180587769 | test_loss: 5.9860978127 | \n",
      "Epoch: 6272 | train_loss: 117.0178604126 | test_loss: 5.9860668182 | \n",
      "Epoch: 6273 | train_loss: 117.0176696777 | test_loss: 5.9860396385 | \n",
      "Epoch: 6274 | train_loss: 117.0174407959 | test_loss: 5.9860134125 | \n",
      "Epoch: 6275 | train_loss: 117.0172424316 | test_loss: 5.9859838486 | \n",
      "Epoch: 6276 | train_loss: 117.0170440674 | test_loss: 5.9859538078 | \n",
      "Epoch: 6277 | train_loss: 117.0168228149 | test_loss: 5.9859251976 | \n",
      "Epoch: 6278 | train_loss: 117.0166091919 | test_loss: 5.9858961105 | \n",
      "Epoch: 6279 | train_loss: 117.0164260864 | test_loss: 5.9858665466 | \n",
      "Epoch: 6280 | train_loss: 117.0162277222 | test_loss: 5.9858341217 | \n",
      "Epoch: 6281 | train_loss: 117.0160064697 | test_loss: 5.9858045578 | \n",
      "Epoch: 6282 | train_loss: 117.0158081055 | test_loss: 5.9857792854 | \n",
      "Epoch: 6283 | train_loss: 117.0155868530 | test_loss: 5.9857478142 | \n",
      "Epoch: 6284 | train_loss: 117.0154037476 | test_loss: 5.9857149124 | \n",
      "Epoch: 6285 | train_loss: 117.0151824951 | test_loss: 5.9856843948 | \n",
      "Epoch: 6286 | train_loss: 117.0149688721 | test_loss: 5.9856581688 | \n",
      "Epoch: 6287 | train_loss: 117.0147476196 | test_loss: 5.9856286049 | \n",
      "Epoch: 6288 | train_loss: 117.0145568848 | test_loss: 5.9855952263 | \n",
      "Epoch: 6289 | train_loss: 117.0143585205 | test_loss: 5.9855656624 | \n",
      "Epoch: 6290 | train_loss: 117.0141372681 | test_loss: 5.9855370522 | \n",
      "Epoch: 6291 | train_loss: 117.0139236450 | test_loss: 5.9855132103 | \n",
      "Epoch: 6292 | train_loss: 117.0137405396 | test_loss: 5.9854822159 | \n",
      "Epoch: 6293 | train_loss: 117.0135269165 | test_loss: 5.9854478836 | \n",
      "Epoch: 6294 | train_loss: 117.0133132935 | test_loss: 5.9854197502 | \n",
      "Epoch: 6295 | train_loss: 117.0131378174 | test_loss: 5.9853863716 | \n",
      "Epoch: 6296 | train_loss: 117.0129165649 | test_loss: 5.9853558540 | \n",
      "Epoch: 6297 | train_loss: 117.0127029419 | test_loss: 5.9853277206 | \n",
      "Epoch: 6298 | train_loss: 117.0125198364 | test_loss: 5.9853038788 | \n",
      "Epoch: 6299 | train_loss: 117.0122756958 | test_loss: 5.9852666855 | \n",
      "Epoch: 6300 | train_loss: 117.0120697021 | test_loss: 5.9852375984 | \n",
      "Epoch: 6301 | train_loss: 117.0118331909 | test_loss: 5.9852061272 | \n",
      "Epoch: 6302 | train_loss: 117.0116195679 | test_loss: 5.9851779938 | \n",
      "Epoch: 6303 | train_loss: 117.0114135742 | test_loss: 5.9851474762 | \n",
      "Epoch: 6304 | train_loss: 117.0112075806 | test_loss: 5.9851164818 | \n",
      "Epoch: 6305 | train_loss: 117.0109939575 | test_loss: 5.9850902557 | \n",
      "Epoch: 6306 | train_loss: 117.0108184814 | test_loss: 5.9850602150 | \n",
      "Epoch: 6307 | train_loss: 117.0106124878 | test_loss: 5.9850311279 | \n",
      "Epoch: 6308 | train_loss: 117.0103683472 | test_loss: 5.9850025177 | \n",
      "Epoch: 6309 | train_loss: 117.0101623535 | test_loss: 5.9849729538 | \n",
      "Epoch: 6310 | train_loss: 117.0099639893 | test_loss: 5.9849443436 | \n",
      "Epoch: 6311 | train_loss: 117.0097351074 | test_loss: 5.9849157333 | \n",
      "Epoch: 6312 | train_loss: 117.0095214844 | test_loss: 5.9848799706 | \n",
      "Epoch: 6313 | train_loss: 117.0093231201 | test_loss: 5.9848480225 | \n",
      "Epoch: 6314 | train_loss: 117.0091323853 | test_loss: 5.9848241806 | \n",
      "Epoch: 6315 | train_loss: 117.0088806152 | test_loss: 5.9847931862 | \n",
      "Epoch: 6316 | train_loss: 117.0086898804 | test_loss: 5.9847645760 | \n",
      "Epoch: 6317 | train_loss: 117.0084686279 | test_loss: 5.9847383499 | \n",
      "Epoch: 6318 | train_loss: 117.0082550049 | test_loss: 5.9847078323 | \n",
      "Epoch: 6319 | train_loss: 117.0080337524 | test_loss: 5.9846777916 | \n",
      "Epoch: 6320 | train_loss: 117.0078430176 | test_loss: 5.9846439362 | \n",
      "Epoch: 6321 | train_loss: 117.0076370239 | test_loss: 5.9846172333 | \n",
      "Epoch: 6322 | train_loss: 117.0074234009 | test_loss: 5.9845910072 | \n",
      "Epoch: 6323 | train_loss: 117.0072097778 | test_loss: 5.9845566750 | \n",
      "Epoch: 6324 | train_loss: 117.0070037842 | test_loss: 5.9845252037 | \n",
      "Epoch: 6325 | train_loss: 117.0068054199 | test_loss: 5.9844994545 | \n",
      "Epoch: 6326 | train_loss: 117.0065917969 | test_loss: 5.9844679832 | \n",
      "Epoch: 6327 | train_loss: 117.0063781738 | test_loss: 5.9844384193 | \n",
      "Epoch: 6328 | train_loss: 117.0061645508 | test_loss: 5.9844098091 | \n",
      "Epoch: 6329 | train_loss: 117.0059509277 | test_loss: 5.9843745232 | \n",
      "Epoch: 6330 | train_loss: 117.0057449341 | test_loss: 5.9843440056 | \n",
      "Epoch: 6331 | train_loss: 117.0054855347 | test_loss: 5.9843196869 | \n",
      "Epoch: 6332 | train_loss: 117.0053024292 | test_loss: 5.9842925072 | \n",
      "Epoch: 6333 | train_loss: 117.0050888062 | test_loss: 5.9842596054 | \n",
      "Epoch: 6334 | train_loss: 117.0048828125 | test_loss: 5.9842290878 | \n",
      "Epoch: 6335 | train_loss: 117.0046844482 | test_loss: 5.9842009544 | \n",
      "Epoch: 6336 | train_loss: 117.0045013428 | test_loss: 5.9841761589 | \n",
      "Epoch: 6337 | train_loss: 117.0042572021 | test_loss: 5.9841442108 | \n",
      "Epoch: 6338 | train_loss: 117.0040588379 | test_loss: 5.9841132164 | \n",
      "Epoch: 6339 | train_loss: 117.0038681030 | test_loss: 5.9840850830 | \n",
      "Epoch: 6340 | train_loss: 117.0036239624 | test_loss: 5.9840569496 | \n",
      "Epoch: 6341 | train_loss: 117.0034408569 | test_loss: 5.9840297699 | \n",
      "Epoch: 6342 | train_loss: 117.0032501221 | test_loss: 5.9839978218 | \n",
      "Epoch: 6343 | train_loss: 117.0029983521 | test_loss: 5.9839634895 | \n",
      "Epoch: 6344 | train_loss: 117.0027999878 | test_loss: 5.9839329720 | \n",
      "Epoch: 6345 | train_loss: 117.0026092529 | test_loss: 5.9838995934 | \n",
      "Epoch: 6346 | train_loss: 117.0024032593 | test_loss: 5.9838690758 | \n",
      "Epoch: 6347 | train_loss: 117.0021972656 | test_loss: 5.9838414192 | \n",
      "Epoch: 6348 | train_loss: 117.0019912720 | test_loss: 5.9838089943 | \n",
      "Epoch: 6349 | train_loss: 117.0017547607 | test_loss: 5.9837784767 | \n",
      "Epoch: 6350 | train_loss: 117.0015716553 | test_loss: 5.9837517738 | \n",
      "Epoch: 6351 | train_loss: 117.0013275146 | test_loss: 5.9837183952 | \n",
      "Epoch: 6352 | train_loss: 117.0011291504 | test_loss: 5.9836921692 | \n",
      "Epoch: 6353 | train_loss: 117.0009307861 | test_loss: 5.9836668968 | \n",
      "Epoch: 6354 | train_loss: 117.0007247925 | test_loss: 5.9836311340 | \n",
      "Epoch: 6355 | train_loss: 117.0004882812 | test_loss: 5.9836053848 | \n",
      "Epoch: 6356 | train_loss: 117.0002822876 | test_loss: 5.9835786819 | \n",
      "Epoch: 6357 | train_loss: 117.0000686646 | test_loss: 5.9835491180 | \n",
      "Epoch: 6358 | train_loss: 116.9998703003 | test_loss: 5.9835267067 | \n",
      "Epoch: 6359 | train_loss: 116.9996643066 | test_loss: 5.9834966660 | \n",
      "Epoch: 6360 | train_loss: 116.9994659424 | test_loss: 5.9834623337 | \n",
      "Epoch: 6361 | train_loss: 116.9992370605 | test_loss: 5.9834337234 | \n",
      "Epoch: 6362 | train_loss: 116.9990463257 | test_loss: 5.9834022522 | \n",
      "Epoch: 6363 | train_loss: 116.9988174438 | test_loss: 5.9833726883 | \n",
      "Epoch: 6364 | train_loss: 116.9986267090 | test_loss: 5.9833436012 | \n",
      "Epoch: 6365 | train_loss: 116.9984207153 | test_loss: 5.9833106995 | \n",
      "Epoch: 6366 | train_loss: 116.9981994629 | test_loss: 5.9832830429 | \n",
      "Epoch: 6367 | train_loss: 116.9980010986 | test_loss: 5.9832496643 | \n",
      "Epoch: 6368 | train_loss: 116.9977340698 | test_loss: 5.9832210541 | \n",
      "Epoch: 6369 | train_loss: 116.9974975586 | test_loss: 5.9831910133 | \n",
      "Epoch: 6370 | train_loss: 116.9972915649 | test_loss: 5.9831628799 | \n",
      "Epoch: 6371 | train_loss: 116.9971008301 | test_loss: 5.9831352234 | \n",
      "Epoch: 6372 | train_loss: 116.9969024658 | test_loss: 5.9831042290 | \n",
      "Epoch: 6373 | train_loss: 116.9966735840 | test_loss: 5.9830756187 | \n",
      "Epoch: 6374 | train_loss: 116.9964523315 | test_loss: 5.9830460548 | \n",
      "Epoch: 6375 | train_loss: 116.9962081909 | test_loss: 5.9830164909 | \n",
      "Epoch: 6376 | train_loss: 116.9960327148 | test_loss: 5.9829921722 | \n",
      "Epoch: 6377 | train_loss: 116.9958190918 | test_loss: 5.9829602242 | \n",
      "Epoch: 6378 | train_loss: 116.9956130981 | test_loss: 5.9829316139 | \n",
      "Epoch: 6379 | train_loss: 116.9953994751 | test_loss: 5.9829006195 | \n",
      "Epoch: 6380 | train_loss: 116.9952087402 | test_loss: 5.9828701019 | \n",
      "Epoch: 6381 | train_loss: 116.9949798584 | test_loss: 5.9828376770 | \n",
      "Epoch: 6382 | train_loss: 116.9947738647 | test_loss: 5.9828095436 | \n",
      "Epoch: 6383 | train_loss: 116.9945678711 | test_loss: 5.9827809334 | \n",
      "Epoch: 6384 | train_loss: 116.9943847656 | test_loss: 5.9827518463 | \n",
      "Epoch: 6385 | train_loss: 116.9941635132 | test_loss: 5.9827213287 | \n",
      "Epoch: 6386 | train_loss: 116.9939422607 | test_loss: 5.9826869965 | \n",
      "Epoch: 6387 | train_loss: 116.9937591553 | test_loss: 5.9826617241 | \n",
      "Epoch: 6388 | train_loss: 116.9935073853 | test_loss: 5.9826316833 | \n",
      "Epoch: 6389 | train_loss: 116.9933319092 | test_loss: 5.9826002121 | \n",
      "Epoch: 6390 | train_loss: 116.9930953979 | test_loss: 5.9825716019 | \n",
      "Epoch: 6391 | train_loss: 116.9929122925 | test_loss: 5.9825439453 | \n",
      "Epoch: 6392 | train_loss: 116.9927215576 | test_loss: 5.9825139046 | \n",
      "Epoch: 6393 | train_loss: 116.9925231934 | test_loss: 5.9824895859 | \n",
      "Epoch: 6394 | train_loss: 116.9923248291 | test_loss: 5.9824581146 | \n",
      "Epoch: 6395 | train_loss: 116.9921188354 | test_loss: 5.9824299812 | \n",
      "Epoch: 6396 | train_loss: 116.9918746948 | test_loss: 5.9824037552 | \n",
      "Epoch: 6397 | train_loss: 116.9916763306 | test_loss: 5.9823760986 | \n",
      "Epoch: 6398 | train_loss: 116.9914550781 | test_loss: 5.9823474884 | \n",
      "Epoch: 6399 | train_loss: 116.9912414551 | test_loss: 5.9823164940 | \n",
      "Epoch: 6400 | train_loss: 116.9910659790 | test_loss: 5.9822869301 | \n",
      "Epoch: 6401 | train_loss: 116.9908370972 | test_loss: 5.9822568893 | \n",
      "Epoch: 6402 | train_loss: 116.9906234741 | test_loss: 5.9822230339 | \n",
      "Epoch: 6403 | train_loss: 116.9904327393 | test_loss: 5.9821977615 | \n",
      "Epoch: 6404 | train_loss: 116.9902420044 | test_loss: 5.9821667671 | \n",
      "Epoch: 6405 | train_loss: 116.9900588989 | test_loss: 5.9821348190 | \n",
      "Epoch: 6406 | train_loss: 116.9898376465 | test_loss: 5.9821128845 | \n",
      "Epoch: 6407 | train_loss: 116.9896469116 | test_loss: 5.9820766449 | \n",
      "Epoch: 6408 | train_loss: 116.9894332886 | test_loss: 5.9820451736 | \n",
      "Epoch: 6409 | train_loss: 116.9891967773 | test_loss: 5.9820156097 | \n",
      "Epoch: 6410 | train_loss: 116.9889831543 | test_loss: 5.9819879532 | \n",
      "Epoch: 6411 | train_loss: 116.9887619019 | test_loss: 5.9819579124 | \n",
      "Epoch: 6412 | train_loss: 116.9885635376 | test_loss: 5.9819312096 | \n",
      "Epoch: 6413 | train_loss: 116.9883270264 | test_loss: 5.9818992615 | \n",
      "Epoch: 6414 | train_loss: 116.9881134033 | test_loss: 5.9818725586 | \n",
      "Epoch: 6415 | train_loss: 116.9879074097 | test_loss: 5.9818449020 | \n",
      "Epoch: 6416 | train_loss: 116.9876861572 | test_loss: 5.9818129539 | \n",
      "Epoch: 6417 | train_loss: 116.9874420166 | test_loss: 5.9817838669 | \n",
      "Epoch: 6418 | train_loss: 116.9872436523 | test_loss: 5.9817500114 | \n",
      "Epoch: 6419 | train_loss: 116.9870147705 | test_loss: 5.9817185402 | \n",
      "Epoch: 6420 | train_loss: 116.9868240356 | test_loss: 5.9816923141 | \n",
      "Epoch: 6421 | train_loss: 116.9866104126 | test_loss: 5.9816637039 | \n",
      "Epoch: 6422 | train_loss: 116.9864044189 | test_loss: 5.9816284180 | \n",
      "Epoch: 6423 | train_loss: 116.9861602783 | test_loss: 5.9815983772 | \n",
      "Epoch: 6424 | train_loss: 116.9859771729 | test_loss: 5.9815692902 | \n",
      "Epoch: 6425 | train_loss: 116.9857788086 | test_loss: 5.9815397263 | \n",
      "Epoch: 6426 | train_loss: 116.9855957031 | test_loss: 5.9815077782 | \n",
      "Epoch: 6427 | train_loss: 116.9853591919 | test_loss: 5.9814820290 | \n",
      "Epoch: 6428 | train_loss: 116.9851684570 | test_loss: 5.9814524651 | \n",
      "Epoch: 6429 | train_loss: 116.9849624634 | test_loss: 5.9814229012 | \n",
      "Epoch: 6430 | train_loss: 116.9847335815 | test_loss: 5.9813928604 | \n",
      "Epoch: 6431 | train_loss: 116.9845123291 | test_loss: 5.9813652039 | \n",
      "Epoch: 6432 | train_loss: 116.9842987061 | test_loss: 5.9813370705 | \n",
      "Epoch: 6433 | train_loss: 116.9840850830 | test_loss: 5.9813032150 | \n",
      "Epoch: 6434 | train_loss: 116.9838790894 | test_loss: 5.9812736511 | \n",
      "Epoch: 6435 | train_loss: 116.9836730957 | test_loss: 5.9812431335 | \n",
      "Epoch: 6436 | train_loss: 116.9834442139 | test_loss: 5.9812150002 | \n",
      "Epoch: 6437 | train_loss: 116.9832611084 | test_loss: 5.9811816216 | \n",
      "Epoch: 6438 | train_loss: 116.9830474854 | test_loss: 5.9811534882 | \n",
      "Epoch: 6439 | train_loss: 116.9828186035 | test_loss: 5.9811244011 | \n",
      "Epoch: 6440 | train_loss: 116.9826049805 | test_loss: 5.9810929298 | \n",
      "Epoch: 6441 | train_loss: 116.9823913574 | test_loss: 5.9810614586 | \n",
      "Epoch: 6442 | train_loss: 116.9821701050 | test_loss: 5.9810304642 | \n",
      "Epoch: 6443 | train_loss: 116.9819946289 | test_loss: 5.9810051918 | \n",
      "Epoch: 6444 | train_loss: 116.9817810059 | test_loss: 5.9809761047 | \n",
      "Epoch: 6445 | train_loss: 116.9815826416 | test_loss: 5.9809498787 | \n",
      "Epoch: 6446 | train_loss: 116.9813537598 | test_loss: 5.9809184074 | \n",
      "Epoch: 6447 | train_loss: 116.9811630249 | test_loss: 5.9808855057 | \n",
      "Epoch: 6448 | train_loss: 116.9809341431 | test_loss: 5.9808521271 | \n",
      "Epoch: 6449 | train_loss: 116.9807357788 | test_loss: 5.9808263779 | \n",
      "Epoch: 6450 | train_loss: 116.9804916382 | test_loss: 5.9807949066 | \n",
      "Epoch: 6451 | train_loss: 116.9803009033 | test_loss: 5.9807653427 | \n",
      "Epoch: 6452 | train_loss: 116.9801177979 | test_loss: 5.9807415009 | \n",
      "Epoch: 6453 | train_loss: 116.9798736572 | test_loss: 5.9807066917 | \n",
      "Epoch: 6454 | train_loss: 116.9796600342 | test_loss: 5.9806780815 | \n",
      "Epoch: 6455 | train_loss: 116.9794616699 | test_loss: 5.9806456566 | \n",
      "Epoch: 6456 | train_loss: 116.9792480469 | test_loss: 5.9806175232 | \n",
      "Epoch: 6457 | train_loss: 116.9790420532 | test_loss: 5.9805893898 | \n",
      "Epoch: 6458 | train_loss: 116.9788360596 | test_loss: 5.9805622101 | \n",
      "Epoch: 6459 | train_loss: 116.9786148071 | test_loss: 5.9805340767 | \n",
      "Epoch: 6460 | train_loss: 116.9784088135 | test_loss: 5.9805030823 | \n",
      "Epoch: 6461 | train_loss: 116.9781951904 | test_loss: 5.9804782867 | \n",
      "Epoch: 6462 | train_loss: 116.9779663086 | test_loss: 5.9804463387 | \n",
      "Epoch: 6463 | train_loss: 116.9777832031 | test_loss: 5.9804186821 | \n",
      "Epoch: 6464 | train_loss: 116.9776000977 | test_loss: 5.9803853035 | \n",
      "Epoch: 6465 | train_loss: 116.9774017334 | test_loss: 5.9803614616 | \n",
      "Epoch: 6466 | train_loss: 116.9771575928 | test_loss: 5.9803357124 | \n",
      "Epoch: 6467 | train_loss: 116.9769439697 | test_loss: 5.9803071022 | \n",
      "Epoch: 6468 | train_loss: 116.9767608643 | test_loss: 5.9802789688 | \n",
      "Epoch: 6469 | train_loss: 116.9765853882 | test_loss: 5.9802527428 | \n",
      "Epoch: 6470 | train_loss: 116.9763793945 | test_loss: 5.9802207947 | \n",
      "Epoch: 6471 | train_loss: 116.9761734009 | test_loss: 5.9801979065 | \n",
      "Epoch: 6472 | train_loss: 116.9759674072 | test_loss: 5.9801721573 | \n",
      "Epoch: 6473 | train_loss: 116.9757690430 | test_loss: 5.9801440239 | \n",
      "Epoch: 6474 | train_loss: 116.9755630493 | test_loss: 5.9801201820 | \n",
      "Epoch: 6475 | train_loss: 116.9753723145 | test_loss: 5.9800910950 | \n",
      "Epoch: 6476 | train_loss: 116.9751586914 | test_loss: 5.9800610542 | \n",
      "Epoch: 6477 | train_loss: 116.9749603271 | test_loss: 5.9800300598 | \n",
      "Epoch: 6478 | train_loss: 116.9747467041 | test_loss: 5.9800043106 | \n",
      "Epoch: 6479 | train_loss: 116.9745483398 | test_loss: 5.9799718857 | \n",
      "Epoch: 6480 | train_loss: 116.9743423462 | test_loss: 5.9799451828 | \n",
      "Epoch: 6481 | train_loss: 116.9740905762 | test_loss: 5.9799156189 | \n",
      "Epoch: 6482 | train_loss: 116.9739074707 | test_loss: 5.9798946381 | \n",
      "Epoch: 6483 | train_loss: 116.9737243652 | test_loss: 5.9798626900 | \n",
      "Epoch: 6484 | train_loss: 116.9735031128 | test_loss: 5.9798336029 | \n",
      "Epoch: 6485 | train_loss: 116.9733200073 | test_loss: 5.9798088074 | \n",
      "Epoch: 6486 | train_loss: 116.9731369019 | test_loss: 5.9797797203 | \n",
      "Epoch: 6487 | train_loss: 116.9729080200 | test_loss: 5.9797554016 | \n",
      "Epoch: 6488 | train_loss: 116.9726943970 | test_loss: 5.9797253609 | \n",
      "Epoch: 6489 | train_loss: 116.9724884033 | test_loss: 5.9796948433 | \n",
      "Epoch: 6490 | train_loss: 116.9722747803 | test_loss: 5.9796681404 | \n",
      "Epoch: 6491 | train_loss: 116.9720687866 | test_loss: 5.9796380997 | \n",
      "Epoch: 6492 | train_loss: 116.9718856812 | test_loss: 5.9796128273 | \n",
      "Epoch: 6493 | train_loss: 116.9716873169 | test_loss: 5.9795832634 | \n",
      "Epoch: 6494 | train_loss: 116.9714660645 | test_loss: 5.9795536995 | \n",
      "Epoch: 6495 | train_loss: 116.9712829590 | test_loss: 5.9795227051 | \n",
      "Epoch: 6496 | train_loss: 116.9710388184 | test_loss: 5.9794917107 | \n",
      "Epoch: 6497 | train_loss: 116.9708480835 | test_loss: 5.9794597626 | \n",
      "Epoch: 6498 | train_loss: 116.9706649780 | test_loss: 5.9794354439 | \n",
      "Epoch: 6499 | train_loss: 116.9704589844 | test_loss: 5.9794044495 | \n",
      "Epoch: 6500 | train_loss: 116.9702453613 | test_loss: 5.9793772697 | \n",
      "Epoch: 6501 | train_loss: 116.9700393677 | test_loss: 5.9793481827 | \n",
      "Epoch: 6502 | train_loss: 116.9698410034 | test_loss: 5.9793214798 | \n",
      "Epoch: 6503 | train_loss: 116.9696197510 | test_loss: 5.9792919159 | \n",
      "Epoch: 6504 | train_loss: 116.9694213867 | test_loss: 5.9792585373 | \n",
      "Epoch: 6505 | train_loss: 116.9692306519 | test_loss: 5.9792308807 | \n",
      "Epoch: 6506 | train_loss: 116.9689941406 | test_loss: 5.9792027473 | \n",
      "Epoch: 6507 | train_loss: 116.9687957764 | test_loss: 5.9791741371 | \n",
      "Epoch: 6508 | train_loss: 116.9685974121 | test_loss: 5.9791398048 | \n",
      "Epoch: 6509 | train_loss: 116.9683990479 | test_loss: 5.9791121483 | \n",
      "Epoch: 6510 | train_loss: 116.9681701660 | test_loss: 5.9790873528 | \n",
      "Epoch: 6511 | train_loss: 116.9679794312 | test_loss: 5.9790568352 | \n",
      "Epoch: 6512 | train_loss: 116.9677810669 | test_loss: 5.9790229797 | \n",
      "Epoch: 6513 | train_loss: 116.9675903320 | test_loss: 5.9789977074 | \n",
      "Epoch: 6514 | train_loss: 116.9673309326 | test_loss: 5.9789671898 | \n",
      "Epoch: 6515 | train_loss: 116.9671554565 | test_loss: 5.9789433479 | \n",
      "Epoch: 6516 | train_loss: 116.9670028687 | test_loss: 5.9789147377 | \n",
      "Epoch: 6517 | train_loss: 116.9667663574 | test_loss: 5.9788818359 | \n",
      "Epoch: 6518 | train_loss: 116.9665603638 | test_loss: 5.9788537025 | \n",
      "Epoch: 6519 | train_loss: 116.9663696289 | test_loss: 5.9788274765 | \n",
      "Epoch: 6520 | train_loss: 116.9661712646 | test_loss: 5.9788012505 | \n",
      "Epoch: 6521 | train_loss: 116.9659576416 | test_loss: 5.9787669182 | \n",
      "Epoch: 6522 | train_loss: 116.9657440186 | test_loss: 5.9787359238 | \n",
      "Epoch: 6523 | train_loss: 116.9655532837 | test_loss: 5.9787125587 | \n",
      "Epoch: 6524 | train_loss: 116.9653625488 | test_loss: 5.9786777496 | \n",
      "Epoch: 6525 | train_loss: 116.9651336670 | test_loss: 5.9786481857 | \n",
      "Epoch: 6526 | train_loss: 116.9649200439 | test_loss: 5.9786167145 | \n",
      "Epoch: 6527 | train_loss: 116.9647140503 | test_loss: 5.9785847664 | \n",
      "Epoch: 6528 | train_loss: 116.9645309448 | test_loss: 5.9785552025 | \n",
      "Epoch: 6529 | train_loss: 116.9643173218 | test_loss: 5.9785261154 | \n",
      "Epoch: 6530 | train_loss: 116.9641113281 | test_loss: 5.9784936905 | \n",
      "Epoch: 6531 | train_loss: 116.9638900757 | test_loss: 5.9784660339 | \n",
      "Epoch: 6532 | train_loss: 116.9636917114 | test_loss: 5.9784345627 | \n",
      "Epoch: 6533 | train_loss: 116.9634857178 | test_loss: 5.9784059525 | \n",
      "Epoch: 6534 | train_loss: 116.9632797241 | test_loss: 5.9783802032 | \n",
      "Epoch: 6535 | train_loss: 116.9630584717 | test_loss: 5.9783492088 | \n",
      "Epoch: 6536 | train_loss: 116.9628448486 | test_loss: 5.9783229828 | \n",
      "Epoch: 6537 | train_loss: 116.9626388550 | test_loss: 5.9782910347 | \n",
      "Epoch: 6538 | train_loss: 116.9624328613 | test_loss: 5.9782619476 | \n",
      "Epoch: 6539 | train_loss: 116.9622039795 | test_loss: 5.9782304764 | \n",
      "Epoch: 6540 | train_loss: 116.9620056152 | test_loss: 5.9782042503 | \n",
      "Epoch: 6541 | train_loss: 116.9617767334 | test_loss: 5.9781751633 | \n",
      "Epoch: 6542 | train_loss: 116.9615936279 | test_loss: 5.9781432152 | \n",
      "Epoch: 6543 | train_loss: 116.9613571167 | test_loss: 5.9781122208 | \n",
      "Epoch: 6544 | train_loss: 116.9611740112 | test_loss: 5.9780921936 | \n",
      "Epoch: 6545 | train_loss: 116.9609680176 | test_loss: 5.9780607224 | \n",
      "Epoch: 6546 | train_loss: 116.9607696533 | test_loss: 5.9780302048 | \n",
      "Epoch: 6547 | train_loss: 116.9605560303 | test_loss: 5.9779992104 | \n",
      "Epoch: 6548 | train_loss: 116.9603729248 | test_loss: 5.9779748917 | \n",
      "Epoch: 6549 | train_loss: 116.9601669312 | test_loss: 5.9779467583 | \n",
      "Epoch: 6550 | train_loss: 116.9599380493 | test_loss: 5.9779148102 | \n",
      "Epoch: 6551 | train_loss: 116.9597244263 | test_loss: 5.9778895378 | \n",
      "Epoch: 6552 | train_loss: 116.9595413208 | test_loss: 5.9778609276 | \n",
      "Epoch: 6553 | train_loss: 116.9593200684 | test_loss: 5.9778308868 | \n",
      "Epoch: 6554 | train_loss: 116.9591140747 | test_loss: 5.9778070450 | \n",
      "Epoch: 6555 | train_loss: 116.9588851929 | test_loss: 5.9777760506 | \n",
      "Epoch: 6556 | train_loss: 116.9586944580 | test_loss: 5.9777507782 | \n",
      "Epoch: 6557 | train_loss: 116.9585189819 | test_loss: 5.9777202606 | \n",
      "Epoch: 6558 | train_loss: 116.9583129883 | test_loss: 5.9776911736 | \n",
      "Epoch: 6559 | train_loss: 116.9581222534 | test_loss: 5.9776573181 | \n",
      "Epoch: 6560 | train_loss: 116.9579086304 | test_loss: 5.9776325226 | \n",
      "Epoch: 6561 | train_loss: 116.9576873779 | test_loss: 5.9776082039 | \n",
      "Epoch: 6562 | train_loss: 116.9574966431 | test_loss: 5.9775776863 | \n",
      "Epoch: 6563 | train_loss: 116.9573135376 | test_loss: 5.9775476456 | \n",
      "Epoch: 6564 | train_loss: 116.9571075439 | test_loss: 5.9775242805 | \n",
      "Epoch: 6565 | train_loss: 116.9568939209 | test_loss: 5.9774980545 | \n",
      "Epoch: 6566 | train_loss: 116.9567184448 | test_loss: 5.9774665833 | \n",
      "Epoch: 6567 | train_loss: 116.9564819336 | test_loss: 5.9774351120 | \n",
      "Epoch: 6568 | train_loss: 116.9562988281 | test_loss: 5.9774065018 | \n",
      "Epoch: 6569 | train_loss: 116.9561233521 | test_loss: 5.9773750305 | \n",
      "Epoch: 6570 | train_loss: 116.9558868408 | test_loss: 5.9773464203 | \n",
      "Epoch: 6571 | train_loss: 116.9556808472 | test_loss: 5.9773197174 | \n",
      "Epoch: 6572 | train_loss: 116.9554595947 | test_loss: 5.9772925377 | \n",
      "Epoch: 6573 | train_loss: 116.9552459717 | test_loss: 5.9772620201 | \n",
      "Epoch: 6574 | train_loss: 116.9550704956 | test_loss: 5.9772338867 | \n",
      "Epoch: 6575 | train_loss: 116.9548645020 | test_loss: 5.9772071838 | \n",
      "Epoch: 6576 | train_loss: 116.9546890259 | test_loss: 5.9771790504 | \n",
      "Epoch: 6577 | train_loss: 116.9544982910 | test_loss: 5.9771494865 | \n",
      "Epoch: 6578 | train_loss: 116.9543151855 | test_loss: 5.9771203995 | \n",
      "Epoch: 6579 | train_loss: 116.9540634155 | test_loss: 5.9770965576 | \n",
      "Epoch: 6580 | train_loss: 116.9539031982 | test_loss: 5.9770622253 | \n",
      "Epoch: 6581 | train_loss: 116.9536895752 | test_loss: 5.9770359993 | \n",
      "Epoch: 6582 | train_loss: 116.9534988403 | test_loss: 5.9770069122 | \n",
      "Epoch: 6583 | train_loss: 116.9533004761 | test_loss: 5.9769783020 | \n",
      "Epoch: 6584 | train_loss: 116.9530792236 | test_loss: 5.9769530296 | \n",
      "Epoch: 6585 | train_loss: 116.9529190063 | test_loss: 5.9769220352 | \n",
      "Epoch: 6586 | train_loss: 116.9526977539 | test_loss: 5.9768977165 | \n",
      "Epoch: 6587 | train_loss: 116.9524688721 | test_loss: 5.9768700600 | \n",
      "Epoch: 6588 | train_loss: 116.9523162842 | test_loss: 5.9768414497 | \n",
      "Epoch: 6589 | train_loss: 116.9520797729 | test_loss: 5.9768128395 | \n",
      "Epoch: 6590 | train_loss: 116.9519119263 | test_loss: 5.9767913818 | \n",
      "Epoch: 6591 | train_loss: 116.9516983032 | test_loss: 5.9767613411 | \n",
      "Epoch: 6592 | train_loss: 116.9515228271 | test_loss: 5.9767298698 | \n",
      "Epoch: 6593 | train_loss: 116.9513092041 | test_loss: 5.9767045975 | \n",
      "Epoch: 6594 | train_loss: 116.9511260986 | test_loss: 5.9766740799 | \n",
      "Epoch: 6595 | train_loss: 116.9509201050 | test_loss: 5.9766435623 | \n",
      "Epoch: 6596 | train_loss: 116.9507064819 | test_loss: 5.9766168594 | \n",
      "Epoch: 6597 | train_loss: 116.9505157471 | test_loss: 5.9765911102 | \n",
      "Epoch: 6598 | train_loss: 116.9502868652 | test_loss: 5.9765586853 | \n",
      "Epoch: 6599 | train_loss: 116.9500808716 | test_loss: 5.9765257835 | \n",
      "Epoch: 6600 | train_loss: 116.9499053955 | test_loss: 5.9764995575 | \n",
      "Epoch: 6601 | train_loss: 116.9497070312 | test_loss: 5.9764709473 | \n",
      "Epoch: 6602 | train_loss: 116.9495010376 | test_loss: 5.9764380455 | \n",
      "Epoch: 6603 | train_loss: 116.9492874146 | test_loss: 5.9764094353 | \n",
      "Epoch: 6604 | train_loss: 116.9491043091 | test_loss: 5.9763875008 | \n",
      "Epoch: 6605 | train_loss: 116.9488754272 | test_loss: 5.9763560295 | \n",
      "Epoch: 6606 | train_loss: 116.9486923218 | test_loss: 5.9763321877 | \n",
      "Epoch: 6607 | train_loss: 116.9485168457 | test_loss: 5.9763035774 | \n",
      "Epoch: 6608 | train_loss: 116.9482803345 | test_loss: 5.9762783051 | \n",
      "Epoch: 6609 | train_loss: 116.9481048584 | test_loss: 5.9762487411 | \n",
      "Epoch: 6610 | train_loss: 116.9478912354 | test_loss: 5.9762196541 | \n",
      "Epoch: 6611 | train_loss: 116.9476852417 | test_loss: 5.9761924744 | \n",
      "Epoch: 6612 | train_loss: 116.9475021362 | test_loss: 5.9761610031 | \n",
      "Epoch: 6613 | train_loss: 116.9472732544 | test_loss: 5.9761338234 | \n",
      "Epoch: 6614 | train_loss: 116.9471282959 | test_loss: 5.9761052132 | \n",
      "Epoch: 6615 | train_loss: 116.9469451904 | test_loss: 5.9760780334 | \n",
      "Epoch: 6616 | train_loss: 116.9467163086 | test_loss: 5.9760479927 | \n",
      "Epoch: 6617 | train_loss: 116.9465103149 | test_loss: 5.9760227203 | \n",
      "Epoch: 6618 | train_loss: 116.9462814331 | test_loss: 5.9759917259 | \n",
      "Epoch: 6619 | train_loss: 116.9460830688 | test_loss: 5.9759616852 | \n",
      "Epoch: 6620 | train_loss: 116.9458923340 | test_loss: 5.9759325981 | \n",
      "Epoch: 6621 | train_loss: 116.9456863403 | test_loss: 5.9759058952 | \n",
      "Epoch: 6622 | train_loss: 116.9455108643 | test_loss: 5.9758753777 | \n",
      "Epoch: 6623 | train_loss: 116.9452819824 | test_loss: 5.9758477211 | \n",
      "Epoch: 6624 | train_loss: 116.9450988770 | test_loss: 5.9758214951 | \n",
      "Epoch: 6625 | train_loss: 116.9449157715 | test_loss: 5.9757947922 | \n",
      "Epoch: 6626 | train_loss: 116.9446945190 | test_loss: 5.9757623672 | \n",
      "Epoch: 6627 | train_loss: 116.9444885254 | test_loss: 5.9757347107 | \n",
      "Epoch: 6628 | train_loss: 116.9442749023 | test_loss: 5.9757013321 | \n",
      "Epoch: 6629 | train_loss: 116.9440841675 | test_loss: 5.9756717682 | \n",
      "Epoch: 6630 | train_loss: 116.9438781738 | test_loss: 5.9756441116 | \n",
      "Epoch: 6631 | train_loss: 116.9436721802 | test_loss: 5.9756150246 | \n",
      "Epoch: 6632 | train_loss: 116.9434585571 | test_loss: 5.9755873680 | \n",
      "Epoch: 6633 | train_loss: 116.9432601929 | test_loss: 5.9755573273 | \n",
      "Epoch: 6634 | train_loss: 116.9430465698 | test_loss: 5.9755306244 | \n",
      "Epoch: 6635 | train_loss: 116.9428405762 | test_loss: 5.9755015373 | \n",
      "Epoch: 6636 | train_loss: 116.9426574707 | test_loss: 5.9754710197 | \n",
      "Epoch: 6637 | train_loss: 116.9424819946 | test_loss: 5.9754428864 | \n",
      "Epoch: 6638 | train_loss: 116.9422531128 | test_loss: 5.9754180908 | \n",
      "Epoch: 6639 | train_loss: 116.9420471191 | test_loss: 5.9753880501 | \n",
      "Epoch: 6640 | train_loss: 116.9418487549 | test_loss: 5.9753565788 | \n",
      "Epoch: 6641 | train_loss: 116.9416580200 | test_loss: 5.9753241539 | \n",
      "Epoch: 6642 | train_loss: 116.9414749146 | test_loss: 5.9752917290 | \n",
      "Epoch: 6643 | train_loss: 116.9412307739 | test_loss: 5.9752631187 | \n",
      "Epoch: 6644 | train_loss: 116.9410552979 | test_loss: 5.9752302170 | \n",
      "Epoch: 6645 | train_loss: 116.9408721924 | test_loss: 5.9752063751 | \n",
      "Epoch: 6646 | train_loss: 116.9406509399 | test_loss: 5.9751801491 | \n",
      "Epoch: 6647 | train_loss: 116.9404678345 | test_loss: 5.9751501083 | \n",
      "Epoch: 6648 | train_loss: 116.9402847290 | test_loss: 5.9751214981 | \n",
      "Epoch: 6649 | train_loss: 116.9400939941 | test_loss: 5.9750952721 | \n",
      "Epoch: 6650 | train_loss: 116.9398956299 | test_loss: 5.9750676155 | \n",
      "Epoch: 6651 | train_loss: 116.9396972656 | test_loss: 5.9750394821 | \n",
      "Epoch: 6652 | train_loss: 116.9394912720 | test_loss: 5.9750103951 | \n",
      "Epoch: 6653 | train_loss: 116.9393081665 | test_loss: 5.9749884605 | \n",
      "Epoch: 6654 | train_loss: 116.9390945435 | test_loss: 5.9749598503 | \n",
      "Epoch: 6655 | train_loss: 116.9389038086 | test_loss: 5.9749326706 | \n",
      "Epoch: 6656 | train_loss: 116.9387207031 | test_loss: 5.9749050140 | \n",
      "Epoch: 6657 | train_loss: 116.9385223389 | test_loss: 5.9748759270 | \n",
      "Epoch: 6658 | train_loss: 116.9383392334 | test_loss: 5.9748473167 | \n",
      "Epoch: 6659 | train_loss: 116.9381179810 | test_loss: 5.9748268127 | \n",
      "Epoch: 6660 | train_loss: 116.9379272461 | test_loss: 5.9747972488 | \n",
      "Epoch: 6661 | train_loss: 116.9377365112 | test_loss: 5.9747681618 | \n",
      "Epoch: 6662 | train_loss: 116.9375305176 | test_loss: 5.9747438431 | \n",
      "Epoch: 6663 | train_loss: 116.9373626709 | test_loss: 5.9747200012 | \n",
      "Epoch: 6664 | train_loss: 116.9371643066 | test_loss: 5.9746913910 | \n",
      "Epoch: 6665 | train_loss: 116.9369506836 | test_loss: 5.9746656418 | \n",
      "Epoch: 6666 | train_loss: 116.9367599487 | test_loss: 5.9746379852 | \n",
      "Epoch: 6667 | train_loss: 116.9365539551 | test_loss: 5.9746074677 | \n",
      "Epoch: 6668 | train_loss: 116.9363708496 | test_loss: 5.9745845795 | \n",
      "Epoch: 6669 | train_loss: 116.9361419678 | test_loss: 5.9745554924 | \n",
      "Epoch: 6670 | train_loss: 116.9359359741 | test_loss: 5.9745268822 | \n",
      "Epoch: 6671 | train_loss: 116.9357299805 | test_loss: 5.9744977951 | \n",
      "Epoch: 6672 | train_loss: 116.9355392456 | test_loss: 5.9744672775 | \n",
      "Epoch: 6673 | train_loss: 116.9353485107 | test_loss: 5.9744358063 | \n",
      "Epoch: 6674 | train_loss: 116.9351577759 | test_loss: 5.9744105339 | \n",
      "Epoch: 6675 | train_loss: 116.9349822998 | test_loss: 5.9743800163 | \n",
      "Epoch: 6676 | train_loss: 116.9347915649 | test_loss: 5.9743485451 | \n",
      "Epoch: 6677 | train_loss: 116.9346084595 | test_loss: 5.9743213654 | \n",
      "Epoch: 6678 | train_loss: 116.9344024658 | test_loss: 5.9742956161 | \n",
      "Epoch: 6679 | train_loss: 116.9341964722 | test_loss: 5.9742665291 | \n",
      "Epoch: 6680 | train_loss: 116.9340057373 | test_loss: 5.9742360115 | \n",
      "Epoch: 6681 | train_loss: 116.9337768555 | test_loss: 5.9742069244 | \n",
      "Epoch: 6682 | train_loss: 116.9335861206 | test_loss: 5.9741759300 | \n",
      "Epoch: 6683 | train_loss: 116.9333953857 | test_loss: 5.9741425514 | \n",
      "Epoch: 6684 | train_loss: 116.9332046509 | test_loss: 5.9741191864 | \n",
      "Epoch: 6685 | train_loss: 116.9330062866 | test_loss: 5.9740900993 | \n",
      "Epoch: 6686 | train_loss: 116.9328002930 | test_loss: 5.9740619659 | \n",
      "Epoch: 6687 | train_loss: 116.9326400757 | test_loss: 5.9740324020 | \n",
      "Epoch: 6688 | train_loss: 116.9324493408 | test_loss: 5.9740071297 | \n",
      "Epoch: 6689 | train_loss: 116.9322433472 | test_loss: 5.9739813805 | \n",
      "Epoch: 6690 | train_loss: 116.9320526123 | test_loss: 5.9739503860 | \n",
      "Epoch: 6691 | train_loss: 116.9318618774 | test_loss: 5.9739227295 | \n",
      "Epoch: 6692 | train_loss: 116.9316864014 | test_loss: 5.9738960266 | \n",
      "Epoch: 6693 | train_loss: 116.9314575195 | test_loss: 5.9738659859 | \n",
      "Epoch: 6694 | train_loss: 116.9312515259 | test_loss: 5.9738402367 | \n",
      "Epoch: 6695 | train_loss: 116.9310913086 | test_loss: 5.9738159180 | \n",
      "Epoch: 6696 | train_loss: 116.9308624268 | test_loss: 5.9737834930 | \n",
      "Epoch: 6697 | train_loss: 116.9306869507 | test_loss: 5.9737553596 | \n",
      "Epoch: 6698 | train_loss: 116.9304809570 | test_loss: 5.9737257957 | \n",
      "Epoch: 6699 | train_loss: 116.9302825928 | test_loss: 5.9736962318 | \n",
      "Epoch: 6700 | train_loss: 116.9300689697 | test_loss: 5.9736719131 | \n",
      "Epoch: 6701 | train_loss: 116.9298858643 | test_loss: 5.9736485481 | \n",
      "Epoch: 6702 | train_loss: 116.9296951294 | test_loss: 5.9736189842 | \n",
      "Epoch: 6703 | train_loss: 116.9295043945 | test_loss: 5.9735898972 | \n",
      "Epoch: 6704 | train_loss: 116.9293060303 | test_loss: 5.9735593796 | \n",
      "Epoch: 6705 | train_loss: 116.9291229248 | test_loss: 5.9735321999 | \n",
      "Epoch: 6706 | train_loss: 116.9289169312 | test_loss: 5.9735059738 | \n",
      "Epoch: 6707 | train_loss: 116.9287567139 | test_loss: 5.9734830856 | \n",
      "Epoch: 6708 | train_loss: 116.9285659790 | test_loss: 5.9734563828 | \n",
      "Epoch: 6709 | train_loss: 116.9283523560 | test_loss: 5.9734277725 | \n",
      "Epoch: 6710 | train_loss: 116.9281463623 | test_loss: 5.9733991623 | \n",
      "Epoch: 6711 | train_loss: 116.9279785156 | test_loss: 5.9733672142 | \n",
      "Epoch: 6712 | train_loss: 116.9277801514 | test_loss: 5.9733390808 | \n",
      "Epoch: 6713 | train_loss: 116.9275894165 | test_loss: 5.9733147621 | \n",
      "Epoch: 6714 | train_loss: 116.9273910522 | test_loss: 5.9732856750 | \n",
      "Epoch: 6715 | train_loss: 116.9272079468 | test_loss: 5.9732594490 | \n",
      "Epoch: 6716 | train_loss: 116.9270248413 | test_loss: 5.9732336998 | \n",
      "Epoch: 6717 | train_loss: 116.9267959595 | test_loss: 5.9732055664 | \n",
      "Epoch: 6718 | train_loss: 116.9266128540 | test_loss: 5.9731721878 | \n",
      "Epoch: 6719 | train_loss: 116.9264221191 | test_loss: 5.9731464386 | \n",
      "Epoch: 6720 | train_loss: 116.9262390137 | test_loss: 5.9731183052 | \n",
      "Epoch: 6721 | train_loss: 116.9260482788 | test_loss: 5.9730901718 | \n",
      "Epoch: 6722 | train_loss: 116.9258575439 | test_loss: 5.9730658531 | \n",
      "Epoch: 6723 | train_loss: 116.9256744385 | test_loss: 5.9730362892 | \n",
      "Epoch: 6724 | train_loss: 116.9254455566 | test_loss: 5.9730100632 | \n",
      "Epoch: 6725 | train_loss: 116.9252624512 | test_loss: 5.9729847908 | \n",
      "Epoch: 6726 | train_loss: 116.9250869751 | test_loss: 5.9729547501 | \n",
      "Epoch: 6727 | train_loss: 116.9248962402 | test_loss: 5.9729247093 | \n",
      "Epoch: 6728 | train_loss: 116.9246749878 | test_loss: 5.9728960991 | \n",
      "Epoch: 6729 | train_loss: 116.9245223999 | test_loss: 5.9728660583 | \n",
      "Epoch: 6730 | train_loss: 116.9243392944 | test_loss: 5.9728388786 | \n",
      "Epoch: 6731 | train_loss: 116.9241409302 | test_loss: 5.9728083611 | \n",
      "Epoch: 6732 | train_loss: 116.9239273071 | test_loss: 5.9727811813 | \n",
      "Epoch: 6733 | train_loss: 116.9237289429 | test_loss: 5.9727492332 | \n",
      "Epoch: 6734 | train_loss: 116.9235610962 | test_loss: 5.9727230072 | \n",
      "Epoch: 6735 | train_loss: 116.9233703613 | test_loss: 5.9726972580 | \n",
      "Epoch: 6736 | train_loss: 116.9231872559 | test_loss: 5.9726667404 | \n",
      "Epoch: 6737 | train_loss: 116.9230270386 | test_loss: 5.9726371765 | \n",
      "Epoch: 6738 | train_loss: 116.9228134155 | test_loss: 5.9726099968 | \n",
      "Epoch: 6739 | train_loss: 116.9226150513 | test_loss: 5.9725818634 | \n",
      "Epoch: 6740 | train_loss: 116.9223937988 | test_loss: 5.9725532532 | \n",
      "Epoch: 6741 | train_loss: 116.9221878052 | test_loss: 5.9725251198 | \n",
      "Epoch: 6742 | train_loss: 116.9220123291 | test_loss: 5.9724974632 | \n",
      "Epoch: 6743 | train_loss: 116.9218368530 | test_loss: 5.9724688530 | \n",
      "Epoch: 6744 | train_loss: 116.9216079712 | test_loss: 5.9724426270 | \n",
      "Epoch: 6745 | train_loss: 116.9214477539 | test_loss: 5.9724097252 | \n",
      "Epoch: 6746 | train_loss: 116.9212341309 | test_loss: 5.9723834991 | \n",
      "Epoch: 6747 | train_loss: 116.9210433960 | test_loss: 5.9723539352 | \n",
      "Epoch: 6748 | train_loss: 116.9208221436 | test_loss: 5.9723329544 | \n",
      "Epoch: 6749 | train_loss: 116.9206619263 | test_loss: 5.9723048210 | \n",
      "Epoch: 6750 | train_loss: 116.9205093384 | test_loss: 5.9722790718 | \n",
      "Epoch: 6751 | train_loss: 116.9203186035 | test_loss: 5.9722566605 | \n",
      "Epoch: 6752 | train_loss: 116.9201583862 | test_loss: 5.9722313881 | \n",
      "Epoch: 6753 | train_loss: 116.9199295044 | test_loss: 5.9722070694 | \n",
      "Epoch: 6754 | train_loss: 116.9197463989 | test_loss: 5.9721765518 | \n",
      "Epoch: 6755 | train_loss: 116.9195709229 | test_loss: 5.9721503258 | \n",
      "Epoch: 6756 | train_loss: 116.9193725586 | test_loss: 5.9721179008 | \n",
      "Epoch: 6757 | train_loss: 116.9191894531 | test_loss: 5.9720940590 | \n",
      "Epoch: 6758 | train_loss: 116.9190368652 | test_loss: 5.9720630646 | \n",
      "Epoch: 6759 | train_loss: 116.9188461304 | test_loss: 5.9720387459 | \n",
      "Epoch: 6760 | train_loss: 116.9186553955 | test_loss: 5.9720063210 | \n",
      "Epoch: 6761 | train_loss: 116.9184188843 | test_loss: 5.9719800949 | \n",
      "Epoch: 6762 | train_loss: 116.9182357788 | test_loss: 5.9719514847 | \n",
      "Epoch: 6763 | train_loss: 116.9180679321 | test_loss: 5.9719252586 | \n",
      "Epoch: 6764 | train_loss: 116.9178695679 | test_loss: 5.9718952179 | \n",
      "Epoch: 6765 | train_loss: 116.9177169800 | test_loss: 5.9718685150 | \n",
      "Epoch: 6766 | train_loss: 116.9175186157 | test_loss: 5.9718403816 | \n",
      "Epoch: 6767 | train_loss: 116.9173431396 | test_loss: 5.9718132019 | \n",
      "Epoch: 6768 | train_loss: 116.9171371460 | test_loss: 5.9717860222 | \n",
      "Epoch: 6769 | train_loss: 116.9169540405 | test_loss: 5.9717564583 | \n",
      "Epoch: 6770 | train_loss: 116.9167556763 | test_loss: 5.9717268944 | \n",
      "Epoch: 6771 | train_loss: 116.9165344238 | test_loss: 5.9716987610 | \n",
      "Epoch: 6772 | train_loss: 116.9163894653 | test_loss: 5.9716725349 | \n",
      "Epoch: 6773 | train_loss: 116.9161987305 | test_loss: 5.9716453552 | \n",
      "Epoch: 6774 | train_loss: 116.9160232544 | test_loss: 5.9716162682 | \n",
      "Epoch: 6775 | train_loss: 116.9158325195 | test_loss: 5.9715895653 | \n",
      "Epoch: 6776 | train_loss: 116.9156341553 | test_loss: 5.9715633392 | \n",
      "Epoch: 6777 | train_loss: 116.9154357910 | test_loss: 5.9715332985 | \n",
      "Epoch: 6778 | train_loss: 116.9152450562 | test_loss: 5.9715061188 | \n",
      "Epoch: 6779 | train_loss: 116.9150619507 | test_loss: 5.9714741707 | \n",
      "Epoch: 6780 | train_loss: 116.9148635864 | test_loss: 5.9714469910 | \n",
      "Epoch: 6781 | train_loss: 116.9146423340 | test_loss: 5.9714188576 | \n",
      "Epoch: 6782 | train_loss: 116.9144515991 | test_loss: 5.9713940620 | \n",
      "Epoch: 6783 | train_loss: 116.9142837524 | test_loss: 5.9713697433 | \n",
      "Epoch: 6784 | train_loss: 116.9141006470 | test_loss: 5.9713373184 | \n",
      "Epoch: 6785 | train_loss: 116.9138870239 | test_loss: 5.9713110924 | \n",
      "Epoch: 6786 | train_loss: 116.9137039185 | test_loss: 5.9712810516 | \n",
      "Epoch: 6787 | train_loss: 116.9134826660 | test_loss: 5.9712553024 | \n",
      "Epoch: 6788 | train_loss: 116.9133224487 | test_loss: 5.9712285995 | \n",
      "Epoch: 6789 | train_loss: 116.9131164551 | test_loss: 5.9711971283 | \n",
      "Epoch: 6790 | train_loss: 116.9129333496 | test_loss: 5.9711637497 | \n",
      "Epoch: 6791 | train_loss: 116.9127731323 | test_loss: 5.9711365700 | \n",
      "Epoch: 6792 | train_loss: 116.9125595093 | test_loss: 5.9711108208 | \n",
      "Epoch: 6793 | train_loss: 116.9123764038 | test_loss: 5.9710817337 | \n",
      "Epoch: 6794 | train_loss: 116.9121856689 | test_loss: 5.9710559845 | \n",
      "Epoch: 6795 | train_loss: 116.9120101929 | test_loss: 5.9710302353 | \n",
      "Epoch: 6796 | train_loss: 116.9118194580 | test_loss: 5.9709973335 | \n",
      "Epoch: 6797 | train_loss: 116.9116210938 | test_loss: 5.9709734917 | \n",
      "Epoch: 6798 | train_loss: 116.9113769531 | test_loss: 5.9709429741 | \n",
      "Epoch: 6799 | train_loss: 116.9112243652 | test_loss: 5.9709172249 | \n",
      "Epoch: 6800 | train_loss: 116.9110260010 | test_loss: 5.9708909988 | \n",
      "Epoch: 6801 | train_loss: 116.9108200073 | test_loss: 5.9708585739 | \n",
      "Epoch: 6802 | train_loss: 116.9106216431 | test_loss: 5.9708299637 | \n",
      "Epoch: 6803 | train_loss: 116.9104385376 | test_loss: 5.9708032608 | \n",
      "Epoch: 6804 | train_loss: 116.9102859497 | test_loss: 5.9707770348 | \n",
      "Epoch: 6805 | train_loss: 116.9100646973 | test_loss: 5.9707493782 | \n",
      "Epoch: 6806 | train_loss: 116.9098892212 | test_loss: 5.9707207680 | \n",
      "Epoch: 6807 | train_loss: 116.9097061157 | test_loss: 5.9706940651 | \n",
      "Epoch: 6808 | train_loss: 116.9095306396 | test_loss: 5.9706645012 | \n",
      "Epoch: 6809 | train_loss: 116.9093246460 | test_loss: 5.9706373215 | \n",
      "Epoch: 6810 | train_loss: 116.9091567993 | test_loss: 5.9706048965 | \n",
      "Epoch: 6811 | train_loss: 116.9089660645 | test_loss: 5.9705796242 | \n",
      "Epoch: 6812 | train_loss: 116.9087600708 | test_loss: 5.9705548286 | \n",
      "Epoch: 6813 | train_loss: 116.9085617065 | test_loss: 5.9705204964 | \n",
      "Epoch: 6814 | train_loss: 116.9084091187 | test_loss: 5.9704942703 | \n",
      "Epoch: 6815 | train_loss: 116.9082489014 | test_loss: 5.9704699516 | \n",
      "Epoch: 6816 | train_loss: 116.9080276489 | test_loss: 5.9704451561 | \n",
      "Epoch: 6817 | train_loss: 116.9078521729 | test_loss: 5.9704136848 | \n",
      "Epoch: 6818 | train_loss: 116.9076766968 | test_loss: 5.9703869820 | \n",
      "Epoch: 6819 | train_loss: 116.9074783325 | test_loss: 5.9703612328 | \n",
      "Epoch: 6820 | train_loss: 116.9072647095 | test_loss: 5.9703354836 | \n",
      "Epoch: 6821 | train_loss: 116.9071121216 | test_loss: 5.9703111649 | \n",
      "Epoch: 6822 | train_loss: 116.9069137573 | test_loss: 5.9702854156 | \n",
      "Epoch: 6823 | train_loss: 116.9067459106 | test_loss: 5.9702587128 | \n",
      "Epoch: 6824 | train_loss: 116.9065856934 | test_loss: 5.9702286720 | \n",
      "Epoch: 6825 | train_loss: 116.9063796997 | test_loss: 5.9702014923 | \n",
      "Epoch: 6826 | train_loss: 116.9061813354 | test_loss: 5.9701714516 | \n",
      "Epoch: 6827 | train_loss: 116.9059829712 | test_loss: 5.9701457024 | \n",
      "Epoch: 6828 | train_loss: 116.9057922363 | test_loss: 5.9701185226 | \n",
      "Epoch: 6829 | train_loss: 116.9056091309 | test_loss: 5.9700913429 | \n",
      "Epoch: 6830 | train_loss: 116.9054183960 | test_loss: 5.9700555801 | \n",
      "Epoch: 6831 | train_loss: 116.9052429199 | test_loss: 5.9700288773 | \n",
      "Epoch: 6832 | train_loss: 116.9050598145 | test_loss: 5.9700007439 | \n",
      "Epoch: 6833 | train_loss: 116.9048767090 | test_loss: 5.9699749947 | \n",
      "Epoch: 6834 | train_loss: 116.9047088623 | test_loss: 5.9699487686 | \n",
      "Epoch: 6835 | train_loss: 116.9044952393 | test_loss: 5.9699177742 | \n",
      "Epoch: 6836 | train_loss: 116.9043197632 | test_loss: 5.9698886871 | \n",
      "Epoch: 6837 | train_loss: 116.9041442871 | test_loss: 5.9698596001 | \n",
      "Epoch: 6838 | train_loss: 116.9039459229 | test_loss: 5.9698328972 | \n",
      "Epoch: 6839 | train_loss: 116.9037780762 | test_loss: 5.9698023796 | \n",
      "Epoch: 6840 | train_loss: 116.9035339355 | test_loss: 5.9697766304 | \n",
      "Epoch: 6841 | train_loss: 116.9033813477 | test_loss: 5.9697508812 | \n",
      "Epoch: 6842 | train_loss: 116.9031524658 | test_loss: 5.9697217941 | \n",
      "Epoch: 6843 | train_loss: 116.9029693604 | test_loss: 5.9696946144 | \n",
      "Epoch: 6844 | train_loss: 116.9027862549 | test_loss: 5.9696679115 | \n",
      "Epoch: 6845 | train_loss: 116.9025878906 | test_loss: 5.9696412086 | \n",
      "Epoch: 6846 | train_loss: 116.9024353027 | test_loss: 5.9696130753 | \n",
      "Epoch: 6847 | train_loss: 116.9022064209 | test_loss: 5.9695811272 | \n",
      "Epoch: 6848 | train_loss: 116.9020004272 | test_loss: 5.9695529938 | \n",
      "Epoch: 6849 | train_loss: 116.9018173218 | test_loss: 5.9695258141 | \n",
      "Epoch: 6850 | train_loss: 116.9016571045 | test_loss: 5.9694995880 | \n",
      "Epoch: 6851 | train_loss: 116.9014663696 | test_loss: 5.9694709778 | \n",
      "Epoch: 6852 | train_loss: 116.9013214111 | test_loss: 5.9694447517 | \n",
      "Epoch: 6853 | train_loss: 116.9011230469 | test_loss: 5.9694170952 | \n",
      "Epoch: 6854 | train_loss: 116.9009475708 | test_loss: 5.9693908691 | \n",
      "Epoch: 6855 | train_loss: 116.9007415771 | test_loss: 5.9693651199 | \n",
      "Epoch: 6856 | train_loss: 116.9005889893 | test_loss: 5.9693341255 | \n",
      "Epoch: 6857 | train_loss: 116.9003601074 | test_loss: 5.9693055153 | \n",
      "Epoch: 6858 | train_loss: 116.9001846313 | test_loss: 5.9692792892 | \n",
      "Epoch: 6859 | train_loss: 116.9000015259 | test_loss: 5.9692473412 | \n",
      "Epoch: 6860 | train_loss: 116.8998336792 | test_loss: 5.9692192078 | \n",
      "Epoch: 6861 | train_loss: 116.8996505737 | test_loss: 5.9691901207 | \n",
      "Epoch: 6862 | train_loss: 116.8994445801 | test_loss: 5.9691619873 | \n",
      "Epoch: 6863 | train_loss: 116.8992462158 | test_loss: 5.9691357613 | \n",
      "Epoch: 6864 | train_loss: 116.8990936279 | test_loss: 5.9691100121 | \n",
      "Epoch: 6865 | train_loss: 116.8989334106 | test_loss: 5.9690809250 | \n",
      "Epoch: 6866 | train_loss: 116.8987197876 | test_loss: 5.9690570831 | \n",
      "Epoch: 6867 | train_loss: 116.8985748291 | test_loss: 5.9690308571 | \n",
      "Epoch: 6868 | train_loss: 116.8984146118 | test_loss: 5.9690070152 | \n",
      "Epoch: 6869 | train_loss: 116.8982543945 | test_loss: 5.9689788818 | \n",
      "Epoch: 6870 | train_loss: 116.8980560303 | test_loss: 5.9689526558 | \n",
      "Epoch: 6871 | train_loss: 116.8978576660 | test_loss: 5.9689259529 | \n",
      "Epoch: 6872 | train_loss: 116.8976821899 | test_loss: 5.9688978195 | \n",
      "Epoch: 6873 | train_loss: 116.8975219727 | test_loss: 5.9688739777 | \n",
      "Epoch: 6874 | train_loss: 116.8973541260 | test_loss: 5.9688429832 | \n",
      "Epoch: 6875 | train_loss: 116.8971557617 | test_loss: 5.9688215256 | \n",
      "Epoch: 6876 | train_loss: 116.8969955444 | test_loss: 5.9687910080 | \n",
      "Epoch: 6877 | train_loss: 116.8967971802 | test_loss: 5.9687647820 | \n",
      "Epoch: 6878 | train_loss: 116.8965988159 | test_loss: 5.9687376022 | \n",
      "Epoch: 6879 | train_loss: 116.8964233398 | test_loss: 5.9687104225 | \n",
      "Epoch: 6880 | train_loss: 116.8962402344 | test_loss: 5.9686818123 | \n",
      "Epoch: 6881 | train_loss: 116.8960647583 | test_loss: 5.9686517715 | \n",
      "Epoch: 6882 | train_loss: 116.8958969116 | test_loss: 5.9686255455 | \n",
      "Epoch: 6883 | train_loss: 116.8956909180 | test_loss: 5.9685988426 | \n",
      "Epoch: 6884 | train_loss: 116.8955383301 | test_loss: 5.9685688019 | \n",
      "Epoch: 6885 | train_loss: 116.8953399658 | test_loss: 5.9685444832 | \n",
      "Epoch: 6886 | train_loss: 116.8951644897 | test_loss: 5.9685130119 | \n",
      "Epoch: 6887 | train_loss: 116.8949737549 | test_loss: 5.9684834480 | \n",
      "Epoch: 6888 | train_loss: 116.8948135376 | test_loss: 5.9684586525 | \n",
      "Epoch: 6889 | train_loss: 116.8946456909 | test_loss: 5.9684314728 | \n",
      "Epoch: 6890 | train_loss: 116.8944396973 | test_loss: 5.9684081078 | \n",
      "Epoch: 6891 | train_loss: 116.8942642212 | test_loss: 5.9683766365 | \n",
      "Epoch: 6892 | train_loss: 116.8940658569 | test_loss: 5.9683532715 | \n",
      "Epoch: 6893 | train_loss: 116.8938903809 | test_loss: 5.9683270454 | \n",
      "Epoch: 6894 | train_loss: 116.8937377930 | test_loss: 5.9682970047 | \n",
      "Epoch: 6895 | train_loss: 116.8935775757 | test_loss: 5.9682703018 | \n",
      "Epoch: 6896 | train_loss: 116.8934173584 | test_loss: 5.9682421684 | \n",
      "Epoch: 6897 | train_loss: 116.8932113647 | test_loss: 5.9682140350 | \n",
      "Epoch: 6898 | train_loss: 116.8930206299 | test_loss: 5.9681878090 | \n",
      "Epoch: 6899 | train_loss: 116.8928833008 | test_loss: 5.9681620598 | \n",
      "Epoch: 6900 | train_loss: 116.8926849365 | test_loss: 5.9681334496 | \n",
      "Epoch: 6901 | train_loss: 116.8925399780 | test_loss: 5.9681072235 | \n",
      "Epoch: 6902 | train_loss: 116.8923721313 | test_loss: 5.9680757523 | \n",
      "Epoch: 6903 | train_loss: 116.8922195435 | test_loss: 5.9680495262 | \n",
      "Epoch: 6904 | train_loss: 116.8920135498 | test_loss: 5.9680194855 | \n",
      "Epoch: 6905 | train_loss: 116.8918762207 | test_loss: 5.9679908752 | \n",
      "Epoch: 6906 | train_loss: 116.8916625977 | test_loss: 5.9679589272 | \n",
      "Epoch: 6907 | train_loss: 116.8914718628 | test_loss: 5.9679298401 | \n",
      "Epoch: 6908 | train_loss: 116.8912887573 | test_loss: 5.9679036140 | \n",
      "Epoch: 6909 | train_loss: 116.8911056519 | test_loss: 5.9678707123 | \n",
      "Epoch: 6910 | train_loss: 116.8909454346 | test_loss: 5.9678468704 | \n",
      "Epoch: 6911 | train_loss: 116.8907775879 | test_loss: 5.9678177834 | \n",
      "Epoch: 6912 | train_loss: 116.8906173706 | test_loss: 5.9677948952 | \n",
      "Epoch: 6913 | train_loss: 116.8904190063 | test_loss: 5.9677705765 | \n",
      "Epoch: 6914 | train_loss: 116.8902359009 | test_loss: 5.9677424431 | \n",
      "Epoch: 6915 | train_loss: 116.8900680542 | test_loss: 5.9677181244 | \n",
      "Epoch: 6916 | train_loss: 116.8898620605 | test_loss: 5.9676923752 | \n",
      "Epoch: 6917 | train_loss: 116.8896789551 | test_loss: 5.9676637650 | \n",
      "Epoch: 6918 | train_loss: 116.8895187378 | test_loss: 5.9676342010 | \n",
      "Epoch: 6919 | train_loss: 116.8893051147 | test_loss: 5.9676098824 | \n",
      "Epoch: 6920 | train_loss: 116.8891372681 | test_loss: 5.9675822258 | \n",
      "Epoch: 6921 | train_loss: 116.8889846802 | test_loss: 5.9675526619 | \n",
      "Epoch: 6922 | train_loss: 116.8888092041 | test_loss: 5.9675235748 | \n",
      "Epoch: 6923 | train_loss: 116.8886108398 | test_loss: 5.9674968719 | \n",
      "Epoch: 6924 | train_loss: 116.8884353638 | test_loss: 5.9674658775 | \n",
      "Epoch: 6925 | train_loss: 116.8882522583 | test_loss: 5.9674391747 | \n",
      "Epoch: 6926 | train_loss: 116.8880691528 | test_loss: 5.9674115181 | \n",
      "Epoch: 6927 | train_loss: 116.8878936768 | test_loss: 5.9673838615 | \n",
      "Epoch: 6928 | train_loss: 116.8877258301 | test_loss: 5.9673614502 | \n",
      "Epoch: 6929 | train_loss: 116.8875503540 | test_loss: 5.9673333168 | \n",
      "Epoch: 6930 | train_loss: 116.8873596191 | test_loss: 5.9673070908 | \n",
      "Epoch: 6931 | train_loss: 116.8871688843 | test_loss: 5.9672837257 | \n",
      "Epoch: 6932 | train_loss: 116.8870239258 | test_loss: 5.9672522545 | \n",
      "Epoch: 6933 | train_loss: 116.8868179321 | test_loss: 5.9672245979 | \n",
      "Epoch: 6934 | train_loss: 116.8866271973 | test_loss: 5.9671964645 | \n",
      "Epoch: 6935 | train_loss: 116.8864669800 | test_loss: 5.9671764374 | \n",
      "Epoch: 6936 | train_loss: 116.8862609863 | test_loss: 5.9671516418 | \n",
      "Epoch: 6937 | train_loss: 116.8860778809 | test_loss: 5.9671220779 | \n",
      "Epoch: 6938 | train_loss: 116.8859176636 | test_loss: 5.9670915604 | \n",
      "Epoch: 6939 | train_loss: 116.8856811523 | test_loss: 5.9670600891 | \n",
      "Epoch: 6940 | train_loss: 116.8855438232 | test_loss: 5.9670343399 | \n",
      "Epoch: 6941 | train_loss: 116.8853454590 | test_loss: 5.9670047760 | \n",
      "Epoch: 6942 | train_loss: 116.8851776123 | test_loss: 5.9669795036 | \n",
      "Epoch: 6943 | train_loss: 116.8850021362 | test_loss: 5.9669532776 | \n",
      "Epoch: 6944 | train_loss: 116.8848037720 | test_loss: 5.9669256210 | \n",
      "Epoch: 6945 | train_loss: 116.8846206665 | test_loss: 5.9668993950 | \n",
      "Epoch: 6946 | train_loss: 116.8844451904 | test_loss: 5.9668664932 | \n",
      "Epoch: 6947 | train_loss: 116.8842468262 | test_loss: 5.9668378830 | \n",
      "Epoch: 6948 | train_loss: 116.8840942383 | test_loss: 5.9668126106 | \n",
      "Epoch: 6949 | train_loss: 116.8838958740 | test_loss: 5.9667863846 | \n",
      "Epoch: 6950 | train_loss: 116.8837585449 | test_loss: 5.9667644501 | \n",
      "Epoch: 6951 | train_loss: 116.8835830688 | test_loss: 5.9667372704 | \n",
      "Epoch: 6952 | train_loss: 116.8833923340 | test_loss: 5.9667072296 | \n",
      "Epoch: 6953 | train_loss: 116.8831863403 | test_loss: 5.9666762352 | \n",
      "Epoch: 6954 | train_loss: 116.8830490112 | test_loss: 5.9666514397 | \n",
      "Epoch: 6955 | train_loss: 116.8828811646 | test_loss: 5.9666247368 | \n",
      "Epoch: 6956 | train_loss: 116.8827133179 | test_loss: 5.9665970802 | \n",
      "Epoch: 6957 | train_loss: 116.8825302124 | test_loss: 5.9665718079 | \n",
      "Epoch: 6958 | train_loss: 116.8823471069 | test_loss: 5.9665465355 | \n",
      "Epoch: 6959 | train_loss: 116.8821716309 | test_loss: 5.9665193558 | \n",
      "Epoch: 6960 | train_loss: 116.8819885254 | test_loss: 5.9664845467 | \n",
      "Epoch: 6961 | train_loss: 116.8817749023 | test_loss: 5.9664597511 | \n",
      "Epoch: 6962 | train_loss: 116.8815917969 | test_loss: 5.9664349556 | \n",
      "Epoch: 6963 | train_loss: 116.8814086914 | test_loss: 5.9664082527 | \n",
      "Epoch: 6964 | train_loss: 116.8812332153 | test_loss: 5.9663834572 | \n",
      "Epoch: 6965 | train_loss: 116.8810729980 | test_loss: 5.9663524628 | \n",
      "Epoch: 6966 | train_loss: 116.8808898926 | test_loss: 5.9663252831 | \n",
      "Epoch: 6967 | train_loss: 116.8807067871 | test_loss: 5.9662933350 | \n",
      "Epoch: 6968 | train_loss: 116.8805084229 | test_loss: 5.9662642479 | \n",
      "Epoch: 6969 | train_loss: 116.8803253174 | test_loss: 5.9662337303 | \n",
      "Epoch: 6970 | train_loss: 116.8801498413 | test_loss: 5.9662084579 | \n",
      "Epoch: 6971 | train_loss: 116.8799743652 | test_loss: 5.9661817551 | \n",
      "Epoch: 6972 | train_loss: 116.8798065186 | test_loss: 5.9661502838 | \n",
      "Epoch: 6973 | train_loss: 116.8796081543 | test_loss: 5.9661250114 | \n",
      "Epoch: 6974 | train_loss: 116.8794403076 | test_loss: 5.9660987854 | \n",
      "Epoch: 6975 | train_loss: 116.8792800903 | test_loss: 5.9660692215 | \n",
      "Epoch: 6976 | train_loss: 116.8790740967 | test_loss: 5.9660420418 | \n",
      "Epoch: 6977 | train_loss: 116.8789367676 | test_loss: 5.9660143852 | \n",
      "Epoch: 6978 | train_loss: 116.8787307739 | test_loss: 5.9659910202 | \n",
      "Epoch: 6979 | train_loss: 116.8785552979 | test_loss: 5.9659614563 | \n",
      "Epoch: 6980 | train_loss: 116.8783493042 | test_loss: 5.9659328461 | \n",
      "Epoch: 6981 | train_loss: 116.8781967163 | test_loss: 5.9659104347 | \n",
      "Epoch: 6982 | train_loss: 116.8780364990 | test_loss: 5.9658832550 | \n",
      "Epoch: 6983 | train_loss: 116.8778381348 | test_loss: 5.9658565521 | \n",
      "Epoch: 6984 | train_loss: 116.8776931763 | test_loss: 5.9658226967 | \n",
      "Epoch: 6985 | train_loss: 116.8774871826 | test_loss: 5.9657940865 | \n",
      "Epoch: 6986 | train_loss: 116.8773117065 | test_loss: 5.9657721519 | \n",
      "Epoch: 6987 | train_loss: 116.8771896362 | test_loss: 5.9657402039 | \n",
      "Epoch: 6988 | train_loss: 116.8769760132 | test_loss: 5.9657073021 | \n",
      "Epoch: 6989 | train_loss: 116.8768157959 | test_loss: 5.9656815529 | \n",
      "Epoch: 6990 | train_loss: 116.8766632080 | test_loss: 5.9656543732 | \n",
      "Epoch: 6991 | train_loss: 116.8764419556 | test_loss: 5.9656324387 | \n",
      "Epoch: 6992 | train_loss: 116.8762435913 | test_loss: 5.9656028748 | \n",
      "Epoch: 6993 | train_loss: 116.8760910034 | test_loss: 5.9655747414 | \n",
      "Epoch: 6994 | train_loss: 116.8759307861 | test_loss: 5.9655485153 | \n",
      "Epoch: 6995 | train_loss: 116.8757324219 | test_loss: 5.9655256271 | \n",
      "Epoch: 6996 | train_loss: 116.8755569458 | test_loss: 5.9654994011 | \n",
      "Epoch: 6997 | train_loss: 116.8753890991 | test_loss: 5.9654731750 | \n",
      "Epoch: 6998 | train_loss: 116.8752136230 | test_loss: 5.9654445648 | \n",
      "Epoch: 6999 | train_loss: 116.8750381470 | test_loss: 5.9654164314 | \n",
      "Epoch: 7000 | train_loss: 116.8748474121 | test_loss: 5.9653911591 | \n",
      "Epoch: 7001 | train_loss: 116.8747177124 | test_loss: 5.9653639793 | \n",
      "Epoch: 7002 | train_loss: 116.8745193481 | test_loss: 5.9653348923 | \n",
      "Epoch: 7003 | train_loss: 116.8743667603 | test_loss: 5.9653105736 | \n",
      "Epoch: 7004 | train_loss: 116.8741989136 | test_loss: 5.9652805328 | \n",
      "Epoch: 7005 | train_loss: 116.8739929199 | test_loss: 5.9652557373 | \n",
      "Epoch: 7006 | train_loss: 116.8738098145 | test_loss: 5.9652299881 | \n",
      "Epoch: 7007 | train_loss: 116.8736038208 | test_loss: 5.9652028084 | \n",
      "Epoch: 7008 | train_loss: 116.8734207153 | test_loss: 5.9651765823 | \n",
      "Epoch: 7009 | train_loss: 116.8732528687 | test_loss: 5.9651513100 | \n",
      "Epoch: 7010 | train_loss: 116.8730926514 | test_loss: 5.9651241302 | \n",
      "Epoch: 7011 | train_loss: 116.8729324341 | test_loss: 5.9650950432 | \n",
      "Epoch: 7012 | train_loss: 116.8727416992 | test_loss: 5.9650697708 | \n",
      "Epoch: 7013 | train_loss: 116.8725814819 | test_loss: 5.9650440216 | \n",
      "Epoch: 7014 | train_loss: 116.8723602295 | test_loss: 5.9650144577 | \n",
      "Epoch: 7015 | train_loss: 116.8722000122 | test_loss: 5.9649834633 | \n",
      "Epoch: 7016 | train_loss: 116.8720092773 | test_loss: 5.9649600983 | \n",
      "Epoch: 7017 | train_loss: 116.8718566895 | test_loss: 5.9649333954 | \n",
      "Epoch: 7018 | train_loss: 116.8716583252 | test_loss: 5.9649033546 | \n",
      "Epoch: 7019 | train_loss: 116.8714828491 | test_loss: 5.9648790359 | \n",
      "Epoch: 7020 | train_loss: 116.8713073730 | test_loss: 5.9648523331 | \n",
      "Epoch: 7021 | train_loss: 116.8711547852 | test_loss: 5.9648280144 | \n",
      "Epoch: 7022 | train_loss: 116.8709259033 | test_loss: 5.9647984505 | \n",
      "Epoch: 7023 | train_loss: 116.8707733154 | test_loss: 5.9647760391 | \n",
      "Epoch: 7024 | train_loss: 116.8706054688 | test_loss: 5.9647464752 | \n",
      "Epoch: 7025 | train_loss: 116.8704223633 | test_loss: 5.9647221565 | \n",
      "Epoch: 7026 | train_loss: 116.8702697754 | test_loss: 5.9646906853 | \n",
      "Epoch: 7027 | train_loss: 116.8700790405 | test_loss: 5.9646601677 | \n",
      "Epoch: 7028 | train_loss: 116.8698959351 | test_loss: 5.9646320343 | \n",
      "Epoch: 7029 | train_loss: 116.8697204590 | test_loss: 5.9646058083 | \n",
      "Epoch: 7030 | train_loss: 116.8695526123 | test_loss: 5.9645748138 | \n",
      "Epoch: 7031 | train_loss: 116.8693923950 | test_loss: 5.9645433426 | \n",
      "Epoch: 7032 | train_loss: 116.8692092896 | test_loss: 5.9645166397 | \n",
      "Epoch: 7033 | train_loss: 116.8690109253 | test_loss: 5.9644927979 | \n",
      "Epoch: 7034 | train_loss: 116.8688354492 | test_loss: 5.9644641876 | \n",
      "Epoch: 7035 | train_loss: 116.8686447144 | test_loss: 5.9644341469 | \n",
      "Epoch: 7036 | train_loss: 116.8685073853 | test_loss: 5.9644107819 | \n",
      "Epoch: 7037 | train_loss: 116.8683166504 | test_loss: 5.9643831253 | \n",
      "Epoch: 7038 | train_loss: 116.8681411743 | test_loss: 5.9643549919 | \n",
      "Epoch: 7039 | train_loss: 116.8679504395 | test_loss: 5.9643287659 | \n",
      "Epoch: 7040 | train_loss: 116.8677520752 | test_loss: 5.9642972946 | \n",
      "Epoch: 7041 | train_loss: 116.8675765991 | test_loss: 5.9642744064 | \n",
      "Epoch: 7042 | train_loss: 116.8674240112 | test_loss: 5.9642510414 | \n",
      "Epoch: 7043 | train_loss: 116.8672332764 | test_loss: 5.9642224312 | \n",
      "Epoch: 7044 | train_loss: 116.8670501709 | test_loss: 5.9641942978 | \n",
      "Epoch: 7045 | train_loss: 116.8668518066 | test_loss: 5.9641642570 | \n",
      "Epoch: 7046 | train_loss: 116.8666763306 | test_loss: 5.9641361237 | \n",
      "Epoch: 7047 | train_loss: 116.8665237427 | test_loss: 5.9641094208 | \n",
      "Epoch: 7048 | train_loss: 116.8663558960 | test_loss: 5.9640808105 | \n",
      "Epoch: 7049 | train_loss: 116.8661804199 | test_loss: 5.9640507698 | \n",
      "Epoch: 7050 | train_loss: 116.8659744263 | test_loss: 5.9640259743 | \n",
      "Epoch: 7051 | train_loss: 116.8658142090 | test_loss: 5.9640016556 | \n",
      "Epoch: 7052 | train_loss: 116.8656082153 | test_loss: 5.9639673233 | \n",
      "Epoch: 7053 | train_loss: 116.8654403687 | test_loss: 5.9639415741 | \n",
      "Epoch: 7054 | train_loss: 116.8652954102 | test_loss: 5.9639210701 | \n",
      "Epoch: 7055 | train_loss: 116.8651199341 | test_loss: 5.9638948441 | \n",
      "Epoch: 7056 | train_loss: 116.8649597168 | test_loss: 5.9638686180 | \n",
      "Epoch: 7057 | train_loss: 116.8647918701 | test_loss: 5.9638414383 | \n",
      "Epoch: 7058 | train_loss: 116.8646087646 | test_loss: 5.9638066292 | \n",
      "Epoch: 7059 | train_loss: 116.8644027710 | test_loss: 5.9637784958 | \n",
      "Epoch: 7060 | train_loss: 116.8642807007 | test_loss: 5.9637556076 | \n",
      "Epoch: 7061 | train_loss: 116.8641128540 | test_loss: 5.9637274742 | \n",
      "Epoch: 7062 | train_loss: 116.8639221191 | test_loss: 5.9637012482 | \n",
      "Epoch: 7063 | train_loss: 116.8637695312 | test_loss: 5.9636712074 | \n",
      "Epoch: 7064 | train_loss: 116.8635864258 | test_loss: 5.9636425972 | \n",
      "Epoch: 7065 | train_loss: 116.8633880615 | test_loss: 5.9636182785 | \n",
      "Epoch: 7066 | train_loss: 116.8632049561 | test_loss: 5.9635887146 | \n",
      "Epoch: 7067 | train_loss: 116.8630447388 | test_loss: 5.9635624886 | \n",
      "Epoch: 7068 | train_loss: 116.8629074097 | test_loss: 5.9635357857 | \n",
      "Epoch: 7069 | train_loss: 116.8626861572 | test_loss: 5.9635095596 | \n",
      "Epoch: 7070 | train_loss: 116.8625411987 | test_loss: 5.9634838104 | \n",
      "Epoch: 7071 | train_loss: 116.8623504639 | test_loss: 5.9634556770 | \n",
      "Epoch: 7072 | train_loss: 116.8621597290 | test_loss: 5.9634318352 | \n",
      "Epoch: 7073 | train_loss: 116.8619918823 | test_loss: 5.9634075165 | \n",
      "Epoch: 7074 | train_loss: 116.8618240356 | test_loss: 5.9633812904 | \n",
      "Epoch: 7075 | train_loss: 116.8616409302 | test_loss: 5.9633522034 | \n",
      "Epoch: 7076 | train_loss: 116.8614578247 | test_loss: 5.9633269310 | \n",
      "Epoch: 7077 | train_loss: 116.8612976074 | test_loss: 5.9633011818 | \n",
      "Epoch: 7078 | train_loss: 116.8611068726 | test_loss: 5.9632720947 | \n",
      "Epoch: 7079 | train_loss: 116.8609390259 | test_loss: 5.9632477760 | \n",
      "Epoch: 7080 | train_loss: 116.8608093262 | test_loss: 5.9632215500 | \n",
      "Epoch: 7081 | train_loss: 116.8606262207 | test_loss: 5.9631900787 | \n",
      "Epoch: 7082 | train_loss: 116.8604431152 | test_loss: 5.9631652832 | \n",
      "Epoch: 7083 | train_loss: 116.8602828979 | test_loss: 5.9631328583 | \n",
      "Epoch: 7084 | train_loss: 116.8601150513 | test_loss: 5.9631042480 | \n",
      "Epoch: 7085 | train_loss: 116.8599243164 | test_loss: 5.9630784988 | \n",
      "Epoch: 7086 | train_loss: 116.8597869873 | test_loss: 5.9630537033 | \n",
      "Epoch: 7087 | train_loss: 116.8595886230 | test_loss: 5.9630255699 | \n",
      "Epoch: 7088 | train_loss: 116.8593978882 | test_loss: 5.9629998207 | \n",
      "Epoch: 7089 | train_loss: 116.8592071533 | test_loss: 5.9629750252 | \n",
      "Epoch: 7090 | train_loss: 116.8590393066 | test_loss: 5.9629435539 | \n",
      "Epoch: 7091 | train_loss: 116.8588943481 | test_loss: 5.9629206657 | \n",
      "Epoch: 7092 | train_loss: 116.8587036133 | test_loss: 5.9628920555 | \n",
      "Epoch: 7093 | train_loss: 116.8585357666 | test_loss: 5.9628643990 | \n",
      "Epoch: 7094 | train_loss: 116.8583908081 | test_loss: 5.9628357887 | \n",
      "Epoch: 7095 | train_loss: 116.8582000732 | test_loss: 5.9628081322 | \n",
      "Epoch: 7096 | train_loss: 116.8580169678 | test_loss: 5.9627799988 | \n",
      "Epoch: 7097 | train_loss: 116.8578338623 | test_loss: 5.9627547264 | \n",
      "Epoch: 7098 | train_loss: 116.8576965332 | test_loss: 5.9627208710 | \n",
      "Epoch: 7099 | train_loss: 116.8574600220 | test_loss: 5.9626970291 | \n",
      "Epoch: 7100 | train_loss: 116.8572921753 | test_loss: 5.9626708031 | \n",
      "Epoch: 7101 | train_loss: 116.8571319580 | test_loss: 5.9626426697 | \n",
      "Epoch: 7102 | train_loss: 116.8569412231 | test_loss: 5.9626116753 | \n",
      "Epoch: 7103 | train_loss: 116.8567581177 | test_loss: 5.9625830650 | \n",
      "Epoch: 7104 | train_loss: 116.8565750122 | test_loss: 5.9625577927 | \n",
      "Epoch: 7105 | train_loss: 116.8563842773 | test_loss: 5.9625248909 | \n",
      "Epoch: 7106 | train_loss: 116.8562469482 | test_loss: 5.9625000954 | \n",
      "Epoch: 7107 | train_loss: 116.8560638428 | test_loss: 5.9624762535 | \n",
      "Epoch: 7108 | train_loss: 116.8558807373 | test_loss: 5.9624466896 | \n",
      "Epoch: 7109 | train_loss: 116.8557205200 | test_loss: 5.9624199867 | \n",
      "Epoch: 7110 | train_loss: 116.8555450439 | test_loss: 5.9623932838 | \n",
      "Epoch: 7111 | train_loss: 116.8553771973 | test_loss: 5.9623661041 | \n",
      "Epoch: 7112 | train_loss: 116.8551864624 | test_loss: 5.9623417854 | \n",
      "Epoch: 7113 | train_loss: 116.8550186157 | test_loss: 5.9623165131 | \n",
      "Epoch: 7114 | train_loss: 116.8548660278 | test_loss: 5.9622883797 | \n",
      "Epoch: 7115 | train_loss: 116.8547058105 | test_loss: 5.9622602463 | \n",
      "Epoch: 7116 | train_loss: 116.8545150757 | test_loss: 5.9622364044 | \n",
      "Epoch: 7117 | train_loss: 116.8543243408 | test_loss: 5.9622135162 | \n",
      "Epoch: 7118 | train_loss: 116.8541717529 | test_loss: 5.9621882439 | \n",
      "Epoch: 7119 | train_loss: 116.8539733887 | test_loss: 5.9621567726 | \n",
      "Epoch: 7120 | train_loss: 116.8538360596 | test_loss: 5.9621319771 | \n",
      "Epoch: 7121 | train_loss: 116.8536453247 | test_loss: 5.9621062279 | \n",
      "Epoch: 7122 | train_loss: 116.8534851074 | test_loss: 5.9620814323 | \n",
      "Epoch: 7123 | train_loss: 116.8532943726 | test_loss: 5.9620513916 | \n",
      "Epoch: 7124 | train_loss: 116.8531570435 | test_loss: 5.9620280266 | \n",
      "Epoch: 7125 | train_loss: 116.8529891968 | test_loss: 5.9619998932 | \n",
      "Epoch: 7126 | train_loss: 116.8528213501 | test_loss: 5.9619750977 | \n",
      "Epoch: 7127 | train_loss: 116.8526458740 | test_loss: 5.9619441032 | \n",
      "Epoch: 7128 | train_loss: 116.8524703979 | test_loss: 5.9619183540 | \n",
      "Epoch: 7129 | train_loss: 116.8522872925 | test_loss: 5.9618921280 | \n",
      "Epoch: 7130 | train_loss: 116.8521270752 | test_loss: 5.9618635178 | \n",
      "Epoch: 7131 | train_loss: 116.8519439697 | test_loss: 5.9618377686 | \n",
      "Epoch: 7132 | train_loss: 116.8517456055 | test_loss: 5.9618139267 | \n",
      "Epoch: 7133 | train_loss: 116.8515853882 | test_loss: 5.9617829323 | \n",
      "Epoch: 7134 | train_loss: 116.8514251709 | test_loss: 5.9617576599 | \n",
      "Epoch: 7135 | train_loss: 116.8512268066 | test_loss: 5.9617295265 | \n",
      "Epoch: 7136 | train_loss: 116.8510360718 | test_loss: 5.9617018700 | \n",
      "Epoch: 7137 | train_loss: 116.8508758545 | test_loss: 5.9616713524 | \n",
      "Epoch: 7138 | train_loss: 116.8506774902 | test_loss: 5.9616451263 | \n",
      "Epoch: 7139 | train_loss: 116.8505096436 | test_loss: 5.9616179466 | \n",
      "Epoch: 7140 | train_loss: 116.8503036499 | test_loss: 5.9615950584 | \n",
      "Epoch: 7141 | train_loss: 116.8501281738 | test_loss: 5.9615654945 | \n",
      "Epoch: 7142 | train_loss: 116.8499603271 | test_loss: 5.9615392685 | \n",
      "Epoch: 7143 | train_loss: 116.8498077393 | test_loss: 5.9615125656 | \n",
      "Epoch: 7144 | train_loss: 116.8496704102 | test_loss: 5.9614887238 | \n",
      "Epoch: 7145 | train_loss: 116.8494415283 | test_loss: 5.9614586830 | \n",
      "Epoch: 7146 | train_loss: 116.8492736816 | test_loss: 5.9614319801 | \n",
      "Epoch: 7147 | train_loss: 116.8490905762 | test_loss: 5.9614038467 | \n",
      "Epoch: 7148 | train_loss: 116.8489379883 | test_loss: 5.9613785744 | \n",
      "Epoch: 7149 | train_loss: 116.8487396240 | test_loss: 5.9613509178 | \n",
      "Epoch: 7150 | train_loss: 116.8485641479 | test_loss: 5.9613261223 | \n",
      "Epoch: 7151 | train_loss: 116.8484115601 | test_loss: 5.9612927437 | \n",
      "Epoch: 7152 | train_loss: 116.8482589722 | test_loss: 5.9612650871 | \n",
      "Epoch: 7153 | train_loss: 116.8481063843 | test_loss: 5.9612340927 | \n",
      "Epoch: 7154 | train_loss: 116.8479003906 | test_loss: 5.9612126350 | \n",
      "Epoch: 7155 | train_loss: 116.8477706909 | test_loss: 5.9611907005 | \n",
      "Epoch: 7156 | train_loss: 116.8475799561 | test_loss: 5.9611620903 | \n",
      "Epoch: 7157 | train_loss: 116.8474426270 | test_loss: 5.9611372948 | \n",
      "Epoch: 7158 | train_loss: 116.8472595215 | test_loss: 5.9611072540 | \n",
      "Epoch: 7159 | train_loss: 116.8470611572 | test_loss: 5.9610810280 | \n",
      "Epoch: 7160 | train_loss: 116.8468780518 | test_loss: 5.9610567093 | \n",
      "Epoch: 7161 | train_loss: 116.8467178345 | test_loss: 5.9610266685 | \n",
      "Epoch: 7162 | train_loss: 116.8465423584 | test_loss: 5.9610023499 | \n",
      "Epoch: 7163 | train_loss: 116.8463897705 | test_loss: 5.9609746933 | \n",
      "Epoch: 7164 | train_loss: 116.8462295532 | test_loss: 5.9609460831 | \n",
      "Epoch: 7165 | train_loss: 116.8460464478 | test_loss: 5.9609179497 | \n",
      "Epoch: 7166 | train_loss: 116.8458709717 | test_loss: 5.9608960152 | \n",
      "Epoch: 7167 | train_loss: 116.8457031250 | test_loss: 5.9608683586 | \n",
      "Epoch: 7168 | train_loss: 116.8455047607 | test_loss: 5.9608454704 | \n",
      "Epoch: 7169 | train_loss: 116.8453521729 | test_loss: 5.9608216286 | \n",
      "Epoch: 7170 | train_loss: 116.8451919556 | test_loss: 5.9607963562 | \n",
      "Epoch: 7171 | train_loss: 116.8450469971 | test_loss: 5.9607734680 | \n",
      "Epoch: 7172 | train_loss: 116.8448638916 | test_loss: 5.9607453346 | \n",
      "Epoch: 7173 | train_loss: 116.8446884155 | test_loss: 5.9607172012 | \n",
      "Epoch: 7174 | train_loss: 116.8445205688 | test_loss: 5.9606928825 | \n",
      "Epoch: 7175 | train_loss: 116.8443527222 | test_loss: 5.9606676102 | \n",
      "Epoch: 7176 | train_loss: 116.8441772461 | test_loss: 5.9606413841 | \n",
      "Epoch: 7177 | train_loss: 116.8440017700 | test_loss: 5.9606151581 | \n",
      "Epoch: 7178 | train_loss: 116.8438415527 | test_loss: 5.9605879784 | \n",
      "Epoch: 7179 | train_loss: 116.8436584473 | test_loss: 5.9605622292 | \n",
      "Epoch: 7180 | train_loss: 116.8434753418 | test_loss: 5.9605393410 | \n",
      "Epoch: 7181 | train_loss: 116.8433380127 | test_loss: 5.9605088234 | \n",
      "Epoch: 7182 | train_loss: 116.8431625366 | test_loss: 5.9604825974 | \n",
      "Epoch: 7183 | train_loss: 116.8430099487 | test_loss: 5.9604554176 | \n",
      "Epoch: 7184 | train_loss: 116.8428344727 | test_loss: 5.9604301453 | \n",
      "Epoch: 7185 | train_loss: 116.8426437378 | test_loss: 5.9604058266 | \n",
      "Epoch: 7186 | train_loss: 116.8425064087 | test_loss: 5.9603757858 | \n",
      "Epoch: 7187 | train_loss: 116.8423309326 | test_loss: 5.9603471756 | \n",
      "Epoch: 7188 | train_loss: 116.8421401978 | test_loss: 5.9603204727 | \n",
      "Epoch: 7189 | train_loss: 116.8419647217 | test_loss: 5.9602904320 | \n",
      "Epoch: 7190 | train_loss: 116.8418045044 | test_loss: 5.9602642059 | \n",
      "Epoch: 7191 | train_loss: 116.8416137695 | test_loss: 5.9602379799 | \n",
      "Epoch: 7192 | train_loss: 116.8414382935 | test_loss: 5.9602088928 | \n",
      "Epoch: 7193 | train_loss: 116.8412857056 | test_loss: 5.9601793289 | \n",
      "Epoch: 7194 | train_loss: 116.8410949707 | test_loss: 5.9601564407 | \n",
      "Epoch: 7195 | train_loss: 116.8409271240 | test_loss: 5.9601354599 | \n",
      "Epoch: 7196 | train_loss: 116.8407592773 | test_loss: 5.9601092339 | \n",
      "Epoch: 7197 | train_loss: 116.8406143188 | test_loss: 5.9600815773 | \n",
      "Epoch: 7198 | train_loss: 116.8404312134 | test_loss: 5.9600586891 | \n",
      "Epoch: 7199 | train_loss: 116.8402862549 | test_loss: 5.9600291252 | \n",
      "Epoch: 7200 | train_loss: 116.8400878906 | test_loss: 5.9600024223 | \n",
      "Epoch: 7201 | train_loss: 116.8399047852 | test_loss: 5.9599733353 | \n",
      "Epoch: 7202 | train_loss: 116.8397521973 | test_loss: 5.9599471092 | \n",
      "Epoch: 7203 | train_loss: 116.8395690918 | test_loss: 5.9599218369 | \n",
      "Epoch: 7204 | train_loss: 116.8394012451 | test_loss: 5.9598937035 | \n",
      "Epoch: 7205 | train_loss: 116.8392562866 | test_loss: 5.9598655701 | \n",
      "Epoch: 7206 | train_loss: 116.8390731812 | test_loss: 5.9598369598 | \n",
      "Epoch: 7207 | train_loss: 116.8388977051 | test_loss: 5.9598126411 | \n",
      "Epoch: 7208 | train_loss: 116.8387374878 | test_loss: 5.9597883224 | \n",
      "Epoch: 7209 | train_loss: 116.8385543823 | test_loss: 5.9597563744 | \n",
      "Epoch: 7210 | train_loss: 116.8383789062 | test_loss: 5.9597320557 | \n",
      "Epoch: 7211 | train_loss: 116.8381958008 | test_loss: 5.9597053528 | \n",
      "Epoch: 7212 | train_loss: 116.8380203247 | test_loss: 5.9596767426 | \n",
      "Epoch: 7213 | train_loss: 116.8378448486 | test_loss: 5.9596529007 | \n",
      "Epoch: 7214 | train_loss: 116.8377075195 | test_loss: 5.9596276283 | \n",
      "Epoch: 7215 | train_loss: 116.8375091553 | test_loss: 5.9596018791 | \n",
      "Epoch: 7216 | train_loss: 116.8373413086 | test_loss: 5.9595756531 | \n",
      "Epoch: 7217 | train_loss: 116.8371963501 | test_loss: 5.9595546722 | \n",
      "Epoch: 7218 | train_loss: 116.8370437622 | test_loss: 5.9595246315 | \n",
      "Epoch: 7219 | train_loss: 116.8368530273 | test_loss: 5.9594984055 | \n",
      "Epoch: 7220 | train_loss: 116.8366928101 | test_loss: 5.9594707489 | \n",
      "Epoch: 7221 | train_loss: 116.8365173340 | test_loss: 5.9594445229 | \n",
      "Epoch: 7222 | train_loss: 116.8363494873 | test_loss: 5.9594202042 | \n",
      "Epoch: 7223 | train_loss: 116.8361587524 | test_loss: 5.9593958855 | \n",
      "Epoch: 7224 | train_loss: 116.8359985352 | test_loss: 5.9593677521 | \n",
      "Epoch: 7225 | train_loss: 116.8358612061 | test_loss: 5.9593372345 | \n",
      "Epoch: 7226 | train_loss: 116.8356552124 | test_loss: 5.9593143463 | \n",
      "Epoch: 7227 | train_loss: 116.8354873657 | test_loss: 5.9592895508 | \n",
      "Epoch: 7228 | train_loss: 116.8353347778 | test_loss: 5.9592661858 | \n",
      "Epoch: 7229 | train_loss: 116.8351593018 | test_loss: 5.9592342377 | \n",
      "Epoch: 7230 | train_loss: 116.8349685669 | test_loss: 5.9592061043 | \n",
      "Epoch: 7231 | train_loss: 116.8348083496 | test_loss: 5.9591784477 | \n",
      "Epoch: 7232 | train_loss: 116.8346405029 | test_loss: 5.9591526985 | \n",
      "Epoch: 7233 | train_loss: 116.8344726562 | test_loss: 5.9591197968 | \n",
      "Epoch: 7234 | train_loss: 116.8343353271 | test_loss: 5.9591002464 | \n",
      "Epoch: 7235 | train_loss: 116.8341369629 | test_loss: 5.9590716362 | \n",
      "Epoch: 7236 | train_loss: 116.8340072632 | test_loss: 5.9590454102 | \n",
      "Epoch: 7237 | train_loss: 116.8338088989 | test_loss: 5.9590153694 | \n",
      "Epoch: 7238 | train_loss: 116.8336257935 | test_loss: 5.9589867592 | \n",
      "Epoch: 7239 | train_loss: 116.8334655762 | test_loss: 5.9589624405 | \n",
      "Epoch: 7240 | train_loss: 116.8332977295 | test_loss: 5.9589381218 | \n",
      "Epoch: 7241 | train_loss: 116.8331375122 | test_loss: 5.9589109421 | \n",
      "Epoch: 7242 | train_loss: 116.8329772949 | test_loss: 5.9588856697 | \n",
      "Epoch: 7243 | train_loss: 116.8327636719 | test_loss: 5.9588603973 | \n",
      "Epoch: 7244 | train_loss: 116.8326187134 | test_loss: 5.9588365555 | \n",
      "Epoch: 7245 | train_loss: 116.8324508667 | test_loss: 5.9588093758 | \n",
      "Epoch: 7246 | train_loss: 116.8322525024 | test_loss: 5.9587817192 | \n",
      "Epoch: 7247 | train_loss: 116.8320922852 | test_loss: 5.9587593079 | \n",
      "Epoch: 7248 | train_loss: 116.8319625854 | test_loss: 5.9587316513 | \n",
      "Epoch: 7249 | train_loss: 116.8317794800 | test_loss: 5.9587049484 | \n",
      "Epoch: 7250 | train_loss: 116.8316040039 | test_loss: 5.9586777687 | \n",
      "Epoch: 7251 | train_loss: 116.8314514160 | test_loss: 5.9586505890 | \n",
      "Epoch: 7252 | train_loss: 116.8312606812 | test_loss: 5.9586229324 | \n",
      "Epoch: 7253 | train_loss: 116.8310928345 | test_loss: 5.9585957527 | \n",
      "Epoch: 7254 | train_loss: 116.8309097290 | test_loss: 5.9585666656 | \n",
      "Epoch: 7255 | train_loss: 116.8307189941 | test_loss: 5.9585399628 | \n",
      "Epoch: 7256 | train_loss: 116.8305740356 | test_loss: 5.9585137367 | \n",
      "Epoch: 7257 | train_loss: 116.8304214478 | test_loss: 5.9584846497 | \n",
      "Epoch: 7258 | train_loss: 116.8302078247 | test_loss: 5.9584584236 | \n",
      "Epoch: 7259 | train_loss: 116.8300552368 | test_loss: 5.9584331512 | \n",
      "Epoch: 7260 | train_loss: 116.8298950195 | test_loss: 5.9584064484 | \n",
      "Epoch: 7261 | train_loss: 116.8297348022 | test_loss: 5.9583783150 | \n",
      "Epoch: 7262 | train_loss: 116.8295669556 | test_loss: 5.9583497047 | \n",
      "Epoch: 7263 | train_loss: 116.8293914795 | test_loss: 5.9583282471 | \n",
      "Epoch: 7264 | train_loss: 116.8292465210 | test_loss: 5.9582972527 | \n",
      "Epoch: 7265 | train_loss: 116.8290405273 | test_loss: 5.9582738876 | \n",
      "Epoch: 7266 | train_loss: 116.8288650513 | test_loss: 5.9582495689 | \n",
      "Epoch: 7267 | train_loss: 116.8287124634 | test_loss: 5.9582257271 | \n",
      "Epoch: 7268 | train_loss: 116.8285064697 | test_loss: 5.9581990242 | \n",
      "Epoch: 7269 | train_loss: 116.8283615112 | test_loss: 5.9581737518 | \n",
      "Epoch: 7270 | train_loss: 116.8281860352 | test_loss: 5.9581446648 | \n",
      "Epoch: 7271 | train_loss: 116.8279953003 | test_loss: 5.9581227303 | \n",
      "Epoch: 7272 | train_loss: 116.8278350830 | test_loss: 5.9580926895 | \n",
      "Epoch: 7273 | train_loss: 116.8276596069 | test_loss: 5.9580631256 | \n",
      "Epoch: 7274 | train_loss: 116.8274993896 | test_loss: 5.9580411911 | \n",
      "Epoch: 7275 | train_loss: 116.8273468018 | test_loss: 5.9580140114 | \n",
      "Epoch: 7276 | train_loss: 116.8271789551 | test_loss: 5.9579854012 | \n",
      "Epoch: 7277 | train_loss: 116.8270416260 | test_loss: 5.9579558372 | \n",
      "Epoch: 7278 | train_loss: 116.8268432617 | test_loss: 5.9579296112 | \n",
      "Epoch: 7279 | train_loss: 116.8266906738 | test_loss: 5.9579095840 | \n",
      "Epoch: 7280 | train_loss: 116.8265151978 | test_loss: 5.9578833580 | \n",
      "Epoch: 7281 | train_loss: 116.8263549805 | test_loss: 5.9578528404 | \n",
      "Epoch: 7282 | train_loss: 116.8262176514 | test_loss: 5.9578328133 | \n",
      "Epoch: 7283 | train_loss: 116.8260269165 | test_loss: 5.9578056335 | \n",
      "Epoch: 7284 | train_loss: 116.8258590698 | test_loss: 5.9577817917 | \n",
      "Epoch: 7285 | train_loss: 116.8257141113 | test_loss: 5.9577550888 | \n",
      "Epoch: 7286 | train_loss: 116.8255081177 | test_loss: 5.9577293396 | \n",
      "Epoch: 7287 | train_loss: 116.8253631592 | test_loss: 5.9577021599 | \n",
      "Epoch: 7288 | train_loss: 116.8251876831 | test_loss: 5.9576725960 | \n",
      "Epoch: 7289 | train_loss: 116.8250122070 | test_loss: 5.9576454163 | \n",
      "Epoch: 7290 | train_loss: 116.8248138428 | test_loss: 5.9576206207 | \n",
      "Epoch: 7291 | train_loss: 116.8246459961 | test_loss: 5.9575920105 | \n",
      "Epoch: 7292 | train_loss: 116.8244934082 | test_loss: 5.9575629234 | \n",
      "Epoch: 7293 | train_loss: 116.8242950439 | test_loss: 5.9575405121 | \n",
      "Epoch: 7294 | train_loss: 116.8241424561 | test_loss: 5.9575066566 | \n",
      "Epoch: 7295 | train_loss: 116.8239517212 | test_loss: 5.9574799538 | \n",
      "Epoch: 7296 | train_loss: 116.8238220215 | test_loss: 5.9574556351 | \n",
      "Epoch: 7297 | train_loss: 116.8236312866 | test_loss: 5.9574294090 | \n",
      "Epoch: 7298 | train_loss: 116.8234786987 | test_loss: 5.9574065208 | \n",
      "Epoch: 7299 | train_loss: 116.8232879639 | test_loss: 5.9573836327 | \n",
      "Epoch: 7300 | train_loss: 116.8231353760 | test_loss: 5.9573578835 | \n",
      "Epoch: 7301 | train_loss: 116.8229751587 | test_loss: 5.9573316574 | \n",
      "Epoch: 7302 | train_loss: 116.8227920532 | test_loss: 5.9573020935 | \n",
      "Epoch: 7303 | train_loss: 116.8226470947 | test_loss: 5.9572768211 | \n",
      "Epoch: 7304 | train_loss: 116.8224792480 | test_loss: 5.9572496414 | \n",
      "Epoch: 7305 | train_loss: 116.8223342896 | test_loss: 5.9572224617 | \n",
      "Epoch: 7306 | train_loss: 116.8221893311 | test_loss: 5.9571967125 | \n",
      "Epoch: 7307 | train_loss: 116.8219757080 | test_loss: 5.9571752548 | \n",
      "Epoch: 7308 | train_loss: 116.8218460083 | test_loss: 5.9571504593 | \n",
      "Epoch: 7309 | train_loss: 116.8216781616 | test_loss: 5.9571232796 | \n",
      "Epoch: 7310 | train_loss: 116.8215408325 | test_loss: 5.9570970535 | \n",
      "Epoch: 7311 | train_loss: 116.8213806152 | test_loss: 5.9570713043 | \n",
      "Epoch: 7312 | train_loss: 116.8211975098 | test_loss: 5.9570460320 | \n",
      "Epoch: 7313 | train_loss: 116.8210525513 | test_loss: 5.9570250511 | \n",
      "Epoch: 7314 | train_loss: 116.8208694458 | test_loss: 5.9570002556 | \n",
      "Epoch: 7315 | train_loss: 116.8207015991 | test_loss: 5.9569740295 | \n",
      "Epoch: 7316 | train_loss: 116.8205490112 | test_loss: 5.9569487572 | \n",
      "Epoch: 7317 | train_loss: 116.8203506470 | test_loss: 5.9569249153 | \n",
      "Epoch: 7318 | train_loss: 116.8201828003 | test_loss: 5.9568996429 | \n",
      "Epoch: 7319 | train_loss: 116.8200302124 | test_loss: 5.9568729401 | \n",
      "Epoch: 7320 | train_loss: 116.8198547363 | test_loss: 5.9568443298 | \n",
      "Epoch: 7321 | train_loss: 116.8196945190 | test_loss: 5.9568181038 | \n",
      "Epoch: 7322 | train_loss: 116.8195343018 | test_loss: 5.9567961693 | \n",
      "Epoch: 7323 | train_loss: 116.8193664551 | test_loss: 5.9567713737 | \n",
      "Epoch: 7324 | train_loss: 116.8192214966 | test_loss: 5.9567480087 | \n",
      "Epoch: 7325 | train_loss: 116.8190460205 | test_loss: 5.9567236900 | \n",
      "Epoch: 7326 | train_loss: 116.8188705444 | test_loss: 5.9566993713 | \n",
      "Epoch: 7327 | train_loss: 116.8186645508 | test_loss: 5.9566707611 | \n",
      "Epoch: 7328 | train_loss: 116.8184967041 | test_loss: 5.9566411972 | \n",
      "Epoch: 7329 | train_loss: 116.8183288574 | test_loss: 5.9566164017 | \n",
      "Epoch: 7330 | train_loss: 116.8181762695 | test_loss: 5.9565892220 | \n",
      "Epoch: 7331 | train_loss: 116.8180160522 | test_loss: 5.9565596581 | \n",
      "Epoch: 7332 | train_loss: 116.8178329468 | test_loss: 5.9565343857 | \n",
      "Epoch: 7333 | train_loss: 116.8176651001 | test_loss: 5.9565081596 | \n",
      "Epoch: 7334 | train_loss: 116.8174591064 | test_loss: 5.9564833641 | \n",
      "Epoch: 7335 | train_loss: 116.8172912598 | test_loss: 5.9564609528 | \n",
      "Epoch: 7336 | train_loss: 116.8171157837 | test_loss: 5.9564347267 | \n",
      "Epoch: 7337 | train_loss: 116.8169860840 | test_loss: 5.9564080238 | \n",
      "Epoch: 7338 | train_loss: 116.8168258667 | test_loss: 5.9563794136 | \n",
      "Epoch: 7339 | train_loss: 116.8166809082 | test_loss: 5.9563536644 | \n",
      "Epoch: 7340 | train_loss: 116.8165130615 | test_loss: 5.9563255310 | \n",
      "Epoch: 7341 | train_loss: 116.8163604736 | test_loss: 5.9563016891 | \n",
      "Epoch: 7342 | train_loss: 116.8162002563 | test_loss: 5.9562773705 | \n",
      "Epoch: 7343 | train_loss: 116.8160171509 | test_loss: 5.9562482834 | \n",
      "Epoch: 7344 | train_loss: 116.8158569336 | test_loss: 5.9562206268 | \n",
      "Epoch: 7345 | train_loss: 116.8156967163 | test_loss: 5.9561939240 | \n",
      "Epoch: 7346 | train_loss: 116.8155593872 | test_loss: 5.9561662674 | \n",
      "Epoch: 7347 | train_loss: 116.8153610229 | test_loss: 5.9561443329 | \n",
      "Epoch: 7348 | train_loss: 116.8152160645 | test_loss: 5.9561204910 | \n",
      "Epoch: 7349 | train_loss: 116.8150558472 | test_loss: 5.9560933113 | \n",
      "Epoch: 7350 | train_loss: 116.8148651123 | test_loss: 5.9560699463 | \n",
      "Epoch: 7351 | train_loss: 116.8147277832 | test_loss: 5.9560418129 | \n",
      "Epoch: 7352 | train_loss: 116.8145828247 | test_loss: 5.9560184479 | \n",
      "Epoch: 7353 | train_loss: 116.8144073486 | test_loss: 5.9559922218 | \n",
      "Epoch: 7354 | train_loss: 116.8142471313 | test_loss: 5.9559659958 | \n",
      "Epoch: 7355 | train_loss: 116.8140792847 | test_loss: 5.9559421539 | \n",
      "Epoch: 7356 | train_loss: 116.8139038086 | test_loss: 5.9559202194 | \n",
      "Epoch: 7357 | train_loss: 116.8137588501 | test_loss: 5.9558916092 | \n",
      "Epoch: 7358 | train_loss: 116.8135528564 | test_loss: 5.9558687210 | \n",
      "Epoch: 7359 | train_loss: 116.8133850098 | test_loss: 5.9558429718 | \n",
      "Epoch: 7360 | train_loss: 116.8132324219 | test_loss: 5.9558210373 | \n",
      "Epoch: 7361 | train_loss: 116.8130798340 | test_loss: 5.9557995796 | \n",
      "Epoch: 7362 | train_loss: 116.8129196167 | test_loss: 5.9557738304 | \n",
      "Epoch: 7363 | train_loss: 116.8127441406 | test_loss: 5.9557495117 | \n",
      "Epoch: 7364 | train_loss: 116.8125915527 | test_loss: 5.9557204247 | \n",
      "Epoch: 7365 | train_loss: 116.8124237061 | test_loss: 5.9556922913 | \n",
      "Epoch: 7366 | train_loss: 116.8122329712 | test_loss: 5.9556684494 | \n",
      "Epoch: 7367 | train_loss: 116.8120803833 | test_loss: 5.9556422234 | \n",
      "Epoch: 7368 | train_loss: 116.8119125366 | test_loss: 5.9556093216 | \n",
      "Epoch: 7369 | train_loss: 116.8117675781 | test_loss: 5.9555864334 | \n",
      "Epoch: 7370 | train_loss: 116.8115997314 | test_loss: 5.9555616379 | \n",
      "Epoch: 7371 | train_loss: 116.8114776611 | test_loss: 5.9555330276 | \n",
      "Epoch: 7372 | train_loss: 116.8112716675 | test_loss: 5.9555025101 | \n",
      "Epoch: 7373 | train_loss: 116.8110961914 | test_loss: 5.9554791451 | \n",
      "Epoch: 7374 | train_loss: 116.8109283447 | test_loss: 5.9554510117 | \n",
      "Epoch: 7375 | train_loss: 116.8107833862 | test_loss: 5.9554224014 | \n",
      "Epoch: 7376 | train_loss: 116.8106079102 | test_loss: 5.9553980827 | \n",
      "Epoch: 7377 | train_loss: 116.8104553223 | test_loss: 5.9553661346 | \n",
      "Epoch: 7378 | train_loss: 116.8102722168 | test_loss: 5.9553432465 | \n",
      "Epoch: 7379 | train_loss: 116.8100891113 | test_loss: 5.9553194046 | \n",
      "Epoch: 7380 | train_loss: 116.8099212646 | test_loss: 5.9552917480 | \n",
      "Epoch: 7381 | train_loss: 116.8097534180 | test_loss: 5.9552650452 | \n",
      "Epoch: 7382 | train_loss: 116.8096084595 | test_loss: 5.9552392960 | \n",
      "Epoch: 7383 | train_loss: 116.8094406128 | test_loss: 5.9552140236 | \n",
      "Epoch: 7384 | train_loss: 116.8092956543 | test_loss: 5.9551906586 | \n",
      "Epoch: 7385 | train_loss: 116.8091430664 | test_loss: 5.9551672935 | \n",
      "Epoch: 7386 | train_loss: 116.8089904785 | test_loss: 5.9551472664 | \n",
      "Epoch: 7387 | train_loss: 116.8087997437 | test_loss: 5.9551172256 | \n",
      "Epoch: 7388 | train_loss: 116.8086700439 | test_loss: 5.9550909996 | \n",
      "Epoch: 7389 | train_loss: 116.8084793091 | test_loss: 5.9550647736 | \n",
      "Epoch: 7390 | train_loss: 116.8083190918 | test_loss: 5.9550342560 | \n",
      "Epoch: 7391 | train_loss: 116.8081741333 | test_loss: 5.9550113678 | \n",
      "Epoch: 7392 | train_loss: 116.8080444336 | test_loss: 5.9549798965 | \n",
      "Epoch: 7393 | train_loss: 116.8078536987 | test_loss: 5.9549555779 | \n",
      "Epoch: 7394 | train_loss: 116.8076858521 | test_loss: 5.9549298286 | \n",
      "Epoch: 7395 | train_loss: 116.8075103760 | test_loss: 5.9549059868 | \n",
      "Epoch: 7396 | train_loss: 116.8073501587 | test_loss: 5.9548778534 | \n",
      "Epoch: 7397 | train_loss: 116.8071670532 | test_loss: 5.9548563957 | \n",
      "Epoch: 7398 | train_loss: 116.8069915771 | test_loss: 5.9548258781 | \n",
      "Epoch: 7399 | train_loss: 116.8068466187 | test_loss: 5.9548029900 | \n",
      "Epoch: 7400 | train_loss: 116.8066711426 | test_loss: 5.9547724724 | \n",
      "Epoch: 7401 | train_loss: 116.8064956665 | test_loss: 5.9547452927 | \n",
      "Epoch: 7402 | train_loss: 116.8063201904 | test_loss: 5.9547252655 | \n",
      "Epoch: 7403 | train_loss: 116.8061523438 | test_loss: 5.9546971321 | \n",
      "Epoch: 7404 | train_loss: 116.8059539795 | test_loss: 5.9546747208 | \n",
      "Epoch: 7405 | train_loss: 116.8058090210 | test_loss: 5.9546494484 | \n",
      "Epoch: 7406 | train_loss: 116.8056640625 | test_loss: 5.9546246529 | \n",
      "Epoch: 7407 | train_loss: 116.8054962158 | test_loss: 5.9545984268 | \n",
      "Epoch: 7408 | train_loss: 116.8053588867 | test_loss: 5.9545745850 | \n",
      "Epoch: 7409 | train_loss: 116.8052062988 | test_loss: 5.9545464516 | \n",
      "Epoch: 7410 | train_loss: 116.8050308228 | test_loss: 5.9545221329 | \n",
      "Epoch: 7411 | train_loss: 116.8048629761 | test_loss: 5.9544978142 | \n",
      "Epoch: 7412 | train_loss: 116.8047103882 | test_loss: 5.9544701576 | \n",
      "Epoch: 7413 | train_loss: 116.8045501709 | test_loss: 5.9544439316 | \n",
      "Epoch: 7414 | train_loss: 116.8043746948 | test_loss: 5.9544181824 | \n",
      "Epoch: 7415 | train_loss: 116.8042068481 | test_loss: 5.9543910027 | \n",
      "Epoch: 7416 | train_loss: 116.8040542603 | test_loss: 5.9543614388 | \n",
      "Epoch: 7417 | train_loss: 116.8038558960 | test_loss: 5.9543399811 | \n",
      "Epoch: 7418 | train_loss: 116.8037185669 | test_loss: 5.9543137550 | \n",
      "Epoch: 7419 | train_loss: 116.8035125732 | test_loss: 5.9542851448 | \n",
      "Epoch: 7420 | train_loss: 116.8033905029 | test_loss: 5.9542613029 | \n",
      "Epoch: 7421 | train_loss: 116.8031997681 | test_loss: 5.9542393684 | \n",
      "Epoch: 7422 | train_loss: 116.8030395508 | test_loss: 5.9542098045 | \n",
      "Epoch: 7423 | train_loss: 116.8029098511 | test_loss: 5.9541888237 | \n",
      "Epoch: 7424 | train_loss: 116.8027038574 | test_loss: 5.9541630745 | \n",
      "Epoch: 7425 | train_loss: 116.8025512695 | test_loss: 5.9541363716 | \n",
      "Epoch: 7426 | train_loss: 116.8023986816 | test_loss: 5.9541110992 | \n",
      "Epoch: 7427 | train_loss: 116.8022232056 | test_loss: 5.9540863037 | \n",
      "Epoch: 7428 | train_loss: 116.8020935059 | test_loss: 5.9540662766 | \n",
      "Epoch: 7429 | train_loss: 116.8019332886 | test_loss: 5.9540357590 | \n",
      "Epoch: 7430 | train_loss: 116.8017501831 | test_loss: 5.9540143013 | \n",
      "Epoch: 7431 | train_loss: 116.8015899658 | test_loss: 5.9539852142 | \n",
      "Epoch: 7432 | train_loss: 116.8014450073 | test_loss: 5.9539628029 | \n",
      "Epoch: 7433 | train_loss: 116.8012390137 | test_loss: 5.9539408684 | \n",
      "Epoch: 7434 | train_loss: 116.8010711670 | test_loss: 5.9539170265 | \n",
      "Epoch: 7435 | train_loss: 116.8009338379 | test_loss: 5.9538941383 | \n",
      "Epoch: 7436 | train_loss: 116.8007888794 | test_loss: 5.9538698196 | \n",
      "Epoch: 7437 | train_loss: 116.8006057739 | test_loss: 5.9538426399 | \n",
      "Epoch: 7438 | train_loss: 116.8004226685 | test_loss: 5.9538197517 | \n",
      "Epoch: 7439 | train_loss: 116.8002929688 | test_loss: 5.9537925720 | \n",
      "Epoch: 7440 | train_loss: 116.8000946045 | test_loss: 5.9537677765 | \n",
      "Epoch: 7441 | train_loss: 116.7999725342 | test_loss: 5.9537439346 | \n",
      "Epoch: 7442 | train_loss: 116.7998352051 | test_loss: 5.9537200928 | \n",
      "Epoch: 7443 | train_loss: 116.7996673584 | test_loss: 5.9536943436 | \n",
      "Epoch: 7444 | train_loss: 116.7994918823 | test_loss: 5.9536638260 | \n",
      "Epoch: 7445 | train_loss: 116.7993392944 | test_loss: 5.9536414146 | \n",
      "Epoch: 7446 | train_loss: 116.7991409302 | test_loss: 5.9536118507 | \n",
      "Epoch: 7447 | train_loss: 116.7989807129 | test_loss: 5.9535808563 | \n",
      "Epoch: 7448 | train_loss: 116.7987976074 | test_loss: 5.9535632133 | \n",
      "Epoch: 7449 | train_loss: 116.7986297607 | test_loss: 5.9535331726 | \n",
      "Epoch: 7450 | train_loss: 116.7985076904 | test_loss: 5.9535059929 | \n",
      "Epoch: 7451 | train_loss: 116.7983322144 | test_loss: 5.9534797668 | \n",
      "Epoch: 7452 | train_loss: 116.7981643677 | test_loss: 5.9534521103 | \n",
      "Epoch: 7453 | train_loss: 116.7979888916 | test_loss: 5.9534277916 | \n",
      "Epoch: 7454 | train_loss: 116.7978210449 | test_loss: 5.9534020424 | \n",
      "Epoch: 7455 | train_loss: 116.7976760864 | test_loss: 5.9533758163 | \n",
      "Epoch: 7456 | train_loss: 116.7975082397 | test_loss: 5.9533519745 | \n",
      "Epoch: 7457 | train_loss: 116.7973403931 | test_loss: 5.9533281326 | \n",
      "Epoch: 7458 | train_loss: 116.7971801758 | test_loss: 5.9533061981 | \n",
      "Epoch: 7459 | train_loss: 116.7970275879 | test_loss: 5.9532814026 | \n",
      "Epoch: 7460 | train_loss: 116.7968750000 | test_loss: 5.9532580376 | \n",
      "Epoch: 7461 | train_loss: 116.7967300415 | test_loss: 5.9532361031 | \n",
      "Epoch: 7462 | train_loss: 116.7965393066 | test_loss: 5.9532117844 | \n",
      "Epoch: 7463 | train_loss: 116.7963867188 | test_loss: 5.9531898499 | \n",
      "Epoch: 7464 | train_loss: 116.7962188721 | test_loss: 5.9531626701 | \n",
      "Epoch: 7465 | train_loss: 116.7960739136 | test_loss: 5.9531335831 | \n",
      "Epoch: 7466 | train_loss: 116.7959213257 | test_loss: 5.9531078339 | \n",
      "Epoch: 7467 | train_loss: 116.7957458496 | test_loss: 5.9530806541 | \n",
      "Epoch: 7468 | train_loss: 116.7955856323 | test_loss: 5.9530544281 | \n",
      "Epoch: 7469 | train_loss: 116.7954483032 | test_loss: 5.9530272484 | \n",
      "Epoch: 7470 | train_loss: 116.7952575684 | test_loss: 5.9530000687 | \n",
      "Epoch: 7471 | train_loss: 116.7950668335 | test_loss: 5.9529767036 | \n",
      "Epoch: 7472 | train_loss: 116.7949523926 | test_loss: 5.9529576302 | \n",
      "Epoch: 7473 | train_loss: 116.7947845459 | test_loss: 5.9529304504 | \n",
      "Epoch: 7474 | train_loss: 116.7946319580 | test_loss: 5.9529056549 | \n",
      "Epoch: 7475 | train_loss: 116.7944564819 | test_loss: 5.9528827667 | \n",
      "Epoch: 7476 | train_loss: 116.7942886353 | test_loss: 5.9528594017 | \n",
      "Epoch: 7477 | train_loss: 116.7941436768 | test_loss: 5.9528350830 | \n",
      "Epoch: 7478 | train_loss: 116.7939758301 | test_loss: 5.9528117180 | \n",
      "Epoch: 7479 | train_loss: 116.7938003540 | test_loss: 5.9527869225 | \n",
      "Epoch: 7480 | train_loss: 116.7936706543 | test_loss: 5.9527568817 | \n",
      "Epoch: 7481 | train_loss: 116.7934799194 | test_loss: 5.9527344704 | \n",
      "Epoch: 7482 | train_loss: 116.7933578491 | test_loss: 5.9527115822 | \n",
      "Epoch: 7483 | train_loss: 116.7931900024 | test_loss: 5.9526853561 | \n",
      "Epoch: 7484 | train_loss: 116.7929992676 | test_loss: 5.9526605606 | \n",
      "Epoch: 7485 | train_loss: 116.7929000854 | test_loss: 5.9526367188 | \n",
      "Epoch: 7486 | train_loss: 116.7927474976 | test_loss: 5.9526100159 | \n",
      "Epoch: 7487 | train_loss: 116.7925491333 | test_loss: 5.9525842667 | \n",
      "Epoch: 7488 | train_loss: 116.7924118042 | test_loss: 5.9525599480 | \n",
      "Epoch: 7489 | train_loss: 116.7922744751 | test_loss: 5.9525346756 | \n",
      "Epoch: 7490 | train_loss: 116.7920608521 | test_loss: 5.9525074959 | \n",
      "Epoch: 7491 | train_loss: 116.7918930054 | test_loss: 5.9524841309 | \n",
      "Epoch: 7492 | train_loss: 116.7917175293 | test_loss: 5.9524574280 | \n",
      "Epoch: 7493 | train_loss: 116.7915649414 | test_loss: 5.9524354935 | \n",
      "Epoch: 7494 | train_loss: 116.7914123535 | test_loss: 5.9524106979 | \n",
      "Epoch: 7495 | train_loss: 116.7912521362 | test_loss: 5.9523854256 | \n",
      "Epoch: 7496 | train_loss: 116.7910919189 | test_loss: 5.9523549080 | \n",
      "Epoch: 7497 | train_loss: 116.7909088135 | test_loss: 5.9523339272 | \n",
      "Epoch: 7498 | train_loss: 116.7907714844 | test_loss: 5.9523162842 | \n",
      "Epoch: 7499 | train_loss: 116.7905960083 | test_loss: 5.9522886276 | \n",
      "Epoch: 7500 | train_loss: 116.7904357910 | test_loss: 5.9522633553 | \n",
      "Epoch: 7501 | train_loss: 116.7903060913 | test_loss: 5.9522442818 | \n",
      "Epoch: 7502 | train_loss: 116.7901306152 | test_loss: 5.9522190094 | \n",
      "Epoch: 7503 | train_loss: 116.7899780273 | test_loss: 5.9521942139 | \n",
      "Epoch: 7504 | train_loss: 116.7898178101 | test_loss: 5.9521665573 | \n",
      "Epoch: 7505 | train_loss: 116.7896652222 | test_loss: 5.9521393776 | \n",
      "Epoch: 7506 | train_loss: 116.7894744873 | test_loss: 5.9521183968 | \n",
      "Epoch: 7507 | train_loss: 116.7893371582 | test_loss: 5.9520964622 | \n",
      "Epoch: 7508 | train_loss: 116.7891769409 | test_loss: 5.9520697594 | \n",
      "Epoch: 7509 | train_loss: 116.7890319824 | test_loss: 5.9520454407 | \n",
      "Epoch: 7510 | train_loss: 116.7888565063 | test_loss: 5.9520215988 | \n",
      "Epoch: 7511 | train_loss: 116.7887115479 | test_loss: 5.9519929886 | \n",
      "Epoch: 7512 | train_loss: 116.7885360718 | test_loss: 5.9519672394 | \n",
      "Epoch: 7513 | train_loss: 116.7883987427 | test_loss: 5.9519410133 | \n",
      "Epoch: 7514 | train_loss: 116.7882156372 | test_loss: 5.9519166946 | \n",
      "Epoch: 7515 | train_loss: 116.7880554199 | test_loss: 5.9518890381 | \n",
      "Epoch: 7516 | train_loss: 116.7879180908 | test_loss: 5.9518651962 | \n",
      "Epoch: 7517 | train_loss: 116.7877273560 | test_loss: 5.9518375397 | \n",
      "Epoch: 7518 | train_loss: 116.7875671387 | test_loss: 5.9518141747 | \n",
      "Epoch: 7519 | train_loss: 116.7873916626 | test_loss: 5.9517889023 | \n",
      "Epoch: 7520 | train_loss: 116.7872467041 | test_loss: 5.9517669678 | \n",
      "Epoch: 7521 | train_loss: 116.7870788574 | test_loss: 5.9517388344 | \n",
      "Epoch: 7522 | train_loss: 116.7869262695 | test_loss: 5.9517140388 | \n",
      "Epoch: 7523 | train_loss: 116.7867584229 | test_loss: 5.9516911507 | \n",
      "Epoch: 7524 | train_loss: 116.7866287231 | test_loss: 5.9516644478 | \n",
      "Epoch: 7525 | train_loss: 116.7864685059 | test_loss: 5.9516358376 | \n",
      "Epoch: 7526 | train_loss: 116.7862854004 | test_loss: 5.9516100883 | \n",
      "Epoch: 7527 | train_loss: 116.7861251831 | test_loss: 5.9515843391 | \n",
      "Epoch: 7528 | train_loss: 116.7859802246 | test_loss: 5.9515619278 | \n",
      "Epoch: 7529 | train_loss: 116.7858123779 | test_loss: 5.9515419006 | \n",
      "Epoch: 7530 | train_loss: 116.7856674194 | test_loss: 5.9515123367 | \n",
      "Epoch: 7531 | train_loss: 116.7854919434 | test_loss: 5.9514827728 | \n",
      "Epoch: 7532 | train_loss: 116.7853240967 | test_loss: 5.9514589310 | \n",
      "Epoch: 7533 | train_loss: 116.7851638794 | test_loss: 5.9514374733 | \n",
      "Epoch: 7534 | train_loss: 116.7849884033 | test_loss: 5.9514102936 | \n",
      "Epoch: 7535 | train_loss: 116.7848358154 | test_loss: 5.9513874054 | \n",
      "Epoch: 7536 | train_loss: 116.7846603394 | test_loss: 5.9513611794 | \n",
      "Epoch: 7537 | train_loss: 116.7845153809 | test_loss: 5.9513392448 | \n",
      "Epoch: 7538 | train_loss: 116.7843551636 | test_loss: 5.9513106346 | \n",
      "Epoch: 7539 | train_loss: 116.7841720581 | test_loss: 5.9512891769 | \n",
      "Epoch: 7540 | train_loss: 116.7840118408 | test_loss: 5.9512672424 | \n",
      "Epoch: 7541 | train_loss: 116.7838592529 | test_loss: 5.9512419701 | \n",
      "Epoch: 7542 | train_loss: 116.7837219238 | test_loss: 5.9512152672 | \n",
      "Epoch: 7543 | train_loss: 116.7835388184 | test_loss: 5.9511899948 | \n",
      "Epoch: 7544 | train_loss: 116.7834167480 | test_loss: 5.9511628151 | \n",
      "Epoch: 7545 | train_loss: 116.7832260132 | test_loss: 5.9511332512 | \n",
      "Epoch: 7546 | train_loss: 116.7830581665 | test_loss: 5.9511137009 | \n",
      "Epoch: 7547 | train_loss: 116.7828674316 | test_loss: 5.9510869980 | \n",
      "Epoch: 7548 | train_loss: 116.7827453613 | test_loss: 5.9510626793 | \n",
      "Epoch: 7549 | train_loss: 116.7825851440 | test_loss: 5.9510364532 | \n",
      "Epoch: 7550 | train_loss: 116.7824249268 | test_loss: 5.9510149956 | \n",
      "Epoch: 7551 | train_loss: 116.7822723389 | test_loss: 5.9509925842 | \n",
      "Epoch: 7552 | train_loss: 116.7821197510 | test_loss: 5.9509654045 | \n",
      "Epoch: 7553 | train_loss: 116.7819595337 | test_loss: 5.9509372711 | \n",
      "Epoch: 7554 | train_loss: 116.7817840576 | test_loss: 5.9509110451 | \n",
      "Epoch: 7555 | train_loss: 116.7816619873 | test_loss: 5.9508862495 | \n",
      "Epoch: 7556 | train_loss: 116.7814865112 | test_loss: 5.9508643150 | \n",
      "Epoch: 7557 | train_loss: 116.7813186646 | test_loss: 5.9508380890 | \n",
      "Epoch: 7558 | train_loss: 116.7811431885 | test_loss: 5.9508118629 | \n",
      "Epoch: 7559 | train_loss: 116.7810058594 | test_loss: 5.9507837296 | \n",
      "Epoch: 7560 | train_loss: 116.7808303833 | test_loss: 5.9507575035 | \n",
      "Epoch: 7561 | train_loss: 116.7806549072 | test_loss: 5.9507336617 | \n",
      "Epoch: 7562 | train_loss: 116.7804794312 | test_loss: 5.9507093430 | \n",
      "Epoch: 7563 | train_loss: 116.7803192139 | test_loss: 5.9506878853 | \n",
      "Epoch: 7564 | train_loss: 116.7801361084 | test_loss: 5.9506626129 | \n",
      "Epoch: 7565 | train_loss: 116.7799911499 | test_loss: 5.9506421089 | \n",
      "Epoch: 7566 | train_loss: 116.7798614502 | test_loss: 5.9506168365 | \n",
      "Epoch: 7567 | train_loss: 116.7797088623 | test_loss: 5.9505949020 | \n",
      "Epoch: 7568 | train_loss: 116.7795715332 | test_loss: 5.9505710602 | \n",
      "Epoch: 7569 | train_loss: 116.7793884277 | test_loss: 5.9505424500 | \n",
      "Epoch: 7570 | train_loss: 116.7792358398 | test_loss: 5.9505238533 | \n",
      "Epoch: 7571 | train_loss: 116.7790832520 | test_loss: 5.9504942894 | \n",
      "Epoch: 7572 | train_loss: 116.7788772583 | test_loss: 5.9504685402 | \n",
      "Epoch: 7573 | train_loss: 116.7787322998 | test_loss: 5.9504499435 | \n",
      "Epoch: 7574 | train_loss: 116.7785720825 | test_loss: 5.9504246712 | \n",
      "Epoch: 7575 | train_loss: 116.7784118652 | test_loss: 5.9504008293 | \n",
      "Epoch: 7576 | train_loss: 116.7782745361 | test_loss: 5.9503812790 | \n",
      "Epoch: 7577 | train_loss: 116.7780838013 | test_loss: 5.9503569603 | \n",
      "Epoch: 7578 | train_loss: 116.7779235840 | test_loss: 5.9503326416 | \n",
      "Epoch: 7579 | train_loss: 116.7777709961 | test_loss: 5.9503111839 | \n",
      "Epoch: 7580 | train_loss: 116.7775650024 | test_loss: 5.9502863884 | \n",
      "Epoch: 7581 | train_loss: 116.7774429321 | test_loss: 5.9502592087 | \n",
      "Epoch: 7582 | train_loss: 116.7772979736 | test_loss: 5.9502382278 | \n",
      "Epoch: 7583 | train_loss: 116.7771301270 | test_loss: 5.9502110481 | \n",
      "Epoch: 7584 | train_loss: 116.7769927979 | test_loss: 5.9501843452 | \n",
      "Epoch: 7585 | train_loss: 116.7768173218 | test_loss: 5.9501605034 | \n",
      "Epoch: 7586 | train_loss: 116.7766418457 | test_loss: 5.9501338005 | \n",
      "Epoch: 7587 | train_loss: 116.7764816284 | test_loss: 5.9501118660 | \n",
      "Epoch: 7588 | train_loss: 116.7763214111 | test_loss: 5.9500827789 | \n",
      "Epoch: 7589 | train_loss: 116.7761611938 | test_loss: 5.9500594139 | \n",
      "Epoch: 7590 | train_loss: 116.7760086060 | test_loss: 5.9500379562 | \n",
      "Epoch: 7591 | train_loss: 116.7758560181 | test_loss: 5.9500164986 | \n",
      "Epoch: 7592 | train_loss: 116.7756805420 | test_loss: 5.9499964714 | \n",
      "Epoch: 7593 | train_loss: 116.7755432129 | test_loss: 5.9499688148 | \n",
      "Epoch: 7594 | train_loss: 116.7753829956 | test_loss: 5.9499397278 | \n",
      "Epoch: 7595 | train_loss: 116.7752380371 | test_loss: 5.9499168396 | \n",
      "Epoch: 7596 | train_loss: 116.7751007080 | test_loss: 5.9498925209 | \n",
      "Epoch: 7597 | train_loss: 116.7749328613 | test_loss: 5.9498658180 | \n",
      "Epoch: 7598 | train_loss: 116.7747726440 | test_loss: 5.9498424530 | \n",
      "Epoch: 7599 | train_loss: 116.7746200562 | test_loss: 5.9498181343 | \n",
      "Epoch: 7600 | train_loss: 116.7744522095 | test_loss: 5.9497900009 | \n",
      "Epoch: 7601 | train_loss: 116.7743072510 | test_loss: 5.9497623444 | \n",
      "Epoch: 7602 | train_loss: 116.7741394043 | test_loss: 5.9497356415 | \n",
      "Epoch: 7603 | train_loss: 116.7739715576 | test_loss: 5.9497117996 | \n",
      "Epoch: 7604 | train_loss: 116.7738189697 | test_loss: 5.9496912956 | \n",
      "Epoch: 7605 | train_loss: 116.7736587524 | test_loss: 5.9496717453 | \n",
      "Epoch: 7606 | train_loss: 116.7735061646 | test_loss: 5.9496455193 | \n",
      "Epoch: 7607 | train_loss: 116.7733306885 | test_loss: 5.9496245384 | \n",
      "Epoch: 7608 | train_loss: 116.7732009888 | test_loss: 5.9495987892 | \n",
      "Epoch: 7609 | train_loss: 116.7730407715 | test_loss: 5.9495768547 | \n",
      "Epoch: 7610 | train_loss: 116.7728805542 | test_loss: 5.9495549202 | \n",
      "Epoch: 7611 | train_loss: 116.7727279663 | test_loss: 5.9495334625 | \n",
      "Epoch: 7612 | train_loss: 116.7725677490 | test_loss: 5.9495100975 | \n",
      "Epoch: 7613 | train_loss: 116.7723999023 | test_loss: 5.9494829178 | \n",
      "Epoch: 7614 | train_loss: 116.7722549438 | test_loss: 5.9494628906 | \n",
      "Epoch: 7615 | train_loss: 116.7721099854 | test_loss: 5.9494352341 | \n",
      "Epoch: 7616 | train_loss: 116.7719497681 | test_loss: 5.9494147301 | \n",
      "Epoch: 7617 | train_loss: 116.7718048096 | test_loss: 5.9493870735 | \n",
      "Epoch: 7618 | train_loss: 116.7716598511 | test_loss: 5.9493627548 | \n",
      "Epoch: 7619 | train_loss: 116.7714843750 | test_loss: 5.9493408203 | \n",
      "Epoch: 7620 | train_loss: 116.7713470459 | test_loss: 5.9493165016 | \n",
      "Epoch: 7621 | train_loss: 116.7712097168 | test_loss: 5.9492936134 | \n",
      "Epoch: 7622 | train_loss: 116.7710189819 | test_loss: 5.9492626190 | \n",
      "Epoch: 7623 | train_loss: 116.7708587646 | test_loss: 5.9492397308 | \n",
      "Epoch: 7624 | train_loss: 116.7707214355 | test_loss: 5.9492211342 | \n",
      "Epoch: 7625 | train_loss: 116.7705459595 | test_loss: 5.9491949081 | \n",
      "Epoch: 7626 | train_loss: 116.7704086304 | test_loss: 5.9491682053 | \n",
      "Epoch: 7627 | train_loss: 116.7702407837 | test_loss: 5.9491429329 | \n",
      "Epoch: 7628 | train_loss: 116.7700958252 | test_loss: 5.9491205215 | \n",
      "Epoch: 7629 | train_loss: 116.7699279785 | test_loss: 5.9491004944 | \n",
      "Epoch: 7630 | train_loss: 116.7697830200 | test_loss: 5.9490771294 | \n",
      "Epoch: 7631 | train_loss: 116.7696075439 | test_loss: 5.9490513802 | \n",
      "Epoch: 7632 | train_loss: 116.7694320679 | test_loss: 5.9490265846 | \n",
      "Epoch: 7633 | train_loss: 116.7692642212 | test_loss: 5.9490013123 | \n",
      "Epoch: 7634 | train_loss: 116.7690887451 | test_loss: 5.9489784241 | \n",
      "Epoch: 7635 | train_loss: 116.7689666748 | test_loss: 5.9489550591 | \n",
      "Epoch: 7636 | train_loss: 116.7688064575 | test_loss: 5.9489331245 | \n",
      "Epoch: 7637 | train_loss: 116.7686233521 | test_loss: 5.9489054680 | \n",
      "Epoch: 7638 | train_loss: 116.7684783936 | test_loss: 5.9488773346 | \n",
      "Epoch: 7639 | train_loss: 116.7683105469 | test_loss: 5.9488568306 | \n",
      "Epoch: 7640 | train_loss: 116.7681427002 | test_loss: 5.9488306046 | \n",
      "Epoch: 7641 | train_loss: 116.7680358887 | test_loss: 5.9488029480 | \n",
      "Epoch: 7642 | train_loss: 116.7678527832 | test_loss: 5.9487776756 | \n",
      "Epoch: 7643 | train_loss: 116.7677078247 | test_loss: 5.9487500191 | \n",
      "Epoch: 7644 | train_loss: 116.7675399780 | test_loss: 5.9487280846 | \n",
      "Epoch: 7645 | train_loss: 116.7673797607 | test_loss: 5.9487042427 | \n",
      "Epoch: 7646 | train_loss: 116.7672042847 | test_loss: 5.9486818314 | \n",
      "Epoch: 7647 | train_loss: 116.7670593262 | test_loss: 5.9486589432 | \n",
      "Epoch: 7648 | train_loss: 116.7669219971 | test_loss: 5.9486317635 | \n",
      "Epoch: 7649 | train_loss: 116.7667465210 | test_loss: 5.9486069679 | \n",
      "Epoch: 7650 | train_loss: 116.7666015625 | test_loss: 5.9485850334 | \n",
      "Epoch: 7651 | train_loss: 116.7664566040 | test_loss: 5.9485640526 | \n",
      "Epoch: 7652 | train_loss: 116.7663116455 | test_loss: 5.9485402107 | \n",
      "Epoch: 7653 | train_loss: 116.7661437988 | test_loss: 5.9485168457 | \n",
      "Epoch: 7654 | train_loss: 116.7659988403 | test_loss: 5.9484920502 | \n",
      "Epoch: 7655 | train_loss: 116.7658386230 | test_loss: 5.9484629631 | \n",
      "Epoch: 7656 | train_loss: 116.7656555176 | test_loss: 5.9484376907 | \n",
      "Epoch: 7657 | train_loss: 116.7654953003 | test_loss: 5.9484186172 | \n",
      "Epoch: 7658 | train_loss: 116.7653579712 | test_loss: 5.9483942986 | \n",
      "Epoch: 7659 | train_loss: 116.7651824951 | test_loss: 5.9483704567 | \n",
      "Epoch: 7660 | train_loss: 116.7650451660 | test_loss: 5.9483442307 | \n",
      "Epoch: 7661 | train_loss: 116.7649154663 | test_loss: 5.9483232498 | \n",
      "Epoch: 7662 | train_loss: 116.7647399902 | test_loss: 5.9483008385 | \n",
      "Epoch: 7663 | train_loss: 116.7645568848 | test_loss: 5.9482755661 | \n",
      "Epoch: 7664 | train_loss: 116.7644271851 | test_loss: 5.9482488632 | \n",
      "Epoch: 7665 | train_loss: 116.7642517090 | test_loss: 5.9482250214 | \n",
      "Epoch: 7666 | train_loss: 116.7640991211 | test_loss: 5.9482035637 | \n",
      "Epoch: 7667 | train_loss: 116.7639465332 | test_loss: 5.9481768608 | \n",
      "Epoch: 7668 | train_loss: 116.7637786865 | test_loss: 5.9481544495 | \n",
      "Epoch: 7669 | train_loss: 116.7636413574 | test_loss: 5.9481306076 | \n",
      "Epoch: 7670 | train_loss: 116.7634735107 | test_loss: 5.9481110573 | \n",
      "Epoch: 7671 | train_loss: 116.7633209229 | test_loss: 5.9480834007 | \n",
      "Epoch: 7672 | train_loss: 116.7631378174 | test_loss: 5.9480586052 | \n",
      "Epoch: 7673 | train_loss: 116.7629776001 | test_loss: 5.9480347633 | \n",
      "Epoch: 7674 | train_loss: 116.7627944946 | test_loss: 5.9480094910 | \n",
      "Epoch: 7675 | train_loss: 116.7626571655 | test_loss: 5.9479856491 | \n",
      "Epoch: 7676 | train_loss: 116.7624893188 | test_loss: 5.9479632378 | \n",
      "Epoch: 7677 | train_loss: 116.7623519897 | test_loss: 5.9479384422 | \n",
      "Epoch: 7678 | train_loss: 116.7621994019 | test_loss: 5.9479107857 | \n",
      "Epoch: 7679 | train_loss: 116.7620391846 | test_loss: 5.9478893280 | \n",
      "Epoch: 7680 | train_loss: 116.7618484497 | test_loss: 5.9478631020 | \n",
      "Epoch: 7681 | train_loss: 116.7616806030 | test_loss: 5.9478435516 | \n",
      "Epoch: 7682 | train_loss: 116.7615356445 | test_loss: 5.9478216171 | \n",
      "Epoch: 7683 | train_loss: 116.7614059448 | test_loss: 5.9477968216 | \n",
      "Epoch: 7684 | train_loss: 116.7612228394 | test_loss: 5.9477739334 | \n",
      "Epoch: 7685 | train_loss: 116.7610778809 | test_loss: 5.9477534294 | \n",
      "Epoch: 7686 | train_loss: 116.7608871460 | test_loss: 5.9477272034 | \n",
      "Epoch: 7687 | train_loss: 116.7607498169 | test_loss: 5.9477019310 | \n",
      "Epoch: 7688 | train_loss: 116.7606353760 | test_loss: 5.9476790428 | \n",
      "Epoch: 7689 | train_loss: 116.7604522705 | test_loss: 5.9476566315 | \n",
      "Epoch: 7690 | train_loss: 116.7603149414 | test_loss: 5.9476342201 | \n",
      "Epoch: 7691 | train_loss: 116.7601547241 | test_loss: 5.9476051331 | \n",
      "Epoch: 7692 | train_loss: 116.7600021362 | test_loss: 5.9475803375 | \n",
      "Epoch: 7693 | train_loss: 116.7598571777 | test_loss: 5.9475531578 | \n",
      "Epoch: 7694 | train_loss: 116.7596893311 | test_loss: 5.9475312233 | \n",
      "Epoch: 7695 | train_loss: 116.7595596313 | test_loss: 5.9475069046 | \n",
      "Epoch: 7696 | train_loss: 116.7593841553 | test_loss: 5.9474835396 | \n",
      "Epoch: 7697 | train_loss: 116.7592391968 | test_loss: 5.9474611282 | \n",
      "Epoch: 7698 | train_loss: 116.7591018677 | test_loss: 5.9474382401 | \n",
      "Epoch: 7699 | train_loss: 116.7589263916 | test_loss: 5.9474143982 | \n",
      "Epoch: 7700 | train_loss: 116.7587814331 | test_loss: 5.9473915100 | \n",
      "Epoch: 7701 | train_loss: 116.7586059570 | test_loss: 5.9473719597 | \n",
      "Epoch: 7702 | train_loss: 116.7584762573 | test_loss: 5.9473476410 | \n",
      "Epoch: 7703 | train_loss: 116.7583007812 | test_loss: 5.9473218918 | \n",
      "Epoch: 7704 | train_loss: 116.7581634521 | test_loss: 5.9472985268 | \n",
      "Epoch: 7705 | train_loss: 116.7580108643 | test_loss: 5.9472765923 | \n",
      "Epoch: 7706 | train_loss: 116.7578353882 | test_loss: 5.9472489357 | \n",
      "Epoch: 7707 | train_loss: 116.7576751709 | test_loss: 5.9472279549 | \n",
      "Epoch: 7708 | train_loss: 116.7575378418 | test_loss: 5.9472060204 | \n",
      "Epoch: 7709 | train_loss: 116.7573852539 | test_loss: 5.9471778870 | \n",
      "Epoch: 7710 | train_loss: 116.7572250366 | test_loss: 5.9471573830 | \n",
      "Epoch: 7711 | train_loss: 116.7570724487 | test_loss: 5.9471354485 | \n",
      "Epoch: 7712 | train_loss: 116.7569274902 | test_loss: 5.9471116066 | \n",
      "Epoch: 7713 | train_loss: 116.7567596436 | test_loss: 5.9470882416 | \n",
      "Epoch: 7714 | train_loss: 116.7565917969 | test_loss: 5.9470677376 | \n",
      "Epoch: 7715 | train_loss: 116.7564849854 | test_loss: 5.9470458031 | \n",
      "Epoch: 7716 | train_loss: 116.7563476562 | test_loss: 5.9470200539 | \n",
      "Epoch: 7717 | train_loss: 116.7561721802 | test_loss: 5.9469995499 | \n",
      "Epoch: 7718 | train_loss: 116.7560577393 | test_loss: 5.9469742775 | \n",
      "Epoch: 7719 | train_loss: 116.7558670044 | test_loss: 5.9469499588 | \n",
      "Epoch: 7720 | train_loss: 116.7557220459 | test_loss: 5.9469275475 | \n",
      "Epoch: 7721 | train_loss: 116.7555694580 | test_loss: 5.9468998909 | \n",
      "Epoch: 7722 | train_loss: 116.7554016113 | test_loss: 5.9468779564 | \n",
      "Epoch: 7723 | train_loss: 116.7552642822 | test_loss: 5.9468522072 | \n",
      "Epoch: 7724 | train_loss: 116.7550964355 | test_loss: 5.9468345642 | \n",
      "Epoch: 7725 | train_loss: 116.7549591064 | test_loss: 5.9468059540 | \n",
      "Epoch: 7726 | train_loss: 116.7547683716 | test_loss: 5.9467821121 | \n",
      "Epoch: 7727 | train_loss: 116.7546081543 | test_loss: 5.9467592239 | \n",
      "Epoch: 7728 | train_loss: 116.7544555664 | test_loss: 5.9467344284 | \n",
      "Epoch: 7729 | train_loss: 116.7543182373 | test_loss: 5.9467091560 | \n",
      "Epoch: 7730 | train_loss: 116.7541732788 | test_loss: 5.9466872215 | \n",
      "Epoch: 7731 | train_loss: 116.7540054321 | test_loss: 5.9466633797 | \n",
      "Epoch: 7732 | train_loss: 116.7538604736 | test_loss: 5.9466400146 | \n",
      "Epoch: 7733 | train_loss: 116.7537078857 | test_loss: 5.9466161728 | \n",
      "Epoch: 7734 | train_loss: 116.7535552979 | test_loss: 5.9465928078 | \n",
      "Epoch: 7735 | train_loss: 116.7533874512 | test_loss: 5.9465718269 | \n",
      "Epoch: 7736 | train_loss: 116.7532653809 | test_loss: 5.9465479851 | \n",
      "Epoch: 7737 | train_loss: 116.7531280518 | test_loss: 5.9465246201 | \n",
      "Epoch: 7738 | train_loss: 116.7529602051 | test_loss: 5.9465007782 | \n",
      "Epoch: 7739 | train_loss: 116.7527770996 | test_loss: 5.9464778900 | \n",
      "Epoch: 7740 | train_loss: 116.7526168823 | test_loss: 5.9464554787 | \n",
      "Epoch: 7741 | train_loss: 116.7524490356 | test_loss: 5.9464287758 | \n",
      "Epoch: 7742 | train_loss: 116.7523040771 | test_loss: 5.9464087486 | \n",
      "Epoch: 7743 | train_loss: 116.7521591187 | test_loss: 5.9463872910 | \n",
      "Epoch: 7744 | train_loss: 116.7520294189 | test_loss: 5.9463624954 | \n",
      "Epoch: 7745 | train_loss: 116.7518615723 | test_loss: 5.9463372231 | \n",
      "Epoch: 7746 | train_loss: 116.7517166138 | test_loss: 5.9463148117 | \n",
      "Epoch: 7747 | train_loss: 116.7515335083 | test_loss: 5.9462952614 | \n",
      "Epoch: 7748 | train_loss: 116.7513961792 | test_loss: 5.9462680817 | \n",
      "Epoch: 7749 | train_loss: 116.7512130737 | test_loss: 5.9462471008 | \n",
      "Epoch: 7750 | train_loss: 116.7510833740 | test_loss: 5.9462218285 | \n",
      "Epoch: 7751 | train_loss: 116.7509384155 | test_loss: 5.9462018013 | \n",
      "Epoch: 7752 | train_loss: 116.7508010864 | test_loss: 5.9461774826 | \n",
      "Epoch: 7753 | train_loss: 116.7506332397 | test_loss: 5.9461560249 | \n",
      "Epoch: 7754 | train_loss: 116.7504501343 | test_loss: 5.9461326599 | \n",
      "Epoch: 7755 | train_loss: 116.7503204346 | test_loss: 5.9461107254 | \n",
      "Epoch: 7756 | train_loss: 116.7501144409 | test_loss: 5.9460883141 | \n",
      "Epoch: 7757 | train_loss: 116.7499771118 | test_loss: 5.9460687637 | \n",
      "Epoch: 7758 | train_loss: 116.7498474121 | test_loss: 5.9460477829 | \n",
      "Epoch: 7759 | train_loss: 116.7497100830 | test_loss: 5.9460163116 | \n",
      "Epoch: 7760 | train_loss: 116.7494888306 | test_loss: 5.9459986687 | \n",
      "Epoch: 7761 | train_loss: 116.7493743896 | test_loss: 5.9459729195 | \n",
      "Epoch: 7762 | train_loss: 116.7492218018 | test_loss: 5.9459490776 | \n",
      "Epoch: 7763 | train_loss: 116.7490615845 | test_loss: 5.9459314346 | \n",
      "Epoch: 7764 | train_loss: 116.7489242554 | test_loss: 5.9459061623 | \n",
      "Epoch: 7765 | train_loss: 116.7487716675 | test_loss: 5.9458856583 | \n",
      "Epoch: 7766 | train_loss: 116.7486267090 | test_loss: 5.9458599091 | \n",
      "Epoch: 7767 | train_loss: 116.7484588623 | test_loss: 5.9458341599 | \n",
      "Epoch: 7768 | train_loss: 116.7482986450 | test_loss: 5.9458155632 | \n",
      "Epoch: 7769 | train_loss: 116.7481384277 | test_loss: 5.9457883835 | \n",
      "Epoch: 7770 | train_loss: 116.7479705811 | test_loss: 5.9457645416 | \n",
      "Epoch: 7771 | train_loss: 116.7478332520 | test_loss: 5.9457435608 | \n",
      "Epoch: 7772 | train_loss: 116.7476882935 | test_loss: 5.9457187653 | \n",
      "Epoch: 7773 | train_loss: 116.7475128174 | test_loss: 5.9456958771 | \n",
      "Epoch: 7774 | train_loss: 116.7473449707 | test_loss: 5.9456663132 | \n",
      "Epoch: 7775 | train_loss: 116.7471847534 | test_loss: 5.9456400871 | \n",
      "Epoch: 7776 | train_loss: 116.7470092773 | test_loss: 5.9456176758 | \n",
      "Epoch: 7777 | train_loss: 116.7468719482 | test_loss: 5.9455938339 | \n",
      "Epoch: 7778 | train_loss: 116.7467269897 | test_loss: 5.9455690384 | \n",
      "Epoch: 7779 | train_loss: 116.7465438843 | test_loss: 5.9455456734 | \n",
      "Epoch: 7780 | train_loss: 116.7463760376 | test_loss: 5.9455246925 | \n",
      "Epoch: 7781 | train_loss: 116.7462387085 | test_loss: 5.9454998970 | \n",
      "Epoch: 7782 | train_loss: 116.7460327148 | test_loss: 5.9454798698 | \n",
      "Epoch: 7783 | train_loss: 116.7458877563 | test_loss: 5.9454565048 | \n",
      "Epoch: 7784 | train_loss: 116.7457504272 | test_loss: 5.9454326630 | \n",
      "Epoch: 7785 | train_loss: 116.7455749512 | test_loss: 5.9454083443 | \n",
      "Epoch: 7786 | train_loss: 116.7454376221 | test_loss: 5.9453830719 | \n",
      "Epoch: 7787 | train_loss: 116.7452850342 | test_loss: 5.9453649521 | \n",
      "Epoch: 7788 | train_loss: 116.7451171875 | test_loss: 5.9453320503 | \n",
      "Epoch: 7789 | train_loss: 116.7449493408 | test_loss: 5.9453067780 | \n",
      "Epoch: 7790 | train_loss: 116.7447814941 | test_loss: 5.9452800751 | \n",
      "Epoch: 7791 | train_loss: 116.7446594238 | test_loss: 5.9452600479 | \n",
      "Epoch: 7792 | train_loss: 116.7444992065 | test_loss: 5.9452400208 | \n",
      "Epoch: 7793 | train_loss: 116.7443389893 | test_loss: 5.9452166557 | \n",
      "Epoch: 7794 | train_loss: 116.7441711426 | test_loss: 5.9451951981 | \n",
      "Epoch: 7795 | train_loss: 116.7440261841 | test_loss: 5.9451723099 | \n",
      "Epoch: 7796 | train_loss: 116.7438354492 | test_loss: 5.9451513290 | \n",
      "Epoch: 7797 | train_loss: 116.7436828613 | test_loss: 5.9451251030 | \n",
      "Epoch: 7798 | train_loss: 116.7435379028 | test_loss: 5.9451036453 | \n",
      "Epoch: 7799 | train_loss: 116.7433853149 | test_loss: 5.9450850487 | \n",
      "Epoch: 7800 | train_loss: 116.7432403564 | test_loss: 5.9450597763 | \n",
      "Epoch: 7801 | train_loss: 116.7430572510 | test_loss: 5.9450440407 | \n",
      "Epoch: 7802 | train_loss: 116.7429199219 | test_loss: 5.9450187683 | \n",
      "Epoch: 7803 | train_loss: 116.7427749634 | test_loss: 5.9449968338 | \n",
      "Epoch: 7804 | train_loss: 116.7426071167 | test_loss: 5.9449729919 | \n",
      "Epoch: 7805 | train_loss: 116.7424621582 | test_loss: 5.9449558258 | \n",
      "Epoch: 7806 | train_loss: 116.7422866821 | test_loss: 5.9449353218 | \n",
      "Epoch: 7807 | train_loss: 116.7421569824 | test_loss: 5.9449172020 | \n",
      "Epoch: 7808 | train_loss: 116.7420043945 | test_loss: 5.9448895454 | \n",
      "Epoch: 7809 | train_loss: 116.7418823242 | test_loss: 5.9448695183 | \n",
      "Epoch: 7810 | train_loss: 116.7417602539 | test_loss: 5.9448485374 | \n",
      "Epoch: 7811 | train_loss: 116.7415924072 | test_loss: 5.9448218346 | \n",
      "Epoch: 7812 | train_loss: 116.7414550781 | test_loss: 5.9447960854 | \n",
      "Epoch: 7813 | train_loss: 116.7413024902 | test_loss: 5.9447741508 | \n",
      "Epoch: 7814 | train_loss: 116.7411346436 | test_loss: 5.9447517395 | \n",
      "Epoch: 7815 | train_loss: 116.7409896851 | test_loss: 5.9447321892 | \n",
      "Epoch: 7816 | train_loss: 116.7408370972 | test_loss: 5.9447059631 | \n",
      "Epoch: 7817 | train_loss: 116.7406845093 | test_loss: 5.9446864128 | \n",
      "Epoch: 7818 | train_loss: 116.7405395508 | test_loss: 5.9446649551 | \n",
      "Epoch: 7819 | train_loss: 116.7403869629 | test_loss: 5.9446434975 | \n",
      "Epoch: 7820 | train_loss: 116.7402038574 | test_loss: 5.9446196556 | \n",
      "Epoch: 7821 | train_loss: 116.7399978638 | test_loss: 5.9446024895 | \n",
      "Epoch: 7822 | train_loss: 116.7398757935 | test_loss: 5.9445776939 | \n",
      "Epoch: 7823 | train_loss: 116.7397079468 | test_loss: 5.9445600510 | \n",
      "Epoch: 7824 | train_loss: 116.7395706177 | test_loss: 5.9445343018 | \n",
      "Epoch: 7825 | train_loss: 116.7393951416 | test_loss: 5.9445166588 | \n",
      "Epoch: 7826 | train_loss: 116.7392883301 | test_loss: 5.9444909096 | \n",
      "Epoch: 7827 | train_loss: 116.7391204834 | test_loss: 5.9444689751 | \n",
      "Epoch: 7828 | train_loss: 116.7389373779 | test_loss: 5.9444456100 | \n",
      "Epoch: 7829 | train_loss: 116.7387619019 | test_loss: 5.9444274902 | \n",
      "Epoch: 7830 | train_loss: 116.7386245728 | test_loss: 5.9444036484 | \n",
      "Epoch: 7831 | train_loss: 116.7384796143 | test_loss: 5.9443836212 | \n",
      "Epoch: 7832 | train_loss: 116.7382812500 | test_loss: 5.9443578720 | \n",
      "Epoch: 7833 | train_loss: 116.7381439209 | test_loss: 5.9443359375 | \n",
      "Epoch: 7834 | train_loss: 116.7379989624 | test_loss: 5.9443125725 | \n",
      "Epoch: 7835 | train_loss: 116.7378540039 | test_loss: 5.9442949295 | \n",
      "Epoch: 7836 | train_loss: 116.7377014160 | test_loss: 5.9442734718 | \n",
      "Epoch: 7837 | train_loss: 116.7375793457 | test_loss: 5.9442491531 | \n",
      "Epoch: 7838 | train_loss: 116.7374191284 | test_loss: 5.9442229271 | \n",
      "Epoch: 7839 | train_loss: 116.7372741699 | test_loss: 5.9442005157 | \n",
      "Epoch: 7840 | train_loss: 116.7371368408 | test_loss: 5.9441776276 | \n",
      "Epoch: 7841 | train_loss: 116.7369613647 | test_loss: 5.9441528320 | \n",
      "Epoch: 7842 | train_loss: 116.7368164062 | test_loss: 5.9441318512 | \n",
      "Epoch: 7843 | train_loss: 116.7366790771 | test_loss: 5.9441132545 | \n",
      "Epoch: 7844 | train_loss: 116.7364959717 | test_loss: 5.9440855980 | \n",
      "Epoch: 7845 | train_loss: 116.7363510132 | test_loss: 5.9440627098 | \n",
      "Epoch: 7846 | train_loss: 116.7362136841 | test_loss: 5.9440388680 | \n",
      "Epoch: 7847 | train_loss: 116.7359924316 | test_loss: 5.9440159798 | \n",
      "Epoch: 7848 | train_loss: 116.7358703613 | test_loss: 5.9439954758 | \n",
      "Epoch: 7849 | train_loss: 116.7357025146 | test_loss: 5.9439668655 | \n",
      "Epoch: 7850 | train_loss: 116.7355575562 | test_loss: 5.9439473152 | \n",
      "Epoch: 7851 | train_loss: 116.7353973389 | test_loss: 5.9439244270 | \n",
      "Epoch: 7852 | train_loss: 116.7352142334 | test_loss: 5.9439039230 | \n",
      "Epoch: 7853 | train_loss: 116.7350616455 | test_loss: 5.9438819885 | \n",
      "Epoch: 7854 | train_loss: 116.7348937988 | test_loss: 5.9438600540 | \n",
      "Epoch: 7855 | train_loss: 116.7347717285 | test_loss: 5.9438395500 | \n",
      "Epoch: 7856 | train_loss: 116.7345962524 | test_loss: 5.9438142776 | \n",
      "Epoch: 7857 | train_loss: 116.7344436646 | test_loss: 5.9437885284 | \n",
      "Epoch: 7858 | train_loss: 116.7342987061 | test_loss: 5.9437665939 | \n",
      "Epoch: 7859 | train_loss: 116.7341766357 | test_loss: 5.9437446594 | \n",
      "Epoch: 7860 | train_loss: 116.7340011597 | test_loss: 5.9437217712 | \n",
      "Epoch: 7861 | train_loss: 116.7338485718 | test_loss: 5.9436993599 | \n",
      "Epoch: 7862 | train_loss: 116.7337112427 | test_loss: 5.9436802864 | \n",
      "Epoch: 7863 | train_loss: 116.7335510254 | test_loss: 5.9436540604 | \n",
      "Epoch: 7864 | train_loss: 116.7334060669 | test_loss: 5.9436302185 | \n",
      "Epoch: 7865 | train_loss: 116.7332458496 | test_loss: 5.9436097145 | \n",
      "Epoch: 7866 | train_loss: 116.7330932617 | test_loss: 5.9435839653 | \n",
      "Epoch: 7867 | train_loss: 116.7329406738 | test_loss: 5.9435606003 | \n",
      "Epoch: 7868 | train_loss: 116.7327423096 | test_loss: 5.9435405731 | \n",
      "Epoch: 7869 | train_loss: 116.7326278687 | test_loss: 5.9435138702 | \n",
      "Epoch: 7870 | train_loss: 116.7324600220 | test_loss: 5.9434919357 | \n",
      "Epoch: 7871 | train_loss: 116.7323150635 | test_loss: 5.9434690475 | \n",
      "Epoch: 7872 | train_loss: 116.7321701050 | test_loss: 5.9434509277 | \n",
      "Epoch: 7873 | train_loss: 116.7320404053 | test_loss: 5.9434266090 | \n",
      "Epoch: 7874 | train_loss: 116.7318878174 | test_loss: 5.9434046745 | \n",
      "Epoch: 7875 | train_loss: 116.7317123413 | test_loss: 5.9433817863 | \n",
      "Epoch: 7876 | train_loss: 116.7315597534 | test_loss: 5.9433608055 | \n",
      "Epoch: 7877 | train_loss: 116.7314376831 | test_loss: 5.9433417320 | \n",
      "Epoch: 7878 | train_loss: 116.7312393188 | test_loss: 5.9433193207 | \n",
      "Epoch: 7879 | train_loss: 116.7311096191 | test_loss: 5.9432964325 | \n",
      "Epoch: 7880 | train_loss: 116.7309875488 | test_loss: 5.9432716370 | \n",
      "Epoch: 7881 | train_loss: 116.7307739258 | test_loss: 5.9432520866 | \n",
      "Epoch: 7882 | train_loss: 116.7306442261 | test_loss: 5.9432344437 | \n",
      "Epoch: 7883 | train_loss: 116.7305068970 | test_loss: 5.9432067871 | \n",
      "Epoch: 7884 | train_loss: 116.7303390503 | test_loss: 5.9431867599 | \n",
      "Epoch: 7885 | train_loss: 116.7301788330 | test_loss: 5.9431633949 | \n",
      "Epoch: 7886 | train_loss: 116.7300186157 | test_loss: 5.9431376457 | \n",
      "Epoch: 7887 | train_loss: 116.7298889160 | test_loss: 5.9431190491 | \n",
      "Epoch: 7888 | train_loss: 116.7297134399 | test_loss: 5.9431023598 | \n",
      "Epoch: 7889 | train_loss: 116.7295989990 | test_loss: 5.9430775642 | \n",
      "Epoch: 7890 | train_loss: 116.7294235229 | test_loss: 5.9430546761 | \n",
      "Epoch: 7891 | train_loss: 116.7292861938 | test_loss: 5.9430360794 | \n",
      "Epoch: 7892 | train_loss: 116.7291564941 | test_loss: 5.9430112839 | \n",
      "Epoch: 7893 | train_loss: 116.7289810181 | test_loss: 5.9429864883 | \n",
      "Epoch: 7894 | train_loss: 116.7288055420 | test_loss: 5.9429664612 | \n",
      "Epoch: 7895 | train_loss: 116.7286605835 | test_loss: 5.9429450035 | \n",
      "Epoch: 7896 | train_loss: 116.7285156250 | test_loss: 5.9429249763 | \n",
      "Epoch: 7897 | train_loss: 116.7283630371 | test_loss: 5.9429030418 | \n",
      "Epoch: 7898 | train_loss: 116.7282257080 | test_loss: 5.9428839684 | \n",
      "Epoch: 7899 | train_loss: 116.7280578613 | test_loss: 5.9428582191 | \n",
      "Epoch: 7900 | train_loss: 116.7278976440 | test_loss: 5.9428377151 | \n",
      "Epoch: 7901 | train_loss: 116.7277603149 | test_loss: 5.9428172112 | \n",
      "Epoch: 7902 | train_loss: 116.7275848389 | test_loss: 5.9427943230 | \n",
      "Epoch: 7903 | train_loss: 116.7274627686 | test_loss: 5.9427700043 | \n",
      "Epoch: 7904 | train_loss: 116.7272949219 | test_loss: 5.9427490234 | \n",
      "Epoch: 7905 | train_loss: 116.7271270752 | test_loss: 5.9427275658 | \n",
      "Epoch: 7906 | train_loss: 116.7269592285 | test_loss: 5.9427056313 | \n",
      "Epoch: 7907 | train_loss: 116.7268524170 | test_loss: 5.9426856041 | \n",
      "Epoch: 7908 | train_loss: 116.7267150879 | test_loss: 5.9426631927 | \n",
      "Epoch: 7909 | train_loss: 116.7265396118 | test_loss: 5.9426417351 | \n",
      "Epoch: 7910 | train_loss: 116.7263870239 | test_loss: 5.9426164627 | \n",
      "Epoch: 7911 | train_loss: 116.7262420654 | test_loss: 5.9425940514 | \n",
      "Epoch: 7912 | train_loss: 116.7260818481 | test_loss: 5.9425687790 | \n",
      "Epoch: 7913 | train_loss: 116.7258987427 | test_loss: 5.9425435066 | \n",
      "Epoch: 7914 | train_loss: 116.7257919312 | test_loss: 5.9425201416 | \n",
      "Epoch: 7915 | train_loss: 116.7256240845 | test_loss: 5.9425034523 | \n",
      "Epoch: 7916 | train_loss: 116.7255096436 | test_loss: 5.9424772263 | \n",
      "Epoch: 7917 | train_loss: 116.7253341675 | test_loss: 5.9424562454 | \n",
      "Epoch: 7918 | train_loss: 116.7251968384 | test_loss: 5.9424343109 | \n",
      "Epoch: 7919 | train_loss: 116.7250289917 | test_loss: 5.9424109459 | \n",
      "Epoch: 7920 | train_loss: 116.7248840332 | test_loss: 5.9423956871 | \n",
      "Epoch: 7921 | train_loss: 116.7247085571 | test_loss: 5.9423718452 | \n",
      "Epoch: 7922 | train_loss: 116.7245788574 | test_loss: 5.9423537254 | \n",
      "Epoch: 7923 | train_loss: 116.7244415283 | test_loss: 5.9423313141 | \n",
      "Epoch: 7924 | train_loss: 116.7242736816 | test_loss: 5.9423084259 | \n",
      "Epoch: 7925 | train_loss: 116.7241287231 | test_loss: 5.9422826767 | \n",
      "Epoch: 7926 | train_loss: 116.7239456177 | test_loss: 5.9422636032 | \n",
      "Epoch: 7927 | train_loss: 116.7237854004 | test_loss: 5.9422411919 | \n",
      "Epoch: 7928 | train_loss: 116.7236785889 | test_loss: 5.9422192574 | \n",
      "Epoch: 7929 | train_loss: 116.7235183716 | test_loss: 5.9421935081 | \n",
      "Epoch: 7930 | train_loss: 116.7233657837 | test_loss: 5.9421720505 | \n",
      "Epoch: 7931 | train_loss: 116.7232208252 | test_loss: 5.9421553612 | \n",
      "Epoch: 7932 | train_loss: 116.7230987549 | test_loss: 5.9421296120 | \n",
      "Epoch: 7933 | train_loss: 116.7229232788 | test_loss: 5.9421143532 | \n",
      "Epoch: 7934 | train_loss: 116.7227630615 | test_loss: 5.9420914650 | \n",
      "Epoch: 7935 | train_loss: 116.7226257324 | test_loss: 5.9420695305 | \n",
      "Epoch: 7936 | train_loss: 116.7224578857 | test_loss: 5.9420495033 | \n",
      "Epoch: 7937 | train_loss: 116.7223205566 | test_loss: 5.9420270920 | \n",
      "Epoch: 7938 | train_loss: 116.7221603394 | test_loss: 5.9420051575 | \n",
      "Epoch: 7939 | train_loss: 116.7220306396 | test_loss: 5.9419856071 | \n",
      "Epoch: 7940 | train_loss: 116.7218475342 | test_loss: 5.9419612885 | \n",
      "Epoch: 7941 | train_loss: 116.7217025757 | test_loss: 5.9419398308 | \n",
      "Epoch: 7942 | train_loss: 116.7215347290 | test_loss: 5.9419178963 | \n",
      "Epoch: 7943 | train_loss: 116.7213897705 | test_loss: 5.9418964386 | \n",
      "Epoch: 7944 | train_loss: 116.7212371826 | test_loss: 5.9418764114 | \n",
      "Epoch: 7945 | train_loss: 116.7210998535 | test_loss: 5.9418549538 | \n",
      "Epoch: 7946 | train_loss: 116.7209396362 | test_loss: 5.9418306351 | \n",
      "Epoch: 7947 | train_loss: 116.7207946777 | test_loss: 5.9418058395 | \n",
      "Epoch: 7948 | train_loss: 116.7206420898 | test_loss: 5.9417853355 | \n",
      "Epoch: 7949 | train_loss: 116.7204818726 | test_loss: 5.9417653084 | \n",
      "Epoch: 7950 | train_loss: 116.7203140259 | test_loss: 5.9417376518 | \n",
      "Epoch: 7951 | train_loss: 116.7201766968 | test_loss: 5.9417195320 | \n",
      "Epoch: 7952 | train_loss: 116.7200317383 | test_loss: 5.9416975975 | \n",
      "Epoch: 7953 | train_loss: 116.7198333740 | test_loss: 5.9416766167 | \n",
      "Epoch: 7954 | train_loss: 116.7197036743 | test_loss: 5.9416551590 | \n",
      "Epoch: 7955 | train_loss: 116.7195510864 | test_loss: 5.9416375160 | \n",
      "Epoch: 7956 | train_loss: 116.7193908691 | test_loss: 5.9416084290 | \n",
      "Epoch: 7957 | train_loss: 116.7192306519 | test_loss: 5.9415888786 | \n",
      "Epoch: 7958 | train_loss: 116.7191009521 | test_loss: 5.9415655136 | \n",
      "Epoch: 7959 | train_loss: 116.7189331055 | test_loss: 5.9415397644 | \n",
      "Epoch: 7960 | train_loss: 116.7187957764 | test_loss: 5.9415216446 | \n",
      "Epoch: 7961 | train_loss: 116.7186431885 | test_loss: 5.9414997101 | \n",
      "Epoch: 7962 | train_loss: 116.7184829712 | test_loss: 5.9414782524 | \n",
      "Epoch: 7963 | train_loss: 116.7183227539 | test_loss: 5.9414563179 | \n",
      "Epoch: 7964 | train_loss: 116.7181625366 | test_loss: 5.9414339066 | \n",
      "Epoch: 7965 | train_loss: 116.7180175781 | test_loss: 5.9414091110 | \n",
      "Epoch: 7966 | train_loss: 116.7178497314 | test_loss: 5.9413909912 | \n",
      "Epoch: 7967 | train_loss: 116.7176818848 | test_loss: 5.9413652420 | \n",
      "Epoch: 7968 | train_loss: 116.7175369263 | test_loss: 5.9413471222 | \n",
      "Epoch: 7969 | train_loss: 116.7173767090 | test_loss: 5.9413237572 | \n",
      "Epoch: 7970 | train_loss: 116.7172088623 | test_loss: 5.9412989616 | \n",
      "Epoch: 7971 | train_loss: 116.7170486450 | test_loss: 5.9412784576 | \n",
      "Epoch: 7972 | train_loss: 116.7169265747 | test_loss: 5.9412574768 | \n",
      "Epoch: 7973 | train_loss: 116.7167282104 | test_loss: 5.9412388802 | \n",
      "Epoch: 7974 | train_loss: 116.7165756226 | test_loss: 5.9412169456 | \n",
      "Epoch: 7975 | train_loss: 116.7164230347 | test_loss: 5.9412021637 | \n",
      "Epoch: 7976 | train_loss: 116.7162857056 | test_loss: 5.9411749840 | \n",
      "Epoch: 7977 | train_loss: 116.7161407471 | test_loss: 5.9411535263 | \n",
      "Epoch: 7978 | train_loss: 116.7160034180 | test_loss: 5.9411339760 | \n",
      "Epoch: 7979 | train_loss: 116.7158508301 | test_loss: 5.9411120415 | \n",
      "Epoch: 7980 | train_loss: 116.7156753540 | test_loss: 5.9410862923 | \n",
      "Epoch: 7981 | train_loss: 116.7155303955 | test_loss: 5.9410648346 | \n",
      "Epoch: 7982 | train_loss: 116.7154235840 | test_loss: 5.9410424232 | \n",
      "Epoch: 7983 | train_loss: 116.7152328491 | test_loss: 5.9410181046 | \n",
      "Epoch: 7984 | train_loss: 116.7150878906 | test_loss: 5.9409971237 | \n",
      "Epoch: 7985 | train_loss: 116.7149200439 | test_loss: 5.9409747124 | \n",
      "Epoch: 7986 | train_loss: 116.7147293091 | test_loss: 5.9409551620 | \n",
      "Epoch: 7987 | train_loss: 116.7146224976 | test_loss: 5.9409332275 | \n",
      "Epoch: 7988 | train_loss: 116.7144317627 | test_loss: 5.9409089088 | \n",
      "Epoch: 7989 | train_loss: 116.7143096924 | test_loss: 5.9408888817 | \n",
      "Epoch: 7990 | train_loss: 116.7141189575 | test_loss: 5.9408640862 | \n",
      "Epoch: 7991 | train_loss: 116.7139434814 | test_loss: 5.9408402443 | \n",
      "Epoch: 7992 | train_loss: 116.7137908936 | test_loss: 5.9408173561 | \n",
      "Epoch: 7993 | train_loss: 116.7136459351 | test_loss: 5.9407935143 | \n",
      "Epoch: 7994 | train_loss: 116.7134704590 | test_loss: 5.9407696724 | \n",
      "Epoch: 7995 | train_loss: 116.7133560181 | test_loss: 5.9407515526 | \n",
      "Epoch: 7996 | train_loss: 116.7132339478 | test_loss: 5.9407281876 | \n",
      "Epoch: 7997 | train_loss: 116.7130661011 | test_loss: 5.9407062531 | \n",
      "Epoch: 7998 | train_loss: 116.7129211426 | test_loss: 5.9406852722 | \n",
      "Epoch: 7999 | train_loss: 116.7127838135 | test_loss: 5.9406623840 | \n",
      "Epoch: 8000 | train_loss: 116.7126159668 | test_loss: 5.9406437874 | \n",
      "Epoch: 8001 | train_loss: 116.7124710083 | test_loss: 5.9406194687 | \n",
      "Epoch: 8002 | train_loss: 116.7123107910 | test_loss: 5.9405980110 | \n",
      "Epoch: 8003 | train_loss: 116.7121658325 | test_loss: 5.9405760765 | \n",
      "Epoch: 8004 | train_loss: 116.7120208740 | test_loss: 5.9405546188 | \n",
      "Epoch: 8005 | train_loss: 116.7118759155 | test_loss: 5.9405307770 | \n",
      "Epoch: 8006 | train_loss: 116.7117385864 | test_loss: 5.9405097961 | \n",
      "Epoch: 8007 | train_loss: 116.7115554810 | test_loss: 5.9404869080 | \n",
      "Epoch: 8008 | train_loss: 116.7114181519 | test_loss: 5.9404692650 | \n",
      "Epoch: 8009 | train_loss: 116.7112274170 | test_loss: 5.9404411316 | \n",
      "Epoch: 8010 | train_loss: 116.7110748291 | test_loss: 5.9404201508 | \n",
      "Epoch: 8011 | train_loss: 116.7109451294 | test_loss: 5.9403991699 | \n",
      "Epoch: 8012 | train_loss: 116.7108078003 | test_loss: 5.9403772354 | \n",
      "Epoch: 8013 | train_loss: 116.7106475830 | test_loss: 5.9403600693 | \n",
      "Epoch: 8014 | train_loss: 116.7104797363 | test_loss: 5.9403376579 | \n",
      "Epoch: 8015 | train_loss: 116.7103347778 | test_loss: 5.9403181076 | \n",
      "Epoch: 8016 | train_loss: 116.7101745605 | test_loss: 5.9403004646 | \n",
      "Epoch: 8017 | train_loss: 116.7100372314 | test_loss: 5.9402785301 | \n",
      "Epoch: 8018 | train_loss: 116.7098922729 | test_loss: 5.9402589798 | \n",
      "Epoch: 8019 | train_loss: 116.7097167969 | test_loss: 5.9402379990 | \n",
      "Epoch: 8020 | train_loss: 116.7095794678 | test_loss: 5.9402184486 | \n",
      "Epoch: 8021 | train_loss: 116.7094268799 | test_loss: 5.9401965141 | \n",
      "Epoch: 8022 | train_loss: 116.7092895508 | test_loss: 5.9401707649 | \n",
      "Epoch: 8023 | train_loss: 116.7090988159 | test_loss: 5.9401464462 | \n",
      "Epoch: 8024 | train_loss: 116.7089462280 | test_loss: 5.9401288033 | \n",
      "Epoch: 8025 | train_loss: 116.7087860107 | test_loss: 5.9401049614 | \n",
      "Epoch: 8026 | train_loss: 116.7086334229 | test_loss: 5.9400873184 | \n",
      "Epoch: 8027 | train_loss: 116.7084579468 | test_loss: 5.9400663376 | \n",
      "Epoch: 8028 | train_loss: 116.7083129883 | test_loss: 5.9400424957 | \n",
      "Epoch: 8029 | train_loss: 116.7081375122 | test_loss: 5.9400200844 | \n",
      "Epoch: 8030 | train_loss: 116.7079696655 | test_loss: 5.9399976730 | \n",
      "Epoch: 8031 | train_loss: 116.7078094482 | test_loss: 5.9399724007 | \n",
      "Epoch: 8032 | train_loss: 116.7076644897 | test_loss: 5.9399456978 | \n",
      "Epoch: 8033 | train_loss: 116.7075195312 | test_loss: 5.9399223328 | \n",
      "Epoch: 8034 | train_loss: 116.7073669434 | test_loss: 5.9398980141 | \n",
      "Epoch: 8035 | train_loss: 116.7072067261 | test_loss: 5.9398736954 | \n",
      "Epoch: 8036 | train_loss: 116.7070236206 | test_loss: 5.9398512840 | \n",
      "Epoch: 8037 | train_loss: 116.7068786621 | test_loss: 5.9398264885 | \n",
      "Epoch: 8038 | train_loss: 116.7067260742 | test_loss: 5.9398055077 | \n",
      "Epoch: 8039 | train_loss: 116.7065887451 | test_loss: 5.9397845268 | \n",
      "Epoch: 8040 | train_loss: 116.7064132690 | test_loss: 5.9397597313 | \n",
      "Epoch: 8041 | train_loss: 116.7062377930 | test_loss: 5.9397373199 | \n",
      "Epoch: 8042 | train_loss: 116.7060928345 | test_loss: 5.9397144318 | \n",
      "Epoch: 8043 | train_loss: 116.7059402466 | test_loss: 5.9396967888 | \n",
      "Epoch: 8044 | train_loss: 116.7058029175 | test_loss: 5.9396691322 | \n",
      "Epoch: 8045 | train_loss: 116.7056350708 | test_loss: 5.9396486282 | \n",
      "Epoch: 8046 | train_loss: 116.7054519653 | test_loss: 5.9396266937 | \n",
      "Epoch: 8047 | train_loss: 116.7052612305 | test_loss: 5.9396052361 | \n",
      "Epoch: 8048 | train_loss: 116.7051239014 | test_loss: 5.9395813942 | \n",
      "Epoch: 8049 | train_loss: 116.7049484253 | test_loss: 5.9395604134 | \n",
      "Epoch: 8050 | train_loss: 116.7048263550 | test_loss: 5.9395389557 | \n",
      "Epoch: 8051 | train_loss: 116.7046966553 | test_loss: 5.9395184517 | \n",
      "Epoch: 8052 | train_loss: 116.7045135498 | test_loss: 5.9395008087 | \n",
      "Epoch: 8053 | train_loss: 116.7043762207 | test_loss: 5.9394764900 | \n",
      "Epoch: 8054 | train_loss: 116.7042465210 | test_loss: 5.9394569397 | \n",
      "Epoch: 8055 | train_loss: 116.7041091919 | test_loss: 5.9394402504 | \n",
      "Epoch: 8056 | train_loss: 116.7039260864 | test_loss: 5.9394164085 | \n",
      "Epoch: 8057 | train_loss: 116.7037963867 | test_loss: 5.9393968582 | \n",
      "Epoch: 8058 | train_loss: 116.7036361694 | test_loss: 5.9393754005 | \n",
      "Epoch: 8059 | train_loss: 116.7034835815 | test_loss: 5.9393525124 | \n",
      "Epoch: 8060 | train_loss: 116.7033233643 | test_loss: 5.9393324852 | \n",
      "Epoch: 8061 | train_loss: 116.7031860352 | test_loss: 5.9393062592 | \n",
      "Epoch: 8062 | train_loss: 116.7030105591 | test_loss: 5.9392848015 | \n",
      "Epoch: 8063 | train_loss: 116.7028732300 | test_loss: 5.9392676353 | \n",
      "Epoch: 8064 | train_loss: 116.7027282715 | test_loss: 5.9392466545 | \n",
      "Epoch: 8065 | train_loss: 116.7025680542 | test_loss: 5.9392304420 | \n",
      "Epoch: 8066 | train_loss: 116.7024002075 | test_loss: 5.9392094612 | \n",
      "Epoch: 8067 | train_loss: 116.7022476196 | test_loss: 5.9391856194 | \n",
      "Epoch: 8068 | train_loss: 116.7021102905 | test_loss: 5.9391617775 | \n",
      "Epoch: 8069 | train_loss: 116.7019500732 | test_loss: 5.9391450882 | \n",
      "Epoch: 8070 | train_loss: 116.7018051147 | test_loss: 5.9391231537 | \n",
      "Epoch: 8071 | train_loss: 116.7016372681 | test_loss: 5.9390978813 | \n",
      "Epoch: 8072 | train_loss: 116.7015075684 | test_loss: 5.9390764236 | \n",
      "Epoch: 8073 | train_loss: 116.7013244629 | test_loss: 5.9390559196 | \n",
      "Epoch: 8074 | train_loss: 116.7011947632 | test_loss: 5.9390306473 | \n",
      "Epoch: 8075 | train_loss: 116.7010726929 | test_loss: 5.9390044212 | \n",
      "Epoch: 8076 | train_loss: 116.7009201050 | test_loss: 5.9389863014 | \n",
      "Epoch: 8077 | train_loss: 116.7007751465 | test_loss: 5.9389705658 | \n",
      "Epoch: 8078 | train_loss: 116.7006378174 | test_loss: 5.9389467239 | \n",
      "Epoch: 8079 | train_loss: 116.7004623413 | test_loss: 5.9389276505 | \n",
      "Epoch: 8080 | train_loss: 116.7003402710 | test_loss: 5.9389057159 | \n",
      "Epoch: 8081 | train_loss: 116.7001724243 | test_loss: 5.9388847351 | \n",
      "Epoch: 8082 | train_loss: 116.7000579834 | test_loss: 5.9388628006 | \n",
      "Epoch: 8083 | train_loss: 116.6998825073 | test_loss: 5.9388341904 | \n",
      "Epoch: 8084 | train_loss: 116.6997146606 | test_loss: 5.9388141632 | \n",
      "Epoch: 8085 | train_loss: 116.6995620728 | test_loss: 5.9387955666 | \n",
      "Epoch: 8086 | train_loss: 116.6994018555 | test_loss: 5.9387736320 | \n",
      "Epoch: 8087 | train_loss: 116.6992797852 | test_loss: 5.9387497902 | \n",
      "Epoch: 8088 | train_loss: 116.6990814209 | test_loss: 5.9387311935 | \n",
      "Epoch: 8089 | train_loss: 116.6989364624 | test_loss: 5.9387078285 | \n",
      "Epoch: 8090 | train_loss: 116.6987762451 | test_loss: 5.9386854172 | \n",
      "Epoch: 8091 | train_loss: 116.6985931396 | test_loss: 5.9386687279 | \n",
      "Epoch: 8092 | train_loss: 116.6984558105 | test_loss: 5.9386448860 | \n",
      "Epoch: 8093 | train_loss: 116.6983108521 | test_loss: 5.9386262894 | \n",
      "Epoch: 8094 | train_loss: 116.6981506348 | test_loss: 5.9386014938 | \n",
      "Epoch: 8095 | train_loss: 116.6979751587 | test_loss: 5.9385814667 | \n",
      "Epoch: 8096 | train_loss: 116.6978302002 | test_loss: 5.9385614395 | \n",
      "Epoch: 8097 | train_loss: 116.6976547241 | test_loss: 5.9385375977 | \n",
      "Epoch: 8098 | train_loss: 116.6974868774 | test_loss: 5.9385137558 | \n",
      "Epoch: 8099 | train_loss: 116.6973648071 | test_loss: 5.9384922981 | \n",
      "Epoch: 8100 | train_loss: 116.6972198486 | test_loss: 5.9384708405 | \n",
      "Epoch: 8101 | train_loss: 116.6970825195 | test_loss: 5.9384508133 | \n",
      "Epoch: 8102 | train_loss: 116.6969451904 | test_loss: 5.9384245872 | \n",
      "Epoch: 8103 | train_loss: 116.6967697144 | test_loss: 5.9384002686 | \n",
      "Epoch: 8104 | train_loss: 116.6965942383 | test_loss: 5.9383807182 | \n",
      "Epoch: 8105 | train_loss: 116.6964416504 | test_loss: 5.9383573532 | \n",
      "Epoch: 8106 | train_loss: 116.6962966919 | test_loss: 5.9383387566 | \n",
      "Epoch: 8107 | train_loss: 116.6961364746 | test_loss: 5.9383149147 | \n",
      "Epoch: 8108 | train_loss: 116.6959838867 | test_loss: 5.9382920265 | \n",
      "Epoch: 8109 | train_loss: 116.6958312988 | test_loss: 5.9382696152 | \n",
      "Epoch: 8110 | train_loss: 116.6956863403 | test_loss: 5.9382495880 | \n",
      "Epoch: 8111 | train_loss: 116.6955337524 | test_loss: 5.9382266998 | \n",
      "Epoch: 8112 | train_loss: 116.6954040527 | test_loss: 5.9382038116 | \n",
      "Epoch: 8113 | train_loss: 116.6952056885 | test_loss: 5.9381847382 | \n",
      "Epoch: 8114 | train_loss: 116.6950607300 | test_loss: 5.9381642342 | \n",
      "Epoch: 8115 | train_loss: 116.6949234009 | test_loss: 5.9381422997 | \n",
      "Epoch: 8116 | train_loss: 116.6947784424 | test_loss: 5.9381208420 | \n",
      "Epoch: 8117 | train_loss: 116.6946105957 | test_loss: 5.9380936623 | \n",
      "Epoch: 8118 | train_loss: 116.6944503784 | test_loss: 5.9380784035 | \n",
      "Epoch: 8119 | train_loss: 116.6942901611 | test_loss: 5.9380559921 | \n",
      "Epoch: 8120 | train_loss: 116.6941223145 | test_loss: 5.9380311966 | \n",
      "Epoch: 8121 | train_loss: 116.6939620972 | test_loss: 5.9380106926 | \n",
      "Epoch: 8122 | train_loss: 116.6938018799 | test_loss: 5.9379906654 | \n",
      "Epoch: 8123 | train_loss: 116.6936264038 | test_loss: 5.9379658699 | \n",
      "Epoch: 8124 | train_loss: 116.6934738159 | test_loss: 5.9379405975 | \n",
      "Epoch: 8125 | train_loss: 116.6933364868 | test_loss: 5.9379215240 | \n",
      "Epoch: 8126 | train_loss: 116.6931915283 | test_loss: 5.9379005432 | \n",
      "Epoch: 8127 | train_loss: 116.6930389404 | test_loss: 5.9378724098 | \n",
      "Epoch: 8128 | train_loss: 116.6928710938 | test_loss: 5.9378576279 | \n",
      "Epoch: 8129 | train_loss: 116.6927566528 | test_loss: 5.9378271103 | \n",
      "Epoch: 8130 | train_loss: 116.6925811768 | test_loss: 5.9378037453 | \n",
      "Epoch: 8131 | train_loss: 116.6924133301 | test_loss: 5.9377894402 | \n",
      "Epoch: 8132 | train_loss: 116.6922836304 | test_loss: 5.9377679825 | \n",
      "Epoch: 8133 | train_loss: 116.6921157837 | test_loss: 5.9377417564 | \n",
      "Epoch: 8134 | train_loss: 116.6919860840 | test_loss: 5.9377207756 | \n",
      "Epoch: 8135 | train_loss: 116.6918334961 | test_loss: 5.9377007484 | \n",
      "Epoch: 8136 | train_loss: 116.6917114258 | test_loss: 5.9376826286 | \n",
      "Epoch: 8137 | train_loss: 116.6915893555 | test_loss: 5.9376616478 | \n",
      "Epoch: 8138 | train_loss: 116.6914367676 | test_loss: 5.9376368523 | \n",
      "Epoch: 8139 | train_loss: 116.6912689209 | test_loss: 5.9376134872 | \n",
      "Epoch: 8140 | train_loss: 116.6911163330 | test_loss: 5.9375867844 | \n",
      "Epoch: 8141 | train_loss: 116.6909637451 | test_loss: 5.9375720024 | \n",
      "Epoch: 8142 | train_loss: 116.6908187866 | test_loss: 5.9375472069 | \n",
      "Epoch: 8143 | train_loss: 116.6906433105 | test_loss: 5.9375224113 | \n",
      "Epoch: 8144 | train_loss: 116.6905364990 | test_loss: 5.9375047684 | \n",
      "Epoch: 8145 | train_loss: 116.6903839111 | test_loss: 5.9374823570 | \n",
      "Epoch: 8146 | train_loss: 116.6902313232 | test_loss: 5.9374632835 | \n",
      "Epoch: 8147 | train_loss: 116.6900863647 | test_loss: 5.9374403954 | \n",
      "Epoch: 8148 | train_loss: 116.6899337769 | test_loss: 5.9374213219 | \n",
      "Epoch: 8149 | train_loss: 116.6897659302 | test_loss: 5.9374017715 | \n",
      "Epoch: 8150 | train_loss: 116.6896057129 | test_loss: 5.9373817444 | \n",
      "Epoch: 8151 | train_loss: 116.6894607544 | test_loss: 5.9373631477 | \n",
      "Epoch: 8152 | train_loss: 116.6893081665 | test_loss: 5.9373359680 | \n",
      "Epoch: 8153 | train_loss: 116.6891479492 | test_loss: 5.9373145103 | \n",
      "Epoch: 8154 | train_loss: 116.6889648438 | test_loss: 5.9372901917 | \n",
      "Epoch: 8155 | train_loss: 116.6888122559 | test_loss: 5.9372673035 | \n",
      "Epoch: 8156 | train_loss: 116.6886596680 | test_loss: 5.9372401237 | \n",
      "Epoch: 8157 | train_loss: 116.6885223389 | test_loss: 5.9372196198 | \n",
      "Epoch: 8158 | train_loss: 116.6883468628 | test_loss: 5.9372005463 | \n",
      "Epoch: 8159 | train_loss: 116.6882095337 | test_loss: 5.9371771812 | \n",
      "Epoch: 8160 | train_loss: 116.6880645752 | test_loss: 5.9371595383 | \n",
      "Epoch: 8161 | train_loss: 116.6878967285 | test_loss: 5.9371352196 | \n",
      "Epoch: 8162 | train_loss: 116.6877670288 | test_loss: 5.9371147156 | \n",
      "Epoch: 8163 | train_loss: 116.6875991821 | test_loss: 5.9370937347 | \n",
      "Epoch: 8164 | train_loss: 116.6874389648 | test_loss: 5.9370751381 | \n",
      "Epoch: 8165 | train_loss: 116.6873245239 | test_loss: 5.9370532036 | \n",
      "Epoch: 8166 | train_loss: 116.6871643066 | test_loss: 5.9370326996 | \n",
      "Epoch: 8167 | train_loss: 116.6870498657 | test_loss: 5.9370098114 | \n",
      "Epoch: 8168 | train_loss: 116.6868820190 | test_loss: 5.9369959831 | \n",
      "Epoch: 8169 | train_loss: 116.6867446899 | test_loss: 5.9369740486 | \n",
      "Epoch: 8170 | train_loss: 116.6865844727 | test_loss: 5.9369506836 | \n",
      "Epoch: 8171 | train_loss: 116.6864166260 | test_loss: 5.9369297028 | \n",
      "Epoch: 8172 | train_loss: 116.6862487793 | test_loss: 5.9369053841 | \n",
      "Epoch: 8173 | train_loss: 116.6861419678 | test_loss: 5.9368853569 | \n",
      "Epoch: 8174 | train_loss: 116.6860046387 | test_loss: 5.9368577003 | \n",
      "Epoch: 8175 | train_loss: 116.6858291626 | test_loss: 5.9368395805 | \n",
      "Epoch: 8176 | train_loss: 116.6856842041 | test_loss: 5.9368214607 | \n",
      "Epoch: 8177 | train_loss: 116.6855163574 | test_loss: 5.9368000031 | \n",
      "Epoch: 8178 | train_loss: 116.6853942871 | test_loss: 5.9367737770 | \n",
      "Epoch: 8179 | train_loss: 116.6852569580 | test_loss: 5.9367537498 | \n",
      "Epoch: 8180 | train_loss: 116.6851272583 | test_loss: 5.9367327690 | \n",
      "Epoch: 8181 | train_loss: 116.6849899292 | test_loss: 5.9367089272 | \n",
      "Epoch: 8182 | train_loss: 116.6848449707 | test_loss: 5.9366865158 | \n",
      "Epoch: 8183 | train_loss: 116.6846771240 | test_loss: 5.9366664886 | \n",
      "Epoch: 8184 | train_loss: 116.6845321655 | test_loss: 5.9366440773 | \n",
      "Epoch: 8185 | train_loss: 116.6843643188 | test_loss: 5.9366216660 | \n",
      "Epoch: 8186 | train_loss: 116.6841888428 | test_loss: 5.9366016388 | \n",
      "Epoch: 8187 | train_loss: 116.6840362549 | test_loss: 5.9365801811 | \n",
      "Epoch: 8188 | train_loss: 116.6839141846 | test_loss: 5.9365611076 | \n",
      "Epoch: 8189 | train_loss: 116.6837387085 | test_loss: 5.9365348816 | \n",
      "Epoch: 8190 | train_loss: 116.6835708618 | test_loss: 5.9365119934 | \n",
      "Epoch: 8191 | train_loss: 116.6833953857 | test_loss: 5.9364910126 | \n",
      "Epoch: 8192 | train_loss: 116.6832580566 | test_loss: 5.9364709854 | \n",
      "Epoch: 8193 | train_loss: 116.6830825806 | test_loss: 5.9364495277 | \n",
      "Epoch: 8194 | train_loss: 116.6829757690 | test_loss: 5.9364256859 | \n",
      "Epoch: 8195 | train_loss: 116.6828384399 | test_loss: 5.9364018440 | \n",
      "Epoch: 8196 | train_loss: 116.6826629639 | test_loss: 5.9363822937 | \n",
      "Epoch: 8197 | train_loss: 116.6825027466 | test_loss: 5.9363594055 | \n",
      "Epoch: 8198 | train_loss: 116.6823730469 | test_loss: 5.9363389015 | \n",
      "Epoch: 8199 | train_loss: 116.6822280884 | test_loss: 5.9363203049 | \n",
      "Epoch: 8200 | train_loss: 116.6820983887 | test_loss: 5.9362959862 | \n",
      "Epoch: 8201 | train_loss: 116.6819229126 | test_loss: 5.9362769127 | \n",
      "Epoch: 8202 | train_loss: 116.6817398071 | test_loss: 5.9362549782 | \n",
      "Epoch: 8203 | train_loss: 116.6816024780 | test_loss: 5.9362378120 | \n",
      "Epoch: 8204 | train_loss: 116.6814651489 | test_loss: 5.9362120628 | \n",
      "Epoch: 8205 | train_loss: 116.6812896729 | test_loss: 5.9361934662 | \n",
      "Epoch: 8206 | train_loss: 116.6811523438 | test_loss: 5.9361739159 | \n",
      "Epoch: 8207 | train_loss: 116.6810073853 | test_loss: 5.9361515045 | \n",
      "Epoch: 8208 | train_loss: 116.6808166504 | test_loss: 5.9361329079 | \n",
      "Epoch: 8209 | train_loss: 116.6806945801 | test_loss: 5.9361100197 | \n",
      "Epoch: 8210 | train_loss: 116.6805953979 | test_loss: 5.9360866547 | \n",
      "Epoch: 8211 | train_loss: 116.6804122925 | test_loss: 5.9360651970 | \n",
      "Epoch: 8212 | train_loss: 116.6802673340 | test_loss: 5.9360437393 | \n",
      "Epoch: 8213 | train_loss: 116.6801376343 | test_loss: 5.9360241890 | \n",
      "Epoch: 8214 | train_loss: 116.6799697876 | test_loss: 5.9360008240 | \n",
      "Epoch: 8215 | train_loss: 116.6798324585 | test_loss: 5.9359779358 | \n",
      "Epoch: 8216 | train_loss: 116.6796875000 | test_loss: 5.9359574318 | \n",
      "Epoch: 8217 | train_loss: 116.6795425415 | test_loss: 5.9359374046 | \n",
      "Epoch: 8218 | train_loss: 116.6794052124 | test_loss: 5.9359111786 | \n",
      "Epoch: 8219 | train_loss: 116.6792373657 | test_loss: 5.9358916283 | \n",
      "Epoch: 8220 | train_loss: 116.6790695190 | test_loss: 5.9358701706 | \n",
      "Epoch: 8221 | train_loss: 116.6789474487 | test_loss: 5.9358501434 | \n",
      "Epoch: 8222 | train_loss: 116.6788101196 | test_loss: 5.9358258247 | \n",
      "Epoch: 8223 | train_loss: 116.6786193848 | test_loss: 5.9358024597 | \n",
      "Epoch: 8224 | train_loss: 116.6784591675 | test_loss: 5.9357800484 | \n",
      "Epoch: 8225 | train_loss: 116.6783218384 | test_loss: 5.9357566833 | \n",
      "Epoch: 8226 | train_loss: 116.6781539917 | test_loss: 5.9357342720 | \n",
      "Epoch: 8227 | train_loss: 116.6780014038 | test_loss: 5.9357109070 | \n",
      "Epoch: 8228 | train_loss: 116.6778335571 | test_loss: 5.9356870651 | \n",
      "Epoch: 8229 | train_loss: 116.6776733398 | test_loss: 5.9356608391 | \n",
      "Epoch: 8230 | train_loss: 116.6775131226 | test_loss: 5.9356369972 | \n",
      "Epoch: 8231 | train_loss: 116.6773605347 | test_loss: 5.9356126785 | \n",
      "Epoch: 8232 | train_loss: 116.6772232056 | test_loss: 5.9355878830 | \n",
      "Epoch: 8233 | train_loss: 116.6770324707 | test_loss: 5.9355664253 | \n",
      "Epoch: 8234 | train_loss: 116.6768951416 | test_loss: 5.9355463982 | \n",
      "Epoch: 8235 | train_loss: 116.6767272949 | test_loss: 5.9355254173 | \n",
      "Epoch: 8236 | train_loss: 116.6765899658 | test_loss: 5.9355039597 | \n",
      "Epoch: 8237 | train_loss: 116.6764678955 | test_loss: 5.9354834557 | \n",
      "Epoch: 8238 | train_loss: 116.6762695312 | test_loss: 5.9354648590 | \n",
      "Epoch: 8239 | train_loss: 116.6761550903 | test_loss: 5.9354424477 | \n",
      "Epoch: 8240 | train_loss: 116.6760177612 | test_loss: 5.9354176521 | \n",
      "Epoch: 8241 | train_loss: 116.6758575439 | test_loss: 5.9353923798 | \n",
      "Epoch: 8242 | train_loss: 116.6757125854 | test_loss: 5.9353718758 | \n",
      "Epoch: 8243 | train_loss: 116.6755599976 | test_loss: 5.9353499413 | \n",
      "Epoch: 8244 | train_loss: 116.6754074097 | test_loss: 5.9353289604 | \n",
      "Epoch: 8245 | train_loss: 116.6752319336 | test_loss: 5.9353084564 | \n",
      "Epoch: 8246 | train_loss: 116.6751098633 | test_loss: 5.9352869987 | \n",
      "Epoch: 8247 | train_loss: 116.6749572754 | test_loss: 5.9352641106 | \n",
      "Epoch: 8248 | train_loss: 116.6748275757 | test_loss: 5.9352412224 | \n",
      "Epoch: 8249 | train_loss: 116.6746902466 | test_loss: 5.9352226257 | \n",
      "Epoch: 8250 | train_loss: 116.6744995117 | test_loss: 5.9351997375 | \n",
      "Epoch: 8251 | train_loss: 116.6743850708 | test_loss: 5.9351744652 | \n",
      "Epoch: 8252 | train_loss: 116.6742782593 | test_loss: 5.9351530075 | \n",
      "Epoch: 8253 | train_loss: 116.6740951538 | test_loss: 5.9351320267 | \n",
      "Epoch: 8254 | train_loss: 116.6739578247 | test_loss: 5.9351129532 | \n",
      "Epoch: 8255 | train_loss: 116.6738281250 | test_loss: 5.9350910187 | \n",
      "Epoch: 8256 | train_loss: 116.6736602783 | test_loss: 5.9350700378 | \n",
      "Epoch: 8257 | train_loss: 116.6735229492 | test_loss: 5.9350457191 | \n",
      "Epoch: 8258 | train_loss: 116.6733856201 | test_loss: 5.9350242615 | \n",
      "Epoch: 8259 | train_loss: 116.6732330322 | test_loss: 5.9350037575 | \n",
      "Epoch: 8260 | train_loss: 116.6730880737 | test_loss: 5.9349827766 | \n",
      "Epoch: 8261 | train_loss: 116.6729202271 | test_loss: 5.9349651337 | \n",
      "Epoch: 8262 | train_loss: 116.6727752686 | test_loss: 5.9349393845 | \n",
      "Epoch: 8263 | train_loss: 116.6726303101 | test_loss: 5.9349145889 | \n",
      "Epoch: 8264 | train_loss: 116.6724777222 | test_loss: 5.9348974228 | \n",
      "Epoch: 8265 | train_loss: 116.6723327637 | test_loss: 5.9348769188 | \n",
      "Epoch: 8266 | train_loss: 116.6721954346 | test_loss: 5.9348564148 | \n",
      "Epoch: 8267 | train_loss: 116.6720581055 | test_loss: 5.9348378181 | \n",
      "Epoch: 8268 | train_loss: 116.6718750000 | test_loss: 5.9348173141 | \n",
      "Epoch: 8269 | train_loss: 116.6717300415 | test_loss: 5.9347963333 | \n",
      "Epoch: 8270 | train_loss: 116.6715698242 | test_loss: 5.9347753525 | \n",
      "Epoch: 8271 | train_loss: 116.6714172363 | test_loss: 5.9347553253 | \n",
      "Epoch: 8272 | train_loss: 116.6712417603 | test_loss: 5.9347367287 | \n",
      "Epoch: 8273 | train_loss: 116.6711120605 | test_loss: 5.9347133636 | \n",
      "Epoch: 8274 | train_loss: 116.6709671021 | test_loss: 5.9346904755 | \n",
      "Epoch: 8275 | train_loss: 116.6708221436 | test_loss: 5.9346704483 | \n",
      "Epoch: 8276 | train_loss: 116.6706466675 | test_loss: 5.9346423149 | \n",
      "Epoch: 8277 | train_loss: 116.6704864502 | test_loss: 5.9346241951 | \n",
      "Epoch: 8278 | train_loss: 116.6703643799 | test_loss: 5.9345946312 | \n",
      "Epoch: 8279 | train_loss: 116.6701889038 | test_loss: 5.9345750809 | \n",
      "Epoch: 8280 | train_loss: 116.6700515747 | test_loss: 5.9345536232 | \n",
      "Epoch: 8281 | train_loss: 116.6698989868 | test_loss: 5.9345269203 | \n",
      "Epoch: 8282 | train_loss: 116.6697158813 | test_loss: 5.9345035553 | \n",
      "Epoch: 8283 | train_loss: 116.6695632935 | test_loss: 5.9344787598 | \n",
      "Epoch: 8284 | train_loss: 116.6694030762 | test_loss: 5.9344568253 | \n",
      "Epoch: 8285 | train_loss: 116.6692581177 | test_loss: 5.9344334602 | \n",
      "Epoch: 8286 | train_loss: 116.6690826416 | test_loss: 5.9344120026 | \n",
      "Epoch: 8287 | train_loss: 116.6689376831 | test_loss: 5.9343962669 | \n",
      "Epoch: 8288 | train_loss: 116.6688003540 | test_loss: 5.9343724251 | \n",
      "Epoch: 8289 | train_loss: 116.6686401367 | test_loss: 5.9343504906 | \n",
      "Epoch: 8290 | train_loss: 116.6684951782 | test_loss: 5.9343276024 | \n",
      "Epoch: 8291 | train_loss: 116.6683502197 | test_loss: 5.9343094826 | \n",
      "Epoch: 8292 | train_loss: 116.6682052612 | test_loss: 5.9342861176 | \n",
      "Epoch: 8293 | train_loss: 116.6680755615 | test_loss: 5.9342646599 | \n",
      "Epoch: 8294 | train_loss: 116.6679000854 | test_loss: 5.9342408180 | \n",
      "Epoch: 8295 | train_loss: 116.6677474976 | test_loss: 5.9342198372 | \n",
      "Epoch: 8296 | train_loss: 116.6675796509 | test_loss: 5.9341998100 | \n",
      "Epoch: 8297 | train_loss: 116.6674346924 | test_loss: 5.9341754913 | \n",
      "Epoch: 8298 | train_loss: 116.6672744751 | test_loss: 5.9341540337 | \n",
      "Epoch: 8299 | train_loss: 116.6670989990 | test_loss: 5.9341325760 | \n",
      "Epoch: 8300 | train_loss: 116.6669692993 | test_loss: 5.9341125488 | \n",
      "Epoch: 8301 | train_loss: 116.6668395996 | test_loss: 5.9340882301 | \n",
      "Epoch: 8302 | train_loss: 116.6666564941 | test_loss: 5.9340643883 | \n",
      "Epoch: 8303 | train_loss: 116.6665115356 | test_loss: 5.9340400696 | \n",
      "Epoch: 8304 | train_loss: 116.6663665771 | test_loss: 5.9340171814 | \n",
      "Epoch: 8305 | train_loss: 116.6662292480 | test_loss: 5.9339990616 | \n",
      "Epoch: 8306 | train_loss: 116.6660842896 | test_loss: 5.9339780807 | \n",
      "Epoch: 8307 | train_loss: 116.6659545898 | test_loss: 5.9339537621 | \n",
      "Epoch: 8308 | train_loss: 116.6658020020 | test_loss: 5.9339365959 | \n",
      "Epoch: 8309 | train_loss: 116.6656951904 | test_loss: 5.9339170456 | \n",
      "Epoch: 8310 | train_loss: 116.6655273438 | test_loss: 5.9338932037 | \n",
      "Epoch: 8311 | train_loss: 116.6653976440 | test_loss: 5.9338712692 | \n",
      "Epoch: 8312 | train_loss: 116.6652221680 | test_loss: 5.9338526726 | \n",
      "Epoch: 8313 | train_loss: 116.6650695801 | test_loss: 5.9338335991 | \n",
      "Epoch: 8314 | train_loss: 116.6649398804 | test_loss: 5.9338088036 | \n",
      "Epoch: 8315 | train_loss: 116.6647949219 | test_loss: 5.9337863922 | \n",
      "Epoch: 8316 | train_loss: 116.6646270752 | test_loss: 5.9337677956 | \n",
      "Epoch: 8317 | train_loss: 116.6645050049 | test_loss: 5.9337434769 | \n",
      "Epoch: 8318 | train_loss: 116.6643524170 | test_loss: 5.9337239265 | \n",
      "Epoch: 8319 | train_loss: 116.6641998291 | test_loss: 5.9337005615 | \n",
      "Epoch: 8320 | train_loss: 116.6640701294 | test_loss: 5.9336833954 | \n",
      "Epoch: 8321 | train_loss: 116.6639175415 | test_loss: 5.9336590767 | \n",
      "Epoch: 8322 | train_loss: 116.6637802124 | test_loss: 5.9336400032 | \n",
      "Epoch: 8323 | train_loss: 116.6636505127 | test_loss: 5.9336180687 | \n",
      "Epoch: 8324 | train_loss: 116.6634445190 | test_loss: 5.9335961342 | \n",
      "Epoch: 8325 | train_loss: 116.6632995605 | test_loss: 5.9335694313 | \n",
      "Epoch: 8326 | train_loss: 116.6631622314 | test_loss: 5.9335398674 | \n",
      "Epoch: 8327 | train_loss: 116.6629867554 | test_loss: 5.9335179329 | \n",
      "Epoch: 8328 | train_loss: 116.6628417969 | test_loss: 5.9334964752 | \n",
      "Epoch: 8329 | train_loss: 116.6626739502 | test_loss: 5.9334754944 | \n",
      "Epoch: 8330 | train_loss: 116.6625595093 | test_loss: 5.9334545135 | \n",
      "Epoch: 8331 | train_loss: 116.6624145508 | test_loss: 5.9334316254 | \n",
      "Epoch: 8332 | train_loss: 116.6622543335 | test_loss: 5.9334082603 | \n",
      "Epoch: 8333 | train_loss: 116.6621093750 | test_loss: 5.9333915710 | \n",
      "Epoch: 8334 | train_loss: 116.6619644165 | test_loss: 5.9333686829 | \n",
      "Epoch: 8335 | train_loss: 116.6617889404 | test_loss: 5.9333429337 | \n",
      "Epoch: 8336 | train_loss: 116.6616287231 | test_loss: 5.9333257675 | \n",
      "Epoch: 8337 | train_loss: 116.6615066528 | test_loss: 5.9332990646 | \n",
      "Epoch: 8338 | train_loss: 116.6613464355 | test_loss: 5.9332756996 | \n",
      "Epoch: 8339 | train_loss: 116.6612243652 | test_loss: 5.9332509041 | \n",
      "Epoch: 8340 | train_loss: 116.6610565186 | test_loss: 5.9332299232 | \n",
      "Epoch: 8341 | train_loss: 116.6608657837 | test_loss: 5.9332032204 | \n",
      "Epoch: 8342 | train_loss: 116.6607589722 | test_loss: 5.9331803322 | \n",
      "Epoch: 8343 | train_loss: 116.6606216431 | test_loss: 5.9331598282 | \n",
      "Epoch: 8344 | train_loss: 116.6604766846 | test_loss: 5.9331417084 | \n",
      "Epoch: 8345 | train_loss: 116.6602935791 | test_loss: 5.9331159592 | \n",
      "Epoch: 8346 | train_loss: 116.6601562500 | test_loss: 5.9330940247 | \n",
      "Epoch: 8347 | train_loss: 116.6600036621 | test_loss: 5.9330682755 | \n",
      "Epoch: 8348 | train_loss: 116.6598129272 | test_loss: 5.9330492020 | \n",
      "Epoch: 8349 | train_loss: 116.6596755981 | test_loss: 5.9330234528 | \n",
      "Epoch: 8350 | train_loss: 116.6595764160 | test_loss: 5.9330048561 | \n",
      "Epoch: 8351 | train_loss: 116.6594314575 | test_loss: 5.9329814911 | \n",
      "Epoch: 8352 | train_loss: 116.6592864990 | test_loss: 5.9329638481 | \n",
      "Epoch: 8353 | train_loss: 116.6591339111 | test_loss: 5.9329419136 | \n",
      "Epoch: 8354 | train_loss: 116.6589431763 | test_loss: 5.9329204559 | \n",
      "Epoch: 8355 | train_loss: 116.6588211060 | test_loss: 5.9328999519 | \n",
      "Epoch: 8356 | train_loss: 116.6586532593 | test_loss: 5.9328742027 | \n",
      "Epoch: 8357 | train_loss: 116.6584854126 | test_loss: 5.9328546524 | \n",
      "Epoch: 8358 | train_loss: 116.6583786011 | test_loss: 5.9328327179 | \n",
      "Epoch: 8359 | train_loss: 116.6582031250 | test_loss: 5.9328098297 | \n",
      "Epoch: 8360 | train_loss: 116.6580657959 | test_loss: 5.9327898026 | \n",
      "Epoch: 8361 | train_loss: 116.6579208374 | test_loss: 5.9327678680 | \n",
      "Epoch: 8362 | train_loss: 116.6577758789 | test_loss: 5.9327459335 | \n",
      "Epoch: 8363 | train_loss: 116.6576232910 | test_loss: 5.9327254295 | \n",
      "Epoch: 8364 | train_loss: 116.6574478149 | test_loss: 5.9327039719 | \n",
      "Epoch: 8365 | train_loss: 116.6573333740 | test_loss: 5.9326820374 | \n",
      "Epoch: 8366 | train_loss: 116.6571807861 | test_loss: 5.9326581955 | \n",
      "Epoch: 8367 | train_loss: 116.6570434570 | test_loss: 5.9326386452 | \n",
      "Epoch: 8368 | train_loss: 116.6568527222 | test_loss: 5.9326190948 | \n",
      "Epoch: 8369 | train_loss: 116.6567306519 | test_loss: 5.9325952530 | \n",
      "Epoch: 8370 | train_loss: 116.6566009521 | test_loss: 5.9325695038 | \n",
      "Epoch: 8371 | train_loss: 116.6564025879 | test_loss: 5.9325561523 | \n",
      "Epoch: 8372 | train_loss: 116.6562576294 | test_loss: 5.9325346947 | \n",
      "Epoch: 8373 | train_loss: 116.6561126709 | test_loss: 5.9325098991 | \n",
      "Epoch: 8374 | train_loss: 116.6559753418 | test_loss: 5.9324927330 | \n",
      "Epoch: 8375 | train_loss: 116.6558227539 | test_loss: 5.9324665070 | \n",
      "Epoch: 8376 | train_loss: 116.6556625366 | test_loss: 5.9324464798 | \n",
      "Epoch: 8377 | train_loss: 116.6555404663 | test_loss: 5.9324231148 | \n",
      "Epoch: 8378 | train_loss: 116.6553802490 | test_loss: 5.9324035645 | \n",
      "Epoch: 8379 | train_loss: 116.6552429199 | test_loss: 5.9323854446 | \n",
      "Epoch: 8380 | train_loss: 116.6550903320 | test_loss: 5.9323582649 | \n",
      "Epoch: 8381 | train_loss: 116.6548995972 | test_loss: 5.9323358536 | \n",
      "Epoch: 8382 | train_loss: 116.6547470093 | test_loss: 5.9323158264 | \n",
      "Epoch: 8383 | train_loss: 116.6546020508 | test_loss: 5.9322981834 | \n",
      "Epoch: 8384 | train_loss: 116.6544342041 | test_loss: 5.9322748184 | \n",
      "Epoch: 8385 | train_loss: 116.6542968750 | test_loss: 5.9322495461 | \n",
      "Epoch: 8386 | train_loss: 116.6541519165 | test_loss: 5.9322299957 | \n",
      "Epoch: 8387 | train_loss: 116.6539840698 | test_loss: 5.9322123528 | \n",
      "Epoch: 8388 | train_loss: 116.6538467407 | test_loss: 5.9321861267 | \n",
      "Epoch: 8389 | train_loss: 116.6537322998 | test_loss: 5.9321651459 | \n",
      "Epoch: 8390 | train_loss: 116.6535568237 | test_loss: 5.9321427345 | \n",
      "Epoch: 8391 | train_loss: 116.6533889771 | test_loss: 5.9321203232 | \n",
      "Epoch: 8392 | train_loss: 116.6532440186 | test_loss: 5.9320988655 | \n",
      "Epoch: 8393 | train_loss: 116.6531219482 | test_loss: 5.9320740700 | \n",
      "Epoch: 8394 | train_loss: 116.6529846191 | test_loss: 5.9320526123 | \n",
      "Epoch: 8395 | train_loss: 116.6528320312 | test_loss: 5.9320292473 | \n",
      "Epoch: 8396 | train_loss: 116.6526641846 | test_loss: 5.9320082664 | \n",
      "Epoch: 8397 | train_loss: 116.6525421143 | test_loss: 5.9319868088 | \n",
      "Epoch: 8398 | train_loss: 116.6523666382 | test_loss: 5.9319624901 | \n",
      "Epoch: 8399 | train_loss: 116.6522140503 | test_loss: 5.9319391251 | \n",
      "Epoch: 8400 | train_loss: 116.6520690918 | test_loss: 5.9319171906 | \n",
      "Epoch: 8401 | train_loss: 116.6519241333 | test_loss: 5.9318909645 | \n",
      "Epoch: 8402 | train_loss: 116.6517486572 | test_loss: 5.9318690300 | \n",
      "Epoch: 8403 | train_loss: 116.6515808105 | test_loss: 5.9318480492 | \n",
      "Epoch: 8404 | train_loss: 116.6514587402 | test_loss: 5.9318275452 | \n",
      "Epoch: 8405 | train_loss: 116.6513290405 | test_loss: 5.9318065643 | \n",
      "Epoch: 8406 | train_loss: 116.6511306763 | test_loss: 5.9317822456 | \n",
      "Epoch: 8407 | train_loss: 116.6510238647 | test_loss: 5.9317641258 | \n",
      "Epoch: 8408 | train_loss: 116.6508483887 | test_loss: 5.9317383766 | \n",
      "Epoch: 8409 | train_loss: 116.6507492065 | test_loss: 5.9317145348 | \n",
      "Epoch: 8410 | train_loss: 116.6505737305 | test_loss: 5.9316949844 | \n",
      "Epoch: 8411 | train_loss: 116.6504898071 | test_loss: 5.9316730499 | \n",
      "Epoch: 8412 | train_loss: 116.6503143311 | test_loss: 5.9316558838 | \n",
      "Epoch: 8413 | train_loss: 116.6501922607 | test_loss: 5.9316301346 | \n",
      "Epoch: 8414 | train_loss: 116.6500549316 | test_loss: 5.9316139221 | \n",
      "Epoch: 8415 | train_loss: 116.6499099731 | test_loss: 5.9315891266 | \n",
      "Epoch: 8416 | train_loss: 116.6497268677 | test_loss: 5.9315662384 | \n",
      "Epoch: 8417 | train_loss: 116.6495666504 | test_loss: 5.9315466881 | \n",
      "Epoch: 8418 | train_loss: 116.6494140625 | test_loss: 5.9315214157 | \n",
      "Epoch: 8419 | train_loss: 116.6492691040 | test_loss: 5.9315004349 | \n",
      "Epoch: 8420 | train_loss: 116.6491088867 | test_loss: 5.9314789772 | \n",
      "Epoch: 8421 | train_loss: 116.6489715576 | test_loss: 5.9314570427 | \n",
      "Epoch: 8422 | train_loss: 116.6487960815 | test_loss: 5.9314393997 | \n",
      "Epoch: 8423 | train_loss: 116.6486511230 | test_loss: 5.9314174652 | \n",
      "Epoch: 8424 | train_loss: 116.6485290527 | test_loss: 5.9313988686 | \n",
      "Epoch: 8425 | train_loss: 116.6483764648 | test_loss: 5.9313783646 | \n",
      "Epoch: 8426 | train_loss: 116.6482315063 | test_loss: 5.9313592911 | \n",
      "Epoch: 8427 | train_loss: 116.6480636597 | test_loss: 5.9313383102 | \n",
      "Epoch: 8428 | train_loss: 116.6479187012 | test_loss: 5.9313163757 | \n",
      "Epoch: 8429 | train_loss: 116.6477813721 | test_loss: 5.9312925339 | \n",
      "Epoch: 8430 | train_loss: 116.6476287842 | test_loss: 5.9312658310 | \n",
      "Epoch: 8431 | train_loss: 116.6474990845 | test_loss: 5.9312443733 | \n",
      "Epoch: 8432 | train_loss: 116.6473159790 | test_loss: 5.9312229156 | \n",
      "Epoch: 8433 | train_loss: 116.6471862793 | test_loss: 5.9311976433 | \n",
      "Epoch: 8434 | train_loss: 116.6470336914 | test_loss: 5.9311761856 | \n",
      "Epoch: 8435 | train_loss: 116.6469039917 | test_loss: 5.9311528206 | \n",
      "Epoch: 8436 | train_loss: 116.6467361450 | test_loss: 5.9311285019 | \n",
      "Epoch: 8437 | train_loss: 116.6465835571 | test_loss: 5.9311046600 | \n",
      "Epoch: 8438 | train_loss: 116.6464385986 | test_loss: 5.9310870171 | \n",
      "Epoch: 8439 | train_loss: 116.6462707520 | test_loss: 5.9310655594 | \n",
      "Epoch: 8440 | train_loss: 116.6461334229 | test_loss: 5.9310426712 | \n",
      "Epoch: 8441 | train_loss: 116.6459350586 | test_loss: 5.9310197830 | \n",
      "Epoch: 8442 | train_loss: 116.6457977295 | test_loss: 5.9309968948 | \n",
      "Epoch: 8443 | train_loss: 116.6456604004 | test_loss: 5.9309806824 | \n",
      "Epoch: 8444 | train_loss: 116.6455001831 | test_loss: 5.9309535027 | \n",
      "Epoch: 8445 | train_loss: 116.6453552246 | test_loss: 5.9309306145 | \n",
      "Epoch: 8446 | train_loss: 116.6452178955 | test_loss: 5.9309096336 | \n",
      "Epoch: 8447 | train_loss: 116.6450653076 | test_loss: 5.9308819771 | \n",
      "Epoch: 8448 | train_loss: 116.6449050903 | test_loss: 5.9308629036 | \n",
      "Epoch: 8449 | train_loss: 116.6447677612 | test_loss: 5.9308423996 | \n",
      "Epoch: 8450 | train_loss: 116.6445922852 | test_loss: 5.9308195114 | \n",
      "Epoch: 8451 | train_loss: 116.6444625854 | test_loss: 5.9307990074 | \n",
      "Epoch: 8452 | train_loss: 116.6443176270 | test_loss: 5.9307732582 | \n",
      "Epoch: 8453 | train_loss: 116.6441345215 | test_loss: 5.9307498932 | \n",
      "Epoch: 8454 | train_loss: 116.6439666748 | test_loss: 5.9307298660 | \n",
      "Epoch: 8455 | train_loss: 116.6438293457 | test_loss: 5.9307060242 | \n",
      "Epoch: 8456 | train_loss: 116.6436843872 | test_loss: 5.9306831360 | \n",
      "Epoch: 8457 | train_loss: 116.6435470581 | test_loss: 5.9306612015 | \n",
      "Epoch: 8458 | train_loss: 116.6433868408 | test_loss: 5.9306397438 | \n",
      "Epoch: 8459 | train_loss: 116.6432647705 | test_loss: 5.9306163788 | \n",
      "Epoch: 8460 | train_loss: 116.6430969238 | test_loss: 5.9305882454 | \n",
      "Epoch: 8461 | train_loss: 116.6429519653 | test_loss: 5.9305710793 | \n",
      "Epoch: 8462 | train_loss: 116.6428146362 | test_loss: 5.9305524826 | \n",
      "Epoch: 8463 | train_loss: 116.6426239014 | test_loss: 5.9305286407 | \n",
      "Epoch: 8464 | train_loss: 116.6425018311 | test_loss: 5.9305024147 | \n",
      "Epoch: 8465 | train_loss: 116.6423645020 | test_loss: 5.9304795265 | \n",
      "Epoch: 8466 | train_loss: 116.6422271729 | test_loss: 5.9304599762 | \n",
      "Epoch: 8467 | train_loss: 116.6420898438 | test_loss: 5.9304370880 | \n",
      "Epoch: 8468 | train_loss: 116.6419601440 | test_loss: 5.9304218292 | \n",
      "Epoch: 8469 | train_loss: 116.6417999268 | test_loss: 5.9303998947 | \n",
      "Epoch: 8470 | train_loss: 116.6416473389 | test_loss: 5.9303798676 | \n",
      "Epoch: 8471 | train_loss: 116.6414871216 | test_loss: 5.9303574562 | \n",
      "Epoch: 8472 | train_loss: 116.6413345337 | test_loss: 5.9303398132 | \n",
      "Epoch: 8473 | train_loss: 116.6411590576 | test_loss: 5.9303178787 | \n",
      "Epoch: 8474 | train_loss: 116.6410217285 | test_loss: 5.9302954674 | \n",
      "Epoch: 8475 | train_loss: 116.6408996582 | test_loss: 5.9302802086 | \n",
      "Epoch: 8476 | train_loss: 116.6407318115 | test_loss: 5.9302520752 | \n",
      "Epoch: 8477 | train_loss: 116.6405639648 | test_loss: 5.9302310944 | \n",
      "Epoch: 8478 | train_loss: 116.6404266357 | test_loss: 5.9302058220 | \n",
      "Epoch: 8479 | train_loss: 116.6402359009 | test_loss: 5.9301853180 | \n",
      "Epoch: 8480 | train_loss: 116.6401367188 | test_loss: 5.9301609993 | \n",
      "Epoch: 8481 | train_loss: 116.6399612427 | test_loss: 5.9301424026 | \n",
      "Epoch: 8482 | train_loss: 116.6398162842 | test_loss: 5.9301190376 | \n",
      "Epoch: 8483 | train_loss: 116.6396713257 | test_loss: 5.9300951958 | \n",
      "Epoch: 8484 | train_loss: 116.6395416260 | test_loss: 5.9300794601 | \n",
      "Epoch: 8485 | train_loss: 116.6393661499 | test_loss: 5.9300522804 | \n",
      "Epoch: 8486 | train_loss: 116.6392288208 | test_loss: 5.9300298691 | \n",
      "Epoch: 8487 | train_loss: 116.6390914917 | test_loss: 5.9300098419 | \n",
      "Epoch: 8488 | train_loss: 116.6389160156 | test_loss: 5.9299879074 | \n",
      "Epoch: 8489 | train_loss: 116.6388015747 | test_loss: 5.9299678802 | \n",
      "Epoch: 8490 | train_loss: 116.6386642456 | test_loss: 5.9299421310 | \n",
      "Epoch: 8491 | train_loss: 116.6385192871 | test_loss: 5.9299178123 | \n",
      "Epoch: 8492 | train_loss: 116.6383819580 | test_loss: 5.9298954010 | \n",
      "Epoch: 8493 | train_loss: 116.6381912231 | test_loss: 5.9298710823 | \n",
      "Epoch: 8494 | train_loss: 116.6380538940 | test_loss: 5.9298491478 | \n",
      "Epoch: 8495 | train_loss: 116.6379089355 | test_loss: 5.9298272133 | \n",
      "Epoch: 8496 | train_loss: 116.6377410889 | test_loss: 5.9298048019 | \n",
      "Epoch: 8497 | train_loss: 116.6376190186 | test_loss: 5.9297809601 | \n",
      "Epoch: 8498 | train_loss: 116.6374816895 | test_loss: 5.9297614098 | \n",
      "Epoch: 8499 | train_loss: 116.6373138428 | test_loss: 5.9297370911 | \n",
      "Epoch: 8500 | train_loss: 116.6371612549 | test_loss: 5.9297156334 | \n",
      "Epoch: 8501 | train_loss: 116.6370010376 | test_loss: 5.9296879768 | \n",
      "Epoch: 8502 | train_loss: 116.6368560791 | test_loss: 5.9296665192 | \n",
      "Epoch: 8503 | train_loss: 116.6367187500 | test_loss: 5.9296460152 | \n",
      "Epoch: 8504 | train_loss: 116.6365737915 | test_loss: 5.9296236038 | \n",
      "Epoch: 8505 | train_loss: 116.6363830566 | test_loss: 5.9296050072 | \n",
      "Epoch: 8506 | train_loss: 116.6362609863 | test_loss: 5.9295835495 | \n",
      "Epoch: 8507 | train_loss: 116.6360931396 | test_loss: 5.9295635223 | \n",
      "Epoch: 8508 | train_loss: 116.6359786987 | test_loss: 5.9295434952 | \n",
      "Epoch: 8509 | train_loss: 116.6358413696 | test_loss: 5.9295244217 | \n",
      "Epoch: 8510 | train_loss: 116.6356506348 | test_loss: 5.9295020103 | \n",
      "Epoch: 8511 | train_loss: 116.6354980469 | test_loss: 5.9294757843 | \n",
      "Epoch: 8512 | train_loss: 116.6353607178 | test_loss: 5.9294567108 | \n",
      "Epoch: 8513 | train_loss: 116.6351852417 | test_loss: 5.9294328690 | \n",
      "Epoch: 8514 | train_loss: 116.6350555420 | test_loss: 5.9294090271 | \n",
      "Epoch: 8515 | train_loss: 116.6349411011 | test_loss: 5.9293894768 | \n",
      "Epoch: 8516 | train_loss: 116.6348114014 | test_loss: 5.9293646812 | \n",
      "Epoch: 8517 | train_loss: 116.6346588135 | test_loss: 5.9293451309 | \n",
      "Epoch: 8518 | train_loss: 116.6345214844 | test_loss: 5.9293193817 | \n",
      "Epoch: 8519 | train_loss: 116.6343765259 | test_loss: 5.9292988777 | \n",
      "Epoch: 8520 | train_loss: 116.6342086792 | test_loss: 5.9292769432 | \n",
      "Epoch: 8521 | train_loss: 116.6340713501 | test_loss: 5.9292531013 | \n",
      "Epoch: 8522 | train_loss: 116.6339263916 | test_loss: 5.9292302132 | \n",
      "Epoch: 8523 | train_loss: 116.6337509155 | test_loss: 5.9292087555 | \n",
      "Epoch: 8524 | train_loss: 116.6336135864 | test_loss: 5.9291858673 | \n",
      "Epoch: 8525 | train_loss: 116.6334152222 | test_loss: 5.9291644096 | \n",
      "Epoch: 8526 | train_loss: 116.6332931519 | test_loss: 5.9291424751 | \n",
      "Epoch: 8527 | train_loss: 116.6331481934 | test_loss: 5.9291114807 | \n",
      "Epoch: 8528 | train_loss: 116.6329650879 | test_loss: 5.9290928841 | \n",
      "Epoch: 8529 | train_loss: 116.6328125000 | test_loss: 5.9290709496 | \n",
      "Epoch: 8530 | train_loss: 116.6326751709 | test_loss: 5.9290480614 | \n",
      "Epoch: 8531 | train_loss: 116.6324768066 | test_loss: 5.9290328026 | \n",
      "Epoch: 8532 | train_loss: 116.6323318481 | test_loss: 5.9290103912 | \n",
      "Epoch: 8533 | train_loss: 116.6321945190 | test_loss: 5.9289913177 | \n",
      "Epoch: 8534 | train_loss: 116.6320114136 | test_loss: 5.9289674759 | \n",
      "Epoch: 8535 | train_loss: 116.6319122314 | test_loss: 5.9289479256 | \n",
      "Epoch: 8536 | train_loss: 116.6317749023 | test_loss: 5.9289216995 | \n",
      "Epoch: 8537 | train_loss: 116.6315612793 | test_loss: 5.9288983345 | \n",
      "Epoch: 8538 | train_loss: 116.6314468384 | test_loss: 5.9288692474 | \n",
      "Epoch: 8539 | train_loss: 116.6313171387 | test_loss: 5.9288482666 | \n",
      "Epoch: 8540 | train_loss: 116.6311721802 | test_loss: 5.9288263321 | \n",
      "Epoch: 8541 | train_loss: 116.6309967041 | test_loss: 5.9287996292 | \n",
      "Epoch: 8542 | train_loss: 116.6308517456 | test_loss: 5.9287834167 | \n",
      "Epoch: 8543 | train_loss: 116.6307067871 | test_loss: 5.9287629128 | \n",
      "Epoch: 8544 | train_loss: 116.6305770874 | test_loss: 5.9287376404 | \n",
      "Epoch: 8545 | train_loss: 116.6304473877 | test_loss: 5.9287204742 | \n",
      "Epoch: 8546 | train_loss: 116.6302719116 | test_loss: 5.9286980629 | \n",
      "Epoch: 8547 | train_loss: 116.6301345825 | test_loss: 5.9286761284 | \n",
      "Epoch: 8548 | train_loss: 116.6300201416 | test_loss: 5.9286570549 | \n",
      "Epoch: 8549 | train_loss: 116.6298599243 | test_loss: 5.9286341667 | \n",
      "Epoch: 8550 | train_loss: 116.6297073364 | test_loss: 5.9286127090 | \n",
      "Epoch: 8551 | train_loss: 116.6295471191 | test_loss: 5.9285950661 | \n",
      "Epoch: 8552 | train_loss: 116.6294174194 | test_loss: 5.9285750389 | \n",
      "Epoch: 8553 | train_loss: 116.6292800903 | test_loss: 5.9285526276 | \n",
      "Epoch: 8554 | train_loss: 116.6291046143 | test_loss: 5.9285321236 | \n",
      "Epoch: 8555 | train_loss: 116.6289825439 | test_loss: 5.9285063744 | \n",
      "Epoch: 8556 | train_loss: 116.6288604736 | test_loss: 5.9284839630 | \n",
      "Epoch: 8557 | train_loss: 116.6287078857 | test_loss: 5.9284586906 | \n",
      "Epoch: 8558 | train_loss: 116.6285324097 | test_loss: 5.9284362793 | \n",
      "Epoch: 8559 | train_loss: 116.6283950806 | test_loss: 5.9284152985 | \n",
      "Epoch: 8560 | train_loss: 116.6282348633 | test_loss: 5.9283933640 | \n",
      "Epoch: 8561 | train_loss: 116.6280822754 | test_loss: 5.9283671379 | \n",
      "Epoch: 8562 | train_loss: 116.6279296875 | test_loss: 5.9283432961 | \n",
      "Epoch: 8563 | train_loss: 116.6278228760 | test_loss: 5.9283213615 | \n",
      "Epoch: 8564 | train_loss: 116.6276779175 | test_loss: 5.9283037186 | \n",
      "Epoch: 8565 | train_loss: 116.6275100708 | test_loss: 5.9282822609 | \n",
      "Epoch: 8566 | train_loss: 116.6273498535 | test_loss: 5.9282603264 | \n",
      "Epoch: 8567 | train_loss: 116.6272048950 | test_loss: 5.9282383919 | \n",
      "Epoch: 8568 | train_loss: 116.6270523071 | test_loss: 5.9282169342 | \n",
      "Epoch: 8569 | train_loss: 116.6269149780 | test_loss: 5.9281945229 | \n",
      "Epoch: 8570 | train_loss: 116.6267471313 | test_loss: 5.9281744957 | \n",
      "Epoch: 8571 | train_loss: 116.6266098022 | test_loss: 5.9281520844 | \n",
      "Epoch: 8572 | train_loss: 116.6264724731 | test_loss: 5.9281353951 | \n",
      "Epoch: 8573 | train_loss: 116.6263504028 | test_loss: 5.9281129837 | \n",
      "Epoch: 8574 | train_loss: 116.6261901855 | test_loss: 5.9280920029 | \n",
      "Epoch: 8575 | train_loss: 116.6260375977 | test_loss: 5.9280629158 | \n",
      "Epoch: 8576 | train_loss: 116.6258697510 | test_loss: 5.9280452728 | \n",
      "Epoch: 8577 | train_loss: 116.6257629395 | test_loss: 5.9280219078 | \n",
      "Epoch: 8578 | train_loss: 116.6255645752 | test_loss: 5.9280004501 | \n",
      "Epoch: 8579 | train_loss: 116.6254196167 | test_loss: 5.9279856682 | \n",
      "Epoch: 8580 | train_loss: 116.6252822876 | test_loss: 5.9279627800 | \n",
      "Epoch: 8581 | train_loss: 116.6251373291 | test_loss: 5.9279356003 | \n",
      "Epoch: 8582 | train_loss: 116.6250228882 | test_loss: 5.9279131889 | \n",
      "Epoch: 8583 | train_loss: 116.6248474121 | test_loss: 5.9278936386 | \n",
      "Epoch: 8584 | train_loss: 116.6247329712 | test_loss: 5.9278697968 | \n",
      "Epoch: 8585 | train_loss: 116.6246109009 | test_loss: 5.9278497696 | \n",
      "Epoch: 8586 | train_loss: 116.6244125366 | test_loss: 5.9278244972 | \n",
      "Epoch: 8587 | train_loss: 116.6242675781 | test_loss: 5.9278039932 | \n",
      "Epoch: 8588 | train_loss: 116.6241149902 | test_loss: 5.9277815819 | \n",
      "Epoch: 8589 | train_loss: 116.6239547729 | test_loss: 5.9277625084 | \n",
      "Epoch: 8590 | train_loss: 116.6238174438 | test_loss: 5.9277405739 | \n",
      "Epoch: 8591 | train_loss: 116.6236419678 | test_loss: 5.9277143478 | \n",
      "Epoch: 8592 | train_loss: 116.6234741211 | test_loss: 5.9276909828 | \n",
      "Epoch: 8593 | train_loss: 116.6233291626 | test_loss: 5.9276685715 | \n",
      "Epoch: 8594 | train_loss: 116.6231918335 | test_loss: 5.9276456833 | \n",
      "Epoch: 8595 | train_loss: 116.6230697632 | test_loss: 5.9276256561 | \n",
      "Epoch: 8596 | train_loss: 116.6229171753 | test_loss: 5.9276018143 | \n",
      "Epoch: 8597 | train_loss: 116.6228027344 | test_loss: 5.9275817871 | \n",
      "Epoch: 8598 | train_loss: 116.6226348877 | test_loss: 5.9275593758 | \n",
      "Epoch: 8599 | train_loss: 116.6224594116 | test_loss: 5.9275388718 | \n",
      "Epoch: 8600 | train_loss: 116.6223220825 | test_loss: 5.9275207520 | \n",
      "Epoch: 8601 | train_loss: 116.6222076416 | test_loss: 5.9275016785 | \n",
      "Epoch: 8602 | train_loss: 116.6220626831 | test_loss: 5.9274773598 | \n",
      "Epoch: 8603 | train_loss: 116.6219177246 | test_loss: 5.9274530411 | \n",
      "Epoch: 8604 | train_loss: 116.6217880249 | test_loss: 5.9274349213 | \n",
      "Epoch: 8605 | train_loss: 116.6216430664 | test_loss: 5.9274163246 | \n",
      "Epoch: 8606 | train_loss: 116.6214904785 | test_loss: 5.9273939133 | \n",
      "Epoch: 8607 | train_loss: 116.6213378906 | test_loss: 5.9273738861 | \n",
      "Epoch: 8608 | train_loss: 116.6212081909 | test_loss: 5.9273533821 | \n",
      "Epoch: 8609 | train_loss: 116.6210861206 | test_loss: 5.9273371696 | \n",
      "Epoch: 8610 | train_loss: 116.6209106445 | test_loss: 5.9273161888 | \n",
      "Epoch: 8611 | train_loss: 116.6207733154 | test_loss: 5.9272904396 | \n",
      "Epoch: 8612 | train_loss: 116.6206130981 | test_loss: 5.9272694588 | \n",
      "Epoch: 8613 | train_loss: 116.6204681396 | test_loss: 5.9272456169 | \n",
      "Epoch: 8614 | train_loss: 116.6203384399 | test_loss: 5.9272246361 | \n",
      "Epoch: 8615 | train_loss: 116.6201782227 | test_loss: 5.9272022247 | \n",
      "Epoch: 8616 | train_loss: 116.6200485229 | test_loss: 5.9271764755 | \n",
      "Epoch: 8617 | train_loss: 116.6198959351 | test_loss: 5.9271559715 | \n",
      "Epoch: 8618 | train_loss: 116.6197280884 | test_loss: 5.9271330833 | \n",
      "Epoch: 8619 | train_loss: 116.6195831299 | test_loss: 5.9271068573 | \n",
      "Epoch: 8620 | train_loss: 116.6194229126 | test_loss: 5.9270849228 | \n",
      "Epoch: 8621 | train_loss: 116.6192932129 | test_loss: 5.9270644188 | \n",
      "Epoch: 8622 | train_loss: 116.6191406250 | test_loss: 5.9270420074 | \n",
      "Epoch: 8623 | train_loss: 116.6189956665 | test_loss: 5.9270191193 | \n",
      "Epoch: 8624 | train_loss: 116.6188507080 | test_loss: 5.9269952774 | \n",
      "Epoch: 8625 | train_loss: 116.6186904907 | test_loss: 5.9269695282 | \n",
      "Epoch: 8626 | train_loss: 116.6185379028 | test_loss: 5.9269433022 | \n",
      "Epoch: 8627 | train_loss: 116.6184082031 | test_loss: 5.9269237518 | \n",
      "Epoch: 8628 | train_loss: 116.6182250977 | test_loss: 5.9269008636 | \n",
      "Epoch: 8629 | train_loss: 116.6180648804 | test_loss: 5.9268736839 | \n",
      "Epoch: 8630 | train_loss: 116.6178817749 | test_loss: 5.9268531799 | \n",
      "Epoch: 8631 | train_loss: 116.6177520752 | test_loss: 5.9268283844 | \n",
      "Epoch: 8632 | train_loss: 116.6175842285 | test_loss: 5.9268107414 | \n",
      "Epoch: 8633 | train_loss: 116.6174774170 | test_loss: 5.9267888069 | \n",
      "Epoch: 8634 | train_loss: 116.6173324585 | test_loss: 5.9267668724 | \n",
      "Epoch: 8635 | train_loss: 116.6171951294 | test_loss: 5.9267468452 | \n",
      "Epoch: 8636 | train_loss: 116.6170349121 | test_loss: 5.9267187119 | \n",
      "Epoch: 8637 | train_loss: 116.6168823242 | test_loss: 5.9266943932 | \n",
      "Epoch: 8638 | train_loss: 116.6167297363 | test_loss: 5.9266786575 | \n",
      "Epoch: 8639 | train_loss: 116.6166076660 | test_loss: 5.9266557693 | \n",
      "Epoch: 8640 | train_loss: 116.6164550781 | test_loss: 5.9266343117 | \n",
      "Epoch: 8641 | train_loss: 116.6163253784 | test_loss: 5.9266138077 | \n",
      "Epoch: 8642 | train_loss: 116.6161804199 | test_loss: 5.9265937805 | \n",
      "Epoch: 8643 | train_loss: 116.6160354614 | test_loss: 5.9265656471 | \n",
      "Epoch: 8644 | train_loss: 116.6158981323 | test_loss: 5.9265456200 | \n",
      "Epoch: 8645 | train_loss: 116.6157608032 | test_loss: 5.9265241623 | \n",
      "Epoch: 8646 | train_loss: 116.6156234741 | test_loss: 5.9265046120 | \n",
      "Epoch: 8647 | train_loss: 116.6154785156 | test_loss: 5.9264845848 | \n",
      "Epoch: 8648 | train_loss: 116.6153030396 | test_loss: 5.9264612198 | \n",
      "Epoch: 8649 | train_loss: 116.6151428223 | test_loss: 5.9264435768 | \n",
      "Epoch: 8650 | train_loss: 116.6149902344 | test_loss: 5.9264240265 | \n",
      "Epoch: 8651 | train_loss: 116.6148529053 | test_loss: 5.9264030457 | \n",
      "Epoch: 8652 | train_loss: 116.6147079468 | test_loss: 5.9263787270 | \n",
      "Epoch: 8653 | train_loss: 116.6145629883 | test_loss: 5.9263548851 | \n",
      "Epoch: 8654 | train_loss: 116.6144104004 | test_loss: 5.9263348579 | \n",
      "Epoch: 8655 | train_loss: 116.6142807007 | test_loss: 5.9263129234 | \n",
      "Epoch: 8656 | train_loss: 116.6141281128 | test_loss: 5.9262876511 | \n",
      "Epoch: 8657 | train_loss: 116.6139526367 | test_loss: 5.9262700081 | \n",
      "Epoch: 8658 | train_loss: 116.6138153076 | test_loss: 5.9262437820 | \n",
      "Epoch: 8659 | train_loss: 116.6136779785 | test_loss: 5.9262232780 | \n",
      "Epoch: 8660 | train_loss: 116.6135177612 | test_loss: 5.9261960983 | \n",
      "Epoch: 8661 | train_loss: 116.6133575439 | test_loss: 5.9261765480 | \n",
      "Epoch: 8662 | train_loss: 116.6132202148 | test_loss: 5.9261584282 | \n",
      "Epoch: 8663 | train_loss: 116.6130676270 | test_loss: 5.9261307716 | \n",
      "Epoch: 8664 | train_loss: 116.6129455566 | test_loss: 5.9261121750 | \n",
      "Epoch: 8665 | train_loss: 116.6127700806 | test_loss: 5.9260911942 | \n",
      "Epoch: 8666 | train_loss: 116.6125869751 | test_loss: 5.9260692596 | \n",
      "Epoch: 8667 | train_loss: 116.6124420166 | test_loss: 5.9260468483 | \n",
      "Epoch: 8668 | train_loss: 116.6123046875 | test_loss: 5.9260234833 | \n",
      "Epoch: 8669 | train_loss: 116.6121597290 | test_loss: 5.9260044098 | \n",
      "Epoch: 8670 | train_loss: 116.6120223999 | test_loss: 5.9259810448 | \n",
      "Epoch: 8671 | train_loss: 116.6118621826 | test_loss: 5.9259619713 | \n",
      "Epoch: 8672 | train_loss: 116.6117401123 | test_loss: 5.9259357452 | \n",
      "Epoch: 8673 | train_loss: 116.6116027832 | test_loss: 5.9259123802 | \n",
      "Epoch: 8674 | train_loss: 116.6114273071 | test_loss: 5.9258909225 | \n",
      "Epoch: 8675 | train_loss: 116.6112823486 | test_loss: 5.9258661270 | \n",
      "Epoch: 8676 | train_loss: 116.6111450195 | test_loss: 5.9258484840 | \n",
      "Epoch: 8677 | train_loss: 116.6109771729 | test_loss: 5.9258275032 | \n",
      "Epoch: 8678 | train_loss: 116.6108398438 | test_loss: 5.9258103371 | \n",
      "Epoch: 8679 | train_loss: 116.6106719971 | test_loss: 5.9257850647 | \n",
      "Epoch: 8680 | train_loss: 116.6105499268 | test_loss: 5.9257688522 | \n",
      "Epoch: 8681 | train_loss: 116.6104125977 | test_loss: 5.9257450104 | \n",
      "Epoch: 8682 | train_loss: 116.6102752686 | test_loss: 5.9257283211 | \n",
      "Epoch: 8683 | train_loss: 116.6101531982 | test_loss: 5.9257092476 | \n",
      "Epoch: 8684 | train_loss: 116.6099929810 | test_loss: 5.9256877899 | \n",
      "Epoch: 8685 | train_loss: 116.6098327637 | test_loss: 5.9256649017 | \n",
      "Epoch: 8686 | train_loss: 116.6097106934 | test_loss: 5.9256401062 | \n",
      "Epoch: 8687 | train_loss: 116.6095581055 | test_loss: 5.9256172180 | \n",
      "Epoch: 8688 | train_loss: 116.6094055176 | test_loss: 5.9255976677 | \n",
      "Epoch: 8689 | train_loss: 116.6092605591 | test_loss: 5.9255771637 | \n",
      "Epoch: 8690 | train_loss: 116.6091384888 | test_loss: 5.9255504608 | \n",
      "Epoch: 8691 | train_loss: 116.6089782715 | test_loss: 5.9255280495 | \n",
      "Epoch: 8692 | train_loss: 116.6088256836 | test_loss: 5.9255065918 | \n",
      "Epoch: 8693 | train_loss: 116.6086654663 | test_loss: 5.9254817963 | \n",
      "Epoch: 8694 | train_loss: 116.6085281372 | test_loss: 5.9254579544 | \n",
      "Epoch: 8695 | train_loss: 116.6083908081 | test_loss: 5.9254345894 | \n",
      "Epoch: 8696 | train_loss: 116.6082153320 | test_loss: 5.9254102707 | \n",
      "Epoch: 8697 | train_loss: 116.6080932617 | test_loss: 5.9253907204 | \n",
      "Epoch: 8698 | train_loss: 116.6079177856 | test_loss: 5.9253697395 | \n",
      "Epoch: 8699 | train_loss: 116.6077804565 | test_loss: 5.9253425598 | \n",
      "Epoch: 8700 | train_loss: 116.6076354980 | test_loss: 5.9253215790 | \n",
      "Epoch: 8701 | train_loss: 116.6074981689 | test_loss: 5.9252996445 | \n",
      "Epoch: 8702 | train_loss: 116.6073532104 | test_loss: 5.9252800941 | \n",
      "Epoch: 8703 | train_loss: 116.6071777344 | test_loss: 5.9252543449 | \n",
      "Epoch: 8704 | train_loss: 116.6070327759 | test_loss: 5.9252338409 | \n",
      "Epoch: 8705 | train_loss: 116.6069030762 | test_loss: 5.9252123833 | \n",
      "Epoch: 8706 | train_loss: 116.6067199707 | test_loss: 5.9251909256 | \n",
      "Epoch: 8707 | train_loss: 116.6065902710 | test_loss: 5.9251689911 | \n",
      "Epoch: 8708 | train_loss: 116.6064529419 | test_loss: 5.9251475334 | \n",
      "Epoch: 8709 | train_loss: 116.6063156128 | test_loss: 5.9251255989 | \n",
      "Epoch: 8710 | train_loss: 116.6061630249 | test_loss: 5.9251060486 | \n",
      "Epoch: 8711 | train_loss: 116.6060180664 | test_loss: 5.9250841141 | \n",
      "Epoch: 8712 | train_loss: 116.6058197021 | test_loss: 5.9250659943 | \n",
      "Epoch: 8713 | train_loss: 116.6057052612 | test_loss: 5.9250450134 | \n",
      "Epoch: 8714 | train_loss: 116.6055374146 | test_loss: 5.9250211716 | \n",
      "Epoch: 8715 | train_loss: 116.6053771973 | test_loss: 5.9250011444 | \n",
      "Epoch: 8716 | train_loss: 116.6052474976 | test_loss: 5.9249806404 | \n",
      "Epoch: 8717 | train_loss: 116.6051101685 | test_loss: 5.9249572754 | \n",
      "Epoch: 8718 | train_loss: 116.6049575806 | test_loss: 5.9249362946 | \n",
      "Epoch: 8719 | train_loss: 116.6048507690 | test_loss: 5.9249138832 | \n",
      "Epoch: 8720 | train_loss: 116.6047058105 | test_loss: 5.9248900414 | \n",
      "Epoch: 8721 | train_loss: 116.6045684814 | test_loss: 5.9248695374 | \n",
      "Epoch: 8722 | train_loss: 116.6044235229 | test_loss: 5.9248480797 | \n",
      "Epoch: 8723 | train_loss: 116.6042480469 | test_loss: 5.9248294830 | \n",
      "Epoch: 8724 | train_loss: 116.6041564941 | test_loss: 5.9248051643 | \n",
      "Epoch: 8725 | train_loss: 116.6040039062 | test_loss: 5.9247870445 | \n",
      "Epoch: 8726 | train_loss: 116.6038665771 | test_loss: 5.9247636795 | \n",
      "Epoch: 8727 | train_loss: 116.6036834717 | test_loss: 5.9247398376 | \n",
      "Epoch: 8728 | train_loss: 116.6035766602 | test_loss: 5.9247188568 | \n",
      "Epoch: 8729 | train_loss: 116.6034393311 | test_loss: 5.9246959686 | \n",
      "Epoch: 8730 | train_loss: 116.6032791138 | test_loss: 5.9246740341 | \n",
      "Epoch: 8731 | train_loss: 116.6031265259 | test_loss: 5.9246535301 | \n",
      "Epoch: 8732 | train_loss: 116.6029663086 | test_loss: 5.9246363640 | \n",
      "Epoch: 8733 | train_loss: 116.6028137207 | test_loss: 5.9246144295 | \n",
      "Epoch: 8734 | train_loss: 116.6026306152 | test_loss: 5.9245915413 | \n",
      "Epoch: 8735 | train_loss: 116.6025085449 | test_loss: 5.9245729446 | \n",
      "Epoch: 8736 | train_loss: 116.6023864746 | test_loss: 5.9245491028 | \n",
      "Epoch: 8737 | train_loss: 116.6022186279 | test_loss: 5.9245262146 | \n",
      "Epoch: 8738 | train_loss: 116.6020889282 | test_loss: 5.9245057106 | \n",
      "Epoch: 8739 | train_loss: 116.6019515991 | test_loss: 5.9244813919 | \n",
      "Epoch: 8740 | train_loss: 116.6017990112 | test_loss: 5.9244632721 | \n",
      "Epoch: 8741 | train_loss: 116.6016540527 | test_loss: 5.9244427681 | \n",
      "Epoch: 8742 | train_loss: 116.6015014648 | test_loss: 5.9244127274 | \n",
      "Epoch: 8743 | train_loss: 116.6013259888 | test_loss: 5.9243907928 | \n",
      "Epoch: 8744 | train_loss: 116.6011734009 | test_loss: 5.9243693352 | \n",
      "Epoch: 8745 | train_loss: 116.6010360718 | test_loss: 5.9243507385 | \n",
      "Epoch: 8746 | train_loss: 116.6008911133 | test_loss: 5.9243302345 | \n",
      "Epoch: 8747 | train_loss: 116.6007537842 | test_loss: 5.9243092537 | \n",
      "Epoch: 8748 | train_loss: 116.6005630493 | test_loss: 5.9242854118 | \n",
      "Epoch: 8749 | train_loss: 116.6004333496 | test_loss: 5.9242615700 | \n",
      "Epoch: 8750 | train_loss: 116.6003036499 | test_loss: 5.9242372513 | \n",
      "Epoch: 8751 | train_loss: 116.6001281738 | test_loss: 5.9242181778 | \n",
      "Epoch: 8752 | train_loss: 116.6000061035 | test_loss: 5.9242000580 | \n",
      "Epoch: 8753 | train_loss: 116.5998611450 | test_loss: 5.9241786003 | \n",
      "Epoch: 8754 | train_loss: 116.5997085571 | test_loss: 5.9241571426 | \n",
      "Epoch: 8755 | train_loss: 116.5995483398 | test_loss: 5.9241361618 | \n",
      "Epoch: 8756 | train_loss: 116.5994567871 | test_loss: 5.9241123199 | \n",
      "Epoch: 8757 | train_loss: 116.5992889404 | test_loss: 5.9240903854 | \n",
      "Epoch: 8758 | train_loss: 116.5991210938 | test_loss: 5.9240698814 | \n",
      "Epoch: 8759 | train_loss: 116.5989837646 | test_loss: 5.9240436554 | \n",
      "Epoch: 8760 | train_loss: 116.5988311768 | test_loss: 5.9240264893 | \n",
      "Epoch: 8761 | train_loss: 116.5987091064 | test_loss: 5.9240002632 | \n",
      "Epoch: 8762 | train_loss: 116.5985717773 | test_loss: 5.9239749908 | \n",
      "Epoch: 8763 | train_loss: 116.5983886719 | test_loss: 5.9239583015 | \n",
      "Epoch: 8764 | train_loss: 116.5982742310 | test_loss: 5.9239392281 | \n",
      "Epoch: 8765 | train_loss: 116.5981216431 | test_loss: 5.9239139557 | \n",
      "Epoch: 8766 | train_loss: 116.5979995728 | test_loss: 5.9238963127 | \n",
      "Epoch: 8767 | train_loss: 116.5978622437 | test_loss: 5.9238753319 | \n",
      "Epoch: 8768 | train_loss: 116.5977325439 | test_loss: 5.9238519669 | \n",
      "Epoch: 8769 | train_loss: 116.5975875854 | test_loss: 5.9238324165 | \n",
      "Epoch: 8770 | train_loss: 116.5974426270 | test_loss: 5.9238128662 | \n",
      "Epoch: 8771 | train_loss: 116.5973052979 | test_loss: 5.9237909317 | \n",
      "Epoch: 8772 | train_loss: 116.5971374512 | test_loss: 5.9237709045 | \n",
      "Epoch: 8773 | train_loss: 116.5970001221 | test_loss: 5.9237470627 | \n",
      "Epoch: 8774 | train_loss: 116.5968627930 | test_loss: 5.9237241745 | \n",
      "Epoch: 8775 | train_loss: 116.5967254639 | test_loss: 5.9237055779 | \n",
      "Epoch: 8776 | train_loss: 116.5966033936 | test_loss: 5.9236793518 | \n",
      "Epoch: 8777 | train_loss: 116.5964126587 | test_loss: 5.9236559868 | \n",
      "Epoch: 8778 | train_loss: 116.5963058472 | test_loss: 5.9236345291 | \n",
      "Epoch: 8779 | train_loss: 116.5961608887 | test_loss: 5.9236130714 | \n",
      "Epoch: 8780 | train_loss: 116.5960388184 | test_loss: 5.9235925674 | \n",
      "Epoch: 8781 | train_loss: 116.5959091187 | test_loss: 5.9235711098 | \n",
      "Epoch: 8782 | train_loss: 116.5957260132 | test_loss: 5.9235477448 | \n",
      "Epoch: 8783 | train_loss: 116.5956344604 | test_loss: 5.9235248566 | \n",
      "Epoch: 8784 | train_loss: 116.5954818726 | test_loss: 5.9235033989 | \n",
      "Epoch: 8785 | train_loss: 116.5953292847 | test_loss: 5.9234857559 | \n",
      "Epoch: 8786 | train_loss: 116.5951843262 | test_loss: 5.9234619141 | \n",
      "Epoch: 8787 | train_loss: 116.5950469971 | test_loss: 5.9234375954 | \n",
      "Epoch: 8788 | train_loss: 116.5949096680 | test_loss: 5.9234166145 | \n",
      "Epoch: 8789 | train_loss: 116.5947494507 | test_loss: 5.9233918190 | \n",
      "Epoch: 8790 | train_loss: 116.5945968628 | test_loss: 5.9233689308 | \n",
      "Epoch: 8791 | train_loss: 116.5944595337 | test_loss: 5.9233460426 | \n",
      "Epoch: 8792 | train_loss: 116.5943450928 | test_loss: 5.9233121872 | \n",
      "Epoch: 8793 | train_loss: 116.5941772461 | test_loss: 5.9232869148 | \n",
      "Epoch: 8794 | train_loss: 116.5940170288 | test_loss: 5.9232659340 | \n",
      "Epoch: 8795 | train_loss: 116.5938873291 | test_loss: 5.9232468605 | \n",
      "Epoch: 8796 | train_loss: 116.5937423706 | test_loss: 5.9232249260 | \n",
      "Epoch: 8797 | train_loss: 116.5935821533 | test_loss: 5.9232044220 | \n",
      "Epoch: 8798 | train_loss: 116.5934448242 | test_loss: 5.9231834412 | \n",
      "Epoch: 8799 | train_loss: 116.5932922363 | test_loss: 5.9231615067 | \n",
      "Epoch: 8800 | train_loss: 116.5931549072 | test_loss: 5.9231476784 | \n",
      "Epoch: 8801 | train_loss: 116.5930404663 | test_loss: 5.9231257439 | \n",
      "Epoch: 8802 | train_loss: 116.5928726196 | test_loss: 5.9231004715 | \n",
      "Epoch: 8803 | train_loss: 116.5927124023 | test_loss: 5.9230804443 | \n",
      "Epoch: 8804 | train_loss: 116.5925674438 | test_loss: 5.9230585098 | \n",
      "Epoch: 8805 | train_loss: 116.5924682617 | test_loss: 5.9230394363 | \n",
      "Epoch: 8806 | train_loss: 116.5922775269 | test_loss: 5.9230260849 | \n",
      "Epoch: 8807 | train_loss: 116.5921554565 | test_loss: 5.9229974747 | \n",
      "Epoch: 8808 | train_loss: 116.5920104980 | test_loss: 5.9229803085 | \n",
      "Epoch: 8809 | train_loss: 116.5918655396 | test_loss: 5.9229598045 | \n",
      "Epoch: 8810 | train_loss: 116.5917282104 | test_loss: 5.9229373932 | \n",
      "Epoch: 8811 | train_loss: 116.5915908813 | test_loss: 5.9229106903 | \n",
      "Epoch: 8812 | train_loss: 116.5914382935 | test_loss: 5.9228925705 | \n",
      "Epoch: 8813 | train_loss: 116.5912628174 | test_loss: 5.9228734970 | \n",
      "Epoch: 8814 | train_loss: 116.5911254883 | test_loss: 5.9228496552 | \n",
      "Epoch: 8815 | train_loss: 116.5910034180 | test_loss: 5.9228281975 | \n",
      "Epoch: 8816 | train_loss: 116.5908432007 | test_loss: 5.9228043556 | \n",
      "Epoch: 8817 | train_loss: 116.5907058716 | test_loss: 5.9227843285 | \n",
      "Epoch: 8818 | train_loss: 116.5905838013 | test_loss: 5.9227576256 | \n",
      "Epoch: 8819 | train_loss: 116.5904312134 | test_loss: 5.9227333069 | \n",
      "Epoch: 8820 | train_loss: 116.5902557373 | test_loss: 5.9227113724 | \n",
      "Epoch: 8821 | train_loss: 116.5901489258 | test_loss: 5.9226922989 | \n",
      "Epoch: 8822 | train_loss: 116.5899810791 | test_loss: 5.9226703644 | \n",
      "Epoch: 8823 | train_loss: 116.5898666382 | test_loss: 5.9226503372 | \n",
      "Epoch: 8824 | train_loss: 116.5897140503 | test_loss: 5.9226293564 | \n",
      "Epoch: 8825 | train_loss: 116.5895919800 | test_loss: 5.9226045609 | \n",
      "Epoch: 8826 | train_loss: 116.5894470215 | test_loss: 5.9225783348 | \n",
      "Epoch: 8827 | train_loss: 116.5893325806 | test_loss: 5.9225602150 | \n",
      "Epoch: 8828 | train_loss: 116.5891571045 | test_loss: 5.9225397110 | \n",
      "Epoch: 8829 | train_loss: 116.5890197754 | test_loss: 5.9225206375 | \n",
      "Epoch: 8830 | train_loss: 116.5889282227 | test_loss: 5.9225020409 | \n",
      "Epoch: 8831 | train_loss: 116.5887756348 | test_loss: 5.9224839211 | \n",
      "Epoch: 8832 | train_loss: 116.5885848999 | test_loss: 5.9224638939 | \n",
      "Epoch: 8833 | train_loss: 116.5884780884 | test_loss: 5.9224424362 | \n",
      "Epoch: 8834 | train_loss: 116.5883178711 | test_loss: 5.9224238396 | \n",
      "Epoch: 8835 | train_loss: 116.5881576538 | test_loss: 5.9223952293 | \n",
      "Epoch: 8836 | train_loss: 116.5880203247 | test_loss: 5.9223766327 | \n",
      "Epoch: 8837 | train_loss: 116.5878829956 | test_loss: 5.9223537445 | \n",
      "Epoch: 8838 | train_loss: 116.5877532959 | test_loss: 5.9223284721 | \n",
      "Epoch: 8839 | train_loss: 116.5876235962 | test_loss: 5.9223051071 | \n",
      "Epoch: 8840 | train_loss: 116.5874633789 | test_loss: 5.9222855568 | \n",
      "Epoch: 8841 | train_loss: 116.5873107910 | test_loss: 5.9222631454 | \n",
      "Epoch: 8842 | train_loss: 116.5871505737 | test_loss: 5.9222412109 | \n",
      "Epoch: 8843 | train_loss: 116.5870208740 | test_loss: 5.9222173691 | \n",
      "Epoch: 8844 | train_loss: 116.5868682861 | test_loss: 5.9221973419 | \n",
      "Epoch: 8845 | train_loss: 116.5867385864 | test_loss: 5.9221696854 | \n",
      "Epoch: 8846 | train_loss: 116.5866088867 | test_loss: 5.9221534729 | \n",
      "Epoch: 8847 | train_loss: 116.5864868164 | test_loss: 5.9221296310 | \n",
      "Epoch: 8848 | train_loss: 116.5863342285 | test_loss: 5.9221110344 | \n",
      "Epoch: 8849 | train_loss: 116.5861892700 | test_loss: 5.9220886230 | \n",
      "Epoch: 8850 | train_loss: 116.5860519409 | test_loss: 5.9220714569 | \n",
      "Epoch: 8851 | train_loss: 116.5859146118 | test_loss: 5.9220495224 | \n",
      "Epoch: 8852 | train_loss: 116.5857543945 | test_loss: 5.9220294952 | \n",
      "Epoch: 8853 | train_loss: 116.5856246948 | test_loss: 5.9220099449 | \n",
      "Epoch: 8854 | train_loss: 116.5854873657 | test_loss: 5.9219865799 | \n",
      "Epoch: 8855 | train_loss: 116.5853805542 | test_loss: 5.9219660759 | \n",
      "Epoch: 8856 | train_loss: 116.5852279663 | test_loss: 5.9219455719 | \n",
      "Epoch: 8857 | train_loss: 116.5851058960 | test_loss: 5.9219207764 | \n",
      "Epoch: 8858 | train_loss: 116.5849304199 | test_loss: 5.9218993187 | \n",
      "Epoch: 8859 | train_loss: 116.5847549438 | test_loss: 5.9218788147 | \n",
      "Epoch: 8860 | train_loss: 116.5846405029 | test_loss: 5.9218602180 | \n",
      "Epoch: 8861 | train_loss: 116.5844802856 | test_loss: 5.9218358994 | \n",
      "Epoch: 8862 | train_loss: 116.5843582153 | test_loss: 5.9218115807 | \n",
      "Epoch: 8863 | train_loss: 116.5841979980 | test_loss: 5.9217982292 | \n",
      "Epoch: 8864 | train_loss: 116.5840759277 | test_loss: 5.9217767715 | \n",
      "Epoch: 8865 | train_loss: 116.5839538574 | test_loss: 5.9217572212 | \n",
      "Epoch: 8866 | train_loss: 116.5837860107 | test_loss: 5.9217391014 | \n",
      "Epoch: 8867 | train_loss: 116.5836791992 | test_loss: 5.9217176437 | \n",
      "Epoch: 8868 | train_loss: 116.5835571289 | test_loss: 5.9216947556 | \n",
      "Epoch: 8869 | train_loss: 116.5833816528 | test_loss: 5.9216704369 | \n",
      "Epoch: 8870 | train_loss: 116.5832138062 | test_loss: 5.9216523170 | \n",
      "Epoch: 8871 | train_loss: 116.5831146240 | test_loss: 5.9216322899 | \n",
      "Epoch: 8872 | train_loss: 116.5829772949 | test_loss: 5.9216136932 | \n",
      "Epoch: 8873 | train_loss: 116.5828399658 | test_loss: 5.9215931892 | \n",
      "Epoch: 8874 | train_loss: 116.5826950073 | test_loss: 5.9215741158 | \n",
      "Epoch: 8875 | train_loss: 116.5825424194 | test_loss: 5.9215540886 | \n",
      "Epoch: 8876 | train_loss: 116.5824050903 | test_loss: 5.9215331078 | \n",
      "Epoch: 8877 | train_loss: 116.5822753906 | test_loss: 5.9215111732 | \n",
      "Epoch: 8878 | train_loss: 116.5821456909 | test_loss: 5.9214935303 | \n",
      "Epoch: 8879 | train_loss: 116.5819702148 | test_loss: 5.9214687347 | \n",
      "Epoch: 8880 | train_loss: 116.5818252563 | test_loss: 5.9214496613 | \n",
      "Epoch: 8881 | train_loss: 116.5816955566 | test_loss: 5.9214243889 | \n",
      "Epoch: 8882 | train_loss: 116.5815658569 | test_loss: 5.9214124680 | \n",
      "Epoch: 8883 | train_loss: 116.5813980103 | test_loss: 5.9213891029 | \n",
      "Epoch: 8884 | train_loss: 116.5812530518 | test_loss: 5.9213690758 | \n",
      "Epoch: 8885 | train_loss: 116.5811309814 | test_loss: 5.9213495255 | \n",
      "Epoch: 8886 | train_loss: 116.5809936523 | test_loss: 5.9213232994 | \n",
      "Epoch: 8887 | train_loss: 116.5808486938 | test_loss: 5.9213018417 | \n",
      "Epoch: 8888 | train_loss: 116.5806732178 | test_loss: 5.9212803841 | \n",
      "Epoch: 8889 | train_loss: 116.5805587769 | test_loss: 5.9212574959 | \n",
      "Epoch: 8890 | train_loss: 116.5803909302 | test_loss: 5.9212412834 | \n",
      "Epoch: 8891 | train_loss: 116.5802536011 | test_loss: 5.9212131500 | \n",
      "Epoch: 8892 | train_loss: 116.5801086426 | test_loss: 5.9211950302 | \n",
      "Epoch: 8893 | train_loss: 116.5799636841 | test_loss: 5.9211711884 | \n",
      "Epoch: 8894 | train_loss: 116.5798110962 | test_loss: 5.9211564064 | \n",
      "Epoch: 8895 | train_loss: 116.5797042847 | test_loss: 5.9211320877 | \n",
      "Epoch: 8896 | train_loss: 116.5795135498 | test_loss: 5.9211106300 | \n",
      "Epoch: 8897 | train_loss: 116.5794143677 | test_loss: 5.9210906029 | \n",
      "Epoch: 8898 | train_loss: 116.5792694092 | test_loss: 5.9210662842 | \n",
      "Epoch: 8899 | train_loss: 116.5791473389 | test_loss: 5.9210486412 | \n",
      "Epoch: 8900 | train_loss: 116.5790023804 | test_loss: 5.9210247993 | \n",
      "Epoch: 8901 | train_loss: 116.5788879395 | test_loss: 5.9210014343 | \n",
      "Epoch: 8902 | train_loss: 116.5787124634 | test_loss: 5.9209775925 | \n",
      "Epoch: 8903 | train_loss: 116.5785903931 | test_loss: 5.9209537506 | \n",
      "Epoch: 8904 | train_loss: 116.5784835815 | test_loss: 5.9209289551 | \n",
      "Epoch: 8905 | train_loss: 116.5783233643 | test_loss: 5.9209065437 | \n",
      "Epoch: 8906 | train_loss: 116.5781936646 | test_loss: 5.9208850861 | \n",
      "Epoch: 8907 | train_loss: 116.5780487061 | test_loss: 5.9208631516 | \n",
      "Epoch: 8908 | train_loss: 116.5778884888 | test_loss: 5.9208416939 | \n",
      "Epoch: 8909 | train_loss: 116.5777511597 | test_loss: 5.9208240509 | \n",
      "Epoch: 8910 | train_loss: 116.5776062012 | test_loss: 5.9208030701 | \n",
      "Epoch: 8911 | train_loss: 116.5774612427 | test_loss: 5.9207806587 | \n",
      "Epoch: 8912 | train_loss: 116.5773315430 | test_loss: 5.9207568169 | \n",
      "Epoch: 8913 | train_loss: 116.5771865845 | test_loss: 5.9207410812 | \n",
      "Epoch: 8914 | train_loss: 116.5770416260 | test_loss: 5.9207191467 | \n",
      "Epoch: 8915 | train_loss: 116.5769271851 | test_loss: 5.9206886292 | \n",
      "Epoch: 8916 | train_loss: 116.5767669678 | test_loss: 5.9206714630 | \n",
      "Epoch: 8917 | train_loss: 116.5766601562 | test_loss: 5.9206485748 | \n",
      "Epoch: 8918 | train_loss: 116.5764923096 | test_loss: 5.9206304550 | \n",
      "Epoch: 8919 | train_loss: 116.5763702393 | test_loss: 5.9206070900 | \n",
      "Epoch: 8920 | train_loss: 116.5762329102 | test_loss: 5.9205875397 | \n",
      "Epoch: 8921 | train_loss: 116.5761108398 | test_loss: 5.9205698967 | \n",
      "Epoch: 8922 | train_loss: 116.5759735107 | test_loss: 5.9205431938 | \n",
      "Epoch: 8923 | train_loss: 116.5758361816 | test_loss: 5.9205231667 | \n",
      "Epoch: 8924 | train_loss: 116.5756759644 | test_loss: 5.9204978943 | \n",
      "Epoch: 8925 | train_loss: 116.5755691528 | test_loss: 5.9204769135 | \n",
      "Epoch: 8926 | train_loss: 116.5754470825 | test_loss: 5.9204554558 | \n",
      "Epoch: 8927 | train_loss: 116.5752944946 | test_loss: 5.9204344749 | \n",
      "Epoch: 8928 | train_loss: 116.5751953125 | test_loss: 5.9204206467 | \n",
      "Epoch: 8929 | train_loss: 116.5750427246 | test_loss: 5.9203991890 | \n",
      "Epoch: 8930 | train_loss: 116.5749130249 | test_loss: 5.9203810692 | \n",
      "Epoch: 8931 | train_loss: 116.5747680664 | test_loss: 5.9203577042 | \n",
      "Epoch: 8932 | train_loss: 116.5746383667 | test_loss: 5.9203376770 | \n",
      "Epoch: 8933 | train_loss: 116.5744934082 | test_loss: 5.9203181267 | \n",
      "Epoch: 8934 | train_loss: 116.5743713379 | test_loss: 5.9202961922 | \n",
      "Epoch: 8935 | train_loss: 116.5742187500 | test_loss: 5.9202775955 | \n",
      "Epoch: 8936 | train_loss: 116.5740890503 | test_loss: 5.9202528000 | \n",
      "Epoch: 8937 | train_loss: 116.5739517212 | test_loss: 5.9202265739 | \n",
      "Epoch: 8938 | train_loss: 116.5738296509 | test_loss: 5.9202103615 | \n",
      "Epoch: 8939 | train_loss: 116.5736618042 | test_loss: 5.9201855659 | \n",
      "Epoch: 8940 | train_loss: 116.5735549927 | test_loss: 5.9201664925 | \n",
      "Epoch: 8941 | train_loss: 116.5734176636 | test_loss: 5.9201459885 | \n",
      "Epoch: 8942 | train_loss: 116.5733032227 | test_loss: 5.9201207161 | \n",
      "Epoch: 8943 | train_loss: 116.5731811523 | test_loss: 5.9201002121 | \n",
      "Epoch: 8944 | train_loss: 116.5730361938 | test_loss: 5.9200744629 | \n",
      "Epoch: 8945 | train_loss: 116.5728988647 | test_loss: 5.9200563431 | \n",
      "Epoch: 8946 | train_loss: 116.5727844238 | test_loss: 5.9200358391 | \n",
      "Epoch: 8947 | train_loss: 116.5726470947 | test_loss: 5.9200162888 | \n",
      "Epoch: 8948 | train_loss: 116.5724716187 | test_loss: 5.9199934006 | \n",
      "Epoch: 8949 | train_loss: 116.5723571777 | test_loss: 5.9199733734 | \n",
      "Epoch: 8950 | train_loss: 116.5722351074 | test_loss: 5.9199552536 | \n",
      "Epoch: 8951 | train_loss: 116.5720901489 | test_loss: 5.9199342728 | \n",
      "Epoch: 8952 | train_loss: 116.5719757080 | test_loss: 5.9199090004 | \n",
      "Epoch: 8953 | train_loss: 116.5718460083 | test_loss: 5.9198865891 | \n",
      "Epoch: 8954 | train_loss: 116.5717010498 | test_loss: 5.9198670387 | \n",
      "Epoch: 8955 | train_loss: 116.5715713501 | test_loss: 5.9198493958 | \n",
      "Epoch: 8956 | train_loss: 116.5714416504 | test_loss: 5.9198327065 | \n",
      "Epoch: 8957 | train_loss: 116.5712966919 | test_loss: 5.9198131561 | \n",
      "Epoch: 8958 | train_loss: 116.5711975098 | test_loss: 5.9197902679 | \n",
      "Epoch: 8959 | train_loss: 116.5710678101 | test_loss: 5.9197697639 | \n",
      "Epoch: 8960 | train_loss: 116.5709304810 | test_loss: 5.9197473526 | \n",
      "Epoch: 8961 | train_loss: 116.5707778931 | test_loss: 5.9197292328 | \n",
      "Epoch: 8962 | train_loss: 116.5706253052 | test_loss: 5.9197063446 | \n",
      "Epoch: 8963 | train_loss: 116.5704956055 | test_loss: 5.9196844101 | \n",
      "Epoch: 8964 | train_loss: 116.5703582764 | test_loss: 5.9196653366 | \n",
      "Epoch: 8965 | train_loss: 116.5702133179 | test_loss: 5.9196472168 | \n",
      "Epoch: 8966 | train_loss: 116.5700683594 | test_loss: 5.9196295738 | \n",
      "Epoch: 8967 | train_loss: 116.5699386597 | test_loss: 5.9196076393 | \n",
      "Epoch: 8968 | train_loss: 116.5698165894 | test_loss: 5.9195818901 | \n",
      "Epoch: 8969 | train_loss: 116.5696487427 | test_loss: 5.9195613861 | \n",
      "Epoch: 8970 | train_loss: 116.5694808960 | test_loss: 5.9195466042 | \n",
      "Epoch: 8971 | train_loss: 116.5693740845 | test_loss: 5.9195208549 | \n",
      "Epoch: 8972 | train_loss: 116.5692214966 | test_loss: 5.9194989204 | \n",
      "Epoch: 8973 | train_loss: 116.5690917969 | test_loss: 5.9194779396 | \n",
      "Epoch: 8974 | train_loss: 116.5689544678 | test_loss: 5.9194536209 | \n",
      "Epoch: 8975 | train_loss: 116.5688247681 | test_loss: 5.9194321632 | \n",
      "Epoch: 8976 | train_loss: 116.5686798096 | test_loss: 5.9194102287 | \n",
      "Epoch: 8977 | train_loss: 116.5685424805 | test_loss: 5.9193906784 | \n",
      "Epoch: 8978 | train_loss: 116.5683898926 | test_loss: 5.9193663597 | \n",
      "Epoch: 8979 | train_loss: 116.5682525635 | test_loss: 5.9193501472 | \n",
      "Epoch: 8980 | train_loss: 116.5681152344 | test_loss: 5.9193305969 | \n",
      "Epoch: 8981 | train_loss: 116.5680007935 | test_loss: 5.9193134308 | \n",
      "Epoch: 8982 | train_loss: 116.5678710938 | test_loss: 5.9192900658 | \n",
      "Epoch: 8983 | train_loss: 116.5676879883 | test_loss: 5.9192681313 | \n",
      "Epoch: 8984 | train_loss: 116.5675811768 | test_loss: 5.9192476273 | \n",
      "Epoch: 8985 | train_loss: 116.5674285889 | test_loss: 5.9192271233 | \n",
      "Epoch: 8986 | train_loss: 116.5673065186 | test_loss: 5.9192075729 | \n",
      "Epoch: 8987 | train_loss: 116.5671463013 | test_loss: 5.9191799164 | \n",
      "Epoch: 8988 | train_loss: 116.5670471191 | test_loss: 5.9191575050 | \n",
      "Epoch: 8989 | train_loss: 116.5668945312 | test_loss: 5.9191327095 | \n",
      "Epoch: 8990 | train_loss: 116.5667266846 | test_loss: 5.9191145897 | \n",
      "Epoch: 8991 | train_loss: 116.5666122437 | test_loss: 5.9190907478 | \n",
      "Epoch: 8992 | train_loss: 116.5664749146 | test_loss: 5.9190716743 | \n",
      "Epoch: 8993 | train_loss: 116.5663146973 | test_loss: 5.9190516472 | \n",
      "Epoch: 8994 | train_loss: 116.5661926270 | test_loss: 5.9190340042 | \n",
      "Epoch: 8995 | train_loss: 116.5660705566 | test_loss: 5.9190149307 | \n",
      "Epoch: 8996 | train_loss: 116.5659484863 | test_loss: 5.9189953804 | \n",
      "Epoch: 8997 | train_loss: 116.5658111572 | test_loss: 5.9189729691 | \n",
      "Epoch: 8998 | train_loss: 116.5656661987 | test_loss: 5.9189524651 | \n",
      "Epoch: 8999 | train_loss: 116.5655059814 | test_loss: 5.9189324379 | \n",
      "Epoch: 9000 | train_loss: 116.5653610229 | test_loss: 5.9189128876 | \n",
      "Epoch: 9001 | train_loss: 116.5652236938 | test_loss: 5.9188957214 | \n",
      "Epoch: 9002 | train_loss: 116.5650787354 | test_loss: 5.9188728333 | \n",
      "Epoch: 9003 | train_loss: 116.5649719238 | test_loss: 5.9188489914 | \n",
      "Epoch: 9004 | train_loss: 116.5647964478 | test_loss: 5.9188275337 | \n",
      "Epoch: 9005 | train_loss: 116.5646743774 | test_loss: 5.9188103676 | \n",
      "Epoch: 9006 | train_loss: 116.5645599365 | test_loss: 5.9187922478 | \n",
      "Epoch: 9007 | train_loss: 116.5644760132 | test_loss: 5.9187650681 | \n",
      "Epoch: 9008 | train_loss: 116.5643157959 | test_loss: 5.9187407494 | \n",
      "Epoch: 9009 | train_loss: 116.5641784668 | test_loss: 5.9187254906 | \n",
      "Epoch: 9010 | train_loss: 116.5640335083 | test_loss: 5.9187059402 | \n",
      "Epoch: 9011 | train_loss: 116.5638732910 | test_loss: 5.9186801910 | \n",
      "Epoch: 9012 | train_loss: 116.5637435913 | test_loss: 5.9186625481 | \n",
      "Epoch: 9013 | train_loss: 116.5636138916 | test_loss: 5.9186420441 | \n",
      "Epoch: 9014 | train_loss: 116.5634765625 | test_loss: 5.9186234474 | \n",
      "Epoch: 9015 | train_loss: 116.5633392334 | test_loss: 5.9186034203 | \n",
      "Epoch: 9016 | train_loss: 116.5632324219 | test_loss: 5.9185853004 | \n",
      "Epoch: 9017 | train_loss: 116.5630264282 | test_loss: 5.9185662270 | \n",
      "Epoch: 9018 | train_loss: 116.5629119873 | test_loss: 5.9185481071 | \n",
      "Epoch: 9019 | train_loss: 116.5627517700 | test_loss: 5.9185261726 | \n",
      "Epoch: 9020 | train_loss: 116.5626373291 | test_loss: 5.9185023308 | \n",
      "Epoch: 9021 | train_loss: 116.5625000000 | test_loss: 5.9184803963 | \n",
      "Epoch: 9022 | train_loss: 116.5623168945 | test_loss: 5.9184613228 | \n",
      "Epoch: 9023 | train_loss: 116.5622100830 | test_loss: 5.9184379578 | \n",
      "Epoch: 9024 | train_loss: 116.5620803833 | test_loss: 5.9184179306 | \n",
      "Epoch: 9025 | train_loss: 116.5619430542 | test_loss: 5.9183936119 | \n",
      "Epoch: 9026 | train_loss: 116.5618133545 | test_loss: 5.9183683395 | \n",
      "Epoch: 9027 | train_loss: 116.5616607666 | test_loss: 5.9183478355 | \n",
      "Epoch: 9028 | train_loss: 116.5615463257 | test_loss: 5.9183235168 | \n",
      "Epoch: 9029 | train_loss: 116.5613708496 | test_loss: 5.9183025360 | \n",
      "Epoch: 9030 | train_loss: 116.5612106323 | test_loss: 5.9182758331 | \n",
      "Epoch: 9031 | train_loss: 116.5610961914 | test_loss: 5.9182567596 | \n",
      "Epoch: 9032 | train_loss: 116.5609741211 | test_loss: 5.9182353020 | \n",
      "Epoch: 9033 | train_loss: 116.5608673096 | test_loss: 5.9182133675 | \n",
      "Epoch: 9034 | train_loss: 116.5607147217 | test_loss: 5.9181962013 | \n",
      "Epoch: 9035 | train_loss: 116.5605850220 | test_loss: 5.9181733131 | \n",
      "Epoch: 9036 | train_loss: 116.5604400635 | test_loss: 5.9181470871 | \n",
      "Epoch: 9037 | train_loss: 116.5602951050 | test_loss: 5.9181303978 | \n",
      "Epoch: 9038 | train_loss: 116.5601959229 | test_loss: 5.9181151390 | \n",
      "Epoch: 9039 | train_loss: 116.5600280762 | test_loss: 5.9180903435 | \n",
      "Epoch: 9040 | train_loss: 116.5599212646 | test_loss: 5.9180693626 | \n",
      "Epoch: 9041 | train_loss: 116.5598144531 | test_loss: 5.9180464745 | \n",
      "Epoch: 9042 | train_loss: 116.5596466064 | test_loss: 5.9180297852 | \n",
      "Epoch: 9043 | train_loss: 116.5595092773 | test_loss: 5.9180073738 | \n",
      "Epoch: 9044 | train_loss: 116.5594024658 | test_loss: 5.9179859161 | \n",
      "Epoch: 9045 | train_loss: 116.5592803955 | test_loss: 5.9179644585 | \n",
      "Epoch: 9046 | train_loss: 116.5591583252 | test_loss: 5.9179401398 | \n",
      "Epoch: 9047 | train_loss: 116.5589904785 | test_loss: 5.9179215431 | \n",
      "Epoch: 9048 | train_loss: 116.5588302612 | test_loss: 5.9179039001 | \n",
      "Epoch: 9049 | train_loss: 116.5587310791 | test_loss: 5.9178810120 | \n",
      "Epoch: 9050 | train_loss: 116.5586166382 | test_loss: 5.9178600311 | \n",
      "Epoch: 9051 | train_loss: 116.5584640503 | test_loss: 5.9178371429 | \n",
      "Epoch: 9052 | train_loss: 116.5583190918 | test_loss: 5.9178161621 | \n",
      "Epoch: 9053 | train_loss: 116.5581512451 | test_loss: 5.9177918434 | \n",
      "Epoch: 9054 | train_loss: 116.5580444336 | test_loss: 5.9177727699 | \n",
      "Epoch: 9055 | train_loss: 116.5578994751 | test_loss: 5.9177465439 | \n",
      "Epoch: 9056 | train_loss: 116.5577239990 | test_loss: 5.9177246094 | \n",
      "Epoch: 9057 | train_loss: 116.5575866699 | test_loss: 5.9177007675 | \n",
      "Epoch: 9058 | train_loss: 116.5574722290 | test_loss: 5.9176793098 | \n",
      "Epoch: 9059 | train_loss: 116.5573348999 | test_loss: 5.9176597595 | \n",
      "Epoch: 9060 | train_loss: 116.5572204590 | test_loss: 5.9176325798 | \n",
      "Epoch: 9061 | train_loss: 116.5570678711 | test_loss: 5.9176130295 | \n",
      "Epoch: 9062 | train_loss: 116.5569305420 | test_loss: 5.9175944328 | \n",
      "Epoch: 9063 | train_loss: 116.5567855835 | test_loss: 5.9175758362 | \n",
      "Epoch: 9064 | train_loss: 116.5566482544 | test_loss: 5.9175539017 | \n",
      "Epoch: 9065 | train_loss: 116.5564956665 | test_loss: 5.9175310135 | \n",
      "Epoch: 9066 | train_loss: 116.5563659668 | test_loss: 5.9175105095 | \n",
      "Epoch: 9067 | train_loss: 116.5562438965 | test_loss: 5.9174914360 | \n",
      "Epoch: 9068 | train_loss: 116.5561065674 | test_loss: 5.9174709320 | \n",
      "Epoch: 9069 | train_loss: 116.5559463501 | test_loss: 5.9174494743 | \n",
      "Epoch: 9070 | train_loss: 116.5558090210 | test_loss: 5.9174284935 | \n",
      "Epoch: 9071 | train_loss: 116.5556945801 | test_loss: 5.9174065590 | \n",
      "Epoch: 9072 | train_loss: 116.5555572510 | test_loss: 5.9173798561 | \n",
      "Epoch: 9073 | train_loss: 116.5554199219 | test_loss: 5.9173588753 | \n",
      "Epoch: 9074 | train_loss: 116.5552749634 | test_loss: 5.9173417091 | \n",
      "Epoch: 9075 | train_loss: 116.5551528931 | test_loss: 5.9173192978 | \n",
      "Epoch: 9076 | train_loss: 116.5549926758 | test_loss: 5.9173002243 | \n",
      "Epoch: 9077 | train_loss: 116.5548553467 | test_loss: 5.9172792435 | \n",
      "Epoch: 9078 | train_loss: 116.5547256470 | test_loss: 5.9172601700 | \n",
      "Epoch: 9079 | train_loss: 116.5545806885 | test_loss: 5.9172401428 | \n",
      "Epoch: 9080 | train_loss: 116.5544815063 | test_loss: 5.9172210693 | \n",
      "Epoch: 9081 | train_loss: 116.5543060303 | test_loss: 5.9172010422 | \n",
      "Epoch: 9082 | train_loss: 116.5541992188 | test_loss: 5.9171776772 | \n",
      "Epoch: 9083 | train_loss: 116.5540847778 | test_loss: 5.9171600342 | \n",
      "Epoch: 9084 | train_loss: 116.5539474487 | test_loss: 5.9171385765 | \n",
      "Epoch: 9085 | train_loss: 116.5538024902 | test_loss: 5.9171161652 | \n",
      "Epoch: 9086 | train_loss: 116.5536651611 | test_loss: 5.9170961380 | \n",
      "Epoch: 9087 | train_loss: 116.5535125732 | test_loss: 5.9170708656 | \n",
      "Epoch: 9088 | train_loss: 116.5533752441 | test_loss: 5.9170536995 | \n",
      "Epoch: 9089 | train_loss: 116.5532455444 | test_loss: 5.9170341492 | \n",
      "Epoch: 9090 | train_loss: 116.5531158447 | test_loss: 5.9170131683 | \n",
      "Epoch: 9091 | train_loss: 116.5529479980 | test_loss: 5.9169898033 | \n",
      "Epoch: 9092 | train_loss: 116.5528106689 | test_loss: 5.9169707298 | \n",
      "Epoch: 9093 | train_loss: 116.5527114868 | test_loss: 5.9169549942 | \n",
      "Epoch: 9094 | train_loss: 116.5525360107 | test_loss: 5.9169306755 | \n",
      "Epoch: 9095 | train_loss: 116.5524215698 | test_loss: 5.9169092178 | \n",
      "Epoch: 9096 | train_loss: 116.5522689819 | test_loss: 5.9168891907 | \n",
      "Epoch: 9097 | train_loss: 116.5521163940 | test_loss: 5.9168653488 | \n",
      "Epoch: 9098 | train_loss: 116.5519943237 | test_loss: 5.9168443680 | \n",
      "Epoch: 9099 | train_loss: 116.5518493652 | test_loss: 5.9168248177 | \n",
      "Epoch: 9100 | train_loss: 116.5516586304 | test_loss: 5.9168090820 | \n",
      "Epoch: 9101 | train_loss: 116.5515441895 | test_loss: 5.9167881012 | \n",
      "Epoch: 9102 | train_loss: 116.5514068604 | test_loss: 5.9167671204 | \n",
      "Epoch: 9103 | train_loss: 116.5512619019 | test_loss: 5.9167461395 | \n",
      "Epoch: 9104 | train_loss: 116.5510940552 | test_loss: 5.9167261124 | \n",
      "Epoch: 9105 | train_loss: 116.5509872437 | test_loss: 5.9167065620 | \n",
      "Epoch: 9106 | train_loss: 116.5508499146 | test_loss: 5.9166841507 | \n",
      "Epoch: 9107 | train_loss: 116.5507202148 | test_loss: 5.9166607857 | \n",
      "Epoch: 9108 | train_loss: 116.5506057739 | test_loss: 5.9166498184 | \n",
      "Epoch: 9109 | train_loss: 116.5504608154 | test_loss: 5.9166264534 | \n",
      "Epoch: 9110 | train_loss: 116.5503311157 | test_loss: 5.9166021347 | \n",
      "Epoch: 9111 | train_loss: 116.5502166748 | test_loss: 5.9165849686 | \n",
      "Epoch: 9112 | train_loss: 116.5500717163 | test_loss: 5.9165644646 | \n",
      "Epoch: 9113 | train_loss: 116.5499114990 | test_loss: 5.9165449142 | \n",
      "Epoch: 9114 | train_loss: 116.5497665405 | test_loss: 5.9165239334 | \n",
      "Epoch: 9115 | train_loss: 116.5496520996 | test_loss: 5.9165024757 | \n",
      "Epoch: 9116 | train_loss: 116.5495300293 | test_loss: 5.9164862633 | \n",
      "Epoch: 9117 | train_loss: 116.5493927002 | test_loss: 5.9164671898 | \n",
      "Epoch: 9118 | train_loss: 116.5492706299 | test_loss: 5.9164452553 | \n",
      "Epoch: 9119 | train_loss: 116.5491409302 | test_loss: 5.9164261818 | \n",
      "Epoch: 9120 | train_loss: 116.5490264893 | test_loss: 5.9164028168 | \n",
      "Epoch: 9121 | train_loss: 116.5488891602 | test_loss: 5.9163856506 | \n",
      "Epoch: 9122 | train_loss: 116.5487365723 | test_loss: 5.9163641930 | \n",
      "Epoch: 9123 | train_loss: 116.5486450195 | test_loss: 5.9163465500 | \n",
      "Epoch: 9124 | train_loss: 116.5485153198 | test_loss: 5.9163227081 | \n",
      "Epoch: 9125 | train_loss: 116.5483474731 | test_loss: 5.9163041115 | \n",
      "Epoch: 9126 | train_loss: 116.5482635498 | test_loss: 5.9162836075 | \n",
      "Epoch: 9127 | train_loss: 116.5481033325 | test_loss: 5.9162702560 | \n",
      "Epoch: 9128 | train_loss: 116.5480041504 | test_loss: 5.9162502289 | \n",
      "Epoch: 9129 | train_loss: 116.5478897095 | test_loss: 5.9162311554 | \n",
      "Epoch: 9130 | train_loss: 116.5477752686 | test_loss: 5.9162082672 | \n",
      "Epoch: 9131 | train_loss: 116.5476455688 | test_loss: 5.9161853790 | \n",
      "Epoch: 9132 | train_loss: 116.5474929810 | test_loss: 5.9161653519 | \n",
      "Epoch: 9133 | train_loss: 116.5473632812 | test_loss: 5.9161443710 | \n",
      "Epoch: 9134 | train_loss: 116.5472259521 | test_loss: 5.9161181450 | \n",
      "Epoch: 9135 | train_loss: 116.5470809937 | test_loss: 5.9161047935 | \n",
      "Epoch: 9136 | train_loss: 116.5469360352 | test_loss: 5.9160823822 | \n",
      "Epoch: 9137 | train_loss: 116.5468292236 | test_loss: 5.9160552025 | \n",
      "Epoch: 9138 | train_loss: 116.5466766357 | test_loss: 5.9160380363 | \n",
      "Epoch: 9139 | train_loss: 116.5465545654 | test_loss: 5.9160203934 | \n",
      "Epoch: 9140 | train_loss: 116.5464248657 | test_loss: 5.9159932137 | \n",
      "Epoch: 9141 | train_loss: 116.5462570190 | test_loss: 5.9159755707 | \n",
      "Epoch: 9142 | train_loss: 116.5461120605 | test_loss: 5.9159526825 | \n",
      "Epoch: 9143 | train_loss: 116.5459671021 | test_loss: 5.9159321785 | \n",
      "Epoch: 9144 | train_loss: 116.5458602905 | test_loss: 5.9159064293 | \n",
      "Epoch: 9145 | train_loss: 116.5457458496 | test_loss: 5.9158897400 | \n",
      "Epoch: 9146 | train_loss: 116.5456085205 | test_loss: 5.9158673286 | \n",
      "Epoch: 9147 | train_loss: 116.5454711914 | test_loss: 5.9158449173 | \n",
      "Epoch: 9148 | train_loss: 116.5453414917 | test_loss: 5.9158229828 | \n",
      "Epoch: 9149 | train_loss: 116.5452346802 | test_loss: 5.9158086777 | \n",
      "Epoch: 9150 | train_loss: 116.5450973511 | test_loss: 5.9157915115 | \n",
      "Epoch: 9151 | train_loss: 116.5449905396 | test_loss: 5.9157757759 | \n",
      "Epoch: 9152 | train_loss: 116.5448303223 | test_loss: 5.9157547951 | \n",
      "Epoch: 9153 | train_loss: 116.5446853638 | test_loss: 5.9157323837 | \n",
      "Epoch: 9154 | train_loss: 116.5446014404 | test_loss: 5.9157133102 | \n",
      "Epoch: 9155 | train_loss: 116.5445022583 | test_loss: 5.9156932831 | \n",
      "Epoch: 9156 | train_loss: 116.5443878174 | test_loss: 5.9156794548 | \n",
      "Epoch: 9157 | train_loss: 116.5442504883 | test_loss: 5.9156589508 | \n",
      "Epoch: 9158 | train_loss: 116.5441284180 | test_loss: 5.9156398773 | \n",
      "Epoch: 9159 | train_loss: 116.5439910889 | test_loss: 5.9156231880 | \n",
      "Epoch: 9160 | train_loss: 116.5438766479 | test_loss: 5.9156031609 | \n",
      "Epoch: 9161 | train_loss: 116.5437240601 | test_loss: 5.9155845642 | \n",
      "Epoch: 9162 | train_loss: 116.5435638428 | test_loss: 5.9155607224 | \n",
      "Epoch: 9163 | train_loss: 116.5434265137 | test_loss: 5.9155468941 | \n",
      "Epoch: 9164 | train_loss: 116.5433502197 | test_loss: 5.9155259132 | \n",
      "Epoch: 9165 | train_loss: 116.5431900024 | test_loss: 5.9155058861 | \n",
      "Epoch: 9166 | train_loss: 116.5430679321 | test_loss: 5.9154877663 | \n",
      "Epoch: 9167 | train_loss: 116.5429458618 | test_loss: 5.9154591560 | \n",
      "Epoch: 9168 | train_loss: 116.5428009033 | test_loss: 5.9154376984 | \n",
      "Epoch: 9169 | train_loss: 116.5427017212 | test_loss: 5.9154200554 | \n",
      "Epoch: 9170 | train_loss: 116.5425796509 | test_loss: 5.9154014587 | \n",
      "Epoch: 9171 | train_loss: 116.5424346924 | test_loss: 5.9153790474 | \n",
      "Epoch: 9172 | train_loss: 116.5422592163 | test_loss: 5.9153652191 | \n",
      "Epoch: 9173 | train_loss: 116.5421218872 | test_loss: 5.9153418541 | \n",
      "Epoch: 9174 | train_loss: 116.5420150757 | test_loss: 5.9153265953 | \n",
      "Epoch: 9175 | train_loss: 116.5418853760 | test_loss: 5.9153032303 | \n",
      "Epoch: 9176 | train_loss: 116.5417709351 | test_loss: 5.9152812958 | \n",
      "Epoch: 9177 | train_loss: 116.5416488647 | test_loss: 5.9152598381 | \n",
      "Epoch: 9178 | train_loss: 116.5415267944 | test_loss: 5.9152431488 | \n",
      "Epoch: 9179 | train_loss: 116.5413742065 | test_loss: 5.9152207375 | \n",
      "Epoch: 9180 | train_loss: 116.5412292480 | test_loss: 5.9151988029 | \n",
      "Epoch: 9181 | train_loss: 116.5411148071 | test_loss: 5.9151821136 | \n",
      "Epoch: 9182 | train_loss: 116.5409469604 | test_loss: 5.9151597023 | \n",
      "Epoch: 9183 | train_loss: 116.5408401489 | test_loss: 5.9151434898 | \n",
      "Epoch: 9184 | train_loss: 116.5406799316 | test_loss: 5.9151196480 | \n",
      "Epoch: 9185 | train_loss: 116.5405654907 | test_loss: 5.9150962830 | \n",
      "Epoch: 9186 | train_loss: 116.5404281616 | test_loss: 5.9150781631 | \n",
      "Epoch: 9187 | train_loss: 116.5403060913 | test_loss: 5.9150462151 | \n",
      "Epoch: 9188 | train_loss: 116.5401458740 | test_loss: 5.9150266647 | \n",
      "Epoch: 9189 | train_loss: 116.5400085449 | test_loss: 5.9150090218 | \n",
      "Epoch: 9190 | train_loss: 116.5398864746 | test_loss: 5.9149904251 | \n",
      "Epoch: 9191 | train_loss: 116.5397491455 | test_loss: 5.9149665833 | \n",
      "Epoch: 9192 | train_loss: 116.5396423340 | test_loss: 5.9149470329 | \n",
      "Epoch: 9193 | train_loss: 116.5394897461 | test_loss: 5.9149284363 | \n",
      "Epoch: 9194 | train_loss: 116.5394058228 | test_loss: 5.9149098396 | \n",
      "Epoch: 9195 | train_loss: 116.5392608643 | test_loss: 5.9148845673 | \n",
      "Epoch: 9196 | train_loss: 116.5391311646 | test_loss: 5.9148645401 | \n",
      "Epoch: 9197 | train_loss: 116.5389862061 | test_loss: 5.9148473740 | \n",
      "Epoch: 9198 | train_loss: 116.5388412476 | test_loss: 5.9148259163 | \n",
      "Epoch: 9199 | train_loss: 116.5387268066 | test_loss: 5.9148082733 | \n",
      "Epoch: 9200 | train_loss: 116.5385665894 | test_loss: 5.9147906303 | \n",
      "Epoch: 9201 | train_loss: 116.5384216309 | test_loss: 5.9147648811 | \n",
      "Epoch: 9202 | train_loss: 116.5382843018 | test_loss: 5.9147448540 | \n",
      "Epoch: 9203 | train_loss: 116.5381774902 | test_loss: 5.9147272110 | \n",
      "Epoch: 9204 | train_loss: 116.5380401611 | test_loss: 5.9147090912 | \n",
      "Epoch: 9205 | train_loss: 116.5379180908 | test_loss: 5.9146885872 | \n",
      "Epoch: 9206 | train_loss: 116.5377655029 | test_loss: 5.9146637917 | \n",
      "Epoch: 9207 | train_loss: 116.5376205444 | test_loss: 5.9146480560 | \n",
      "Epoch: 9208 | train_loss: 116.5374908447 | test_loss: 5.9146251678 | \n",
      "Epoch: 9209 | train_loss: 116.5373764038 | test_loss: 5.9146070480 | \n",
      "Epoch: 9210 | train_loss: 116.5372238159 | test_loss: 5.9145874977 | \n",
      "Epoch: 9211 | train_loss: 116.5370788574 | test_loss: 5.9145646095 | \n",
      "Epoch: 9212 | train_loss: 116.5369567871 | test_loss: 5.9145417213 | \n",
      "Epoch: 9213 | train_loss: 116.5368041992 | test_loss: 5.9145221710 | \n",
      "Epoch: 9214 | train_loss: 116.5366592407 | test_loss: 5.9145030975 | \n",
      "Epoch: 9215 | train_loss: 116.5365219116 | test_loss: 5.9144783020 | \n",
      "Epoch: 9216 | train_loss: 116.5363845825 | test_loss: 5.9144616127 | \n",
      "Epoch: 9217 | train_loss: 116.5362319946 | test_loss: 5.9144420624 | \n",
      "Epoch: 9218 | train_loss: 116.5360870361 | test_loss: 5.9144172668 | \n",
      "Epoch: 9219 | train_loss: 116.5359344482 | test_loss: 5.9143934250 | \n",
      "Epoch: 9220 | train_loss: 116.5357894897 | test_loss: 5.9143762589 | \n",
      "Epoch: 9221 | train_loss: 116.5356826782 | test_loss: 5.9143538475 | \n",
      "Epoch: 9222 | train_loss: 116.5355300903 | test_loss: 5.9143314362 | \n",
      "Epoch: 9223 | train_loss: 116.5354003906 | test_loss: 5.9143114090 | \n",
      "Epoch: 9224 | train_loss: 116.5352554321 | test_loss: 5.9142937660 | \n",
      "Epoch: 9225 | train_loss: 116.5351333618 | test_loss: 5.9142699242 | \n",
      "Epoch: 9226 | train_loss: 116.5350036621 | test_loss: 5.9142522812 | \n",
      "Epoch: 9227 | train_loss: 116.5348510742 | test_loss: 5.9142332077 | \n",
      "Epoch: 9228 | train_loss: 116.5347137451 | test_loss: 5.9142127037 | \n",
      "Epoch: 9229 | train_loss: 116.5345535278 | test_loss: 5.9141936302 | \n",
      "Epoch: 9230 | train_loss: 116.5344314575 | test_loss: 5.9141755104 | \n",
      "Epoch: 9231 | train_loss: 116.5342788696 | test_loss: 5.9141559601 | \n",
      "Epoch: 9232 | train_loss: 116.5341491699 | test_loss: 5.9141321182 | \n",
      "Epoch: 9233 | train_loss: 116.5339813232 | test_loss: 5.9141130447 | \n",
      "Epoch: 9234 | train_loss: 116.5338439941 | test_loss: 5.9140944481 | \n",
      "Epoch: 9235 | train_loss: 116.5337142944 | test_loss: 5.9140696526 | \n",
      "Epoch: 9236 | train_loss: 116.5335769653 | test_loss: 5.9140515327 | \n",
      "Epoch: 9237 | train_loss: 116.5334548950 | test_loss: 5.9140262604 | \n",
      "Epoch: 9238 | train_loss: 116.5333099365 | test_loss: 5.9140105247 | \n",
      "Epoch: 9239 | train_loss: 116.5332031250 | test_loss: 5.9139890671 | \n",
      "Epoch: 9240 | train_loss: 116.5330352783 | test_loss: 5.9139685631 | \n",
      "Epoch: 9241 | train_loss: 116.5329666138 | test_loss: 5.9139471054 | \n",
      "Epoch: 9242 | train_loss: 116.5328063965 | test_loss: 5.9139318466 | \n",
      "Epoch: 9243 | train_loss: 116.5326766968 | test_loss: 5.9139156342 | \n",
      "Epoch: 9244 | train_loss: 116.5325469971 | test_loss: 5.9138908386 | \n",
      "Epoch: 9245 | train_loss: 116.5324401855 | test_loss: 5.9138712883 | \n",
      "Epoch: 9246 | train_loss: 116.5322952271 | test_loss: 5.9138536453 | \n",
      "Epoch: 9247 | train_loss: 116.5321884155 | test_loss: 5.9138355255 | \n",
      "Epoch: 9248 | train_loss: 116.5320587158 | test_loss: 5.9138169289 | \n",
      "Epoch: 9249 | train_loss: 116.5319213867 | test_loss: 5.9137935638 | \n",
      "Epoch: 9250 | train_loss: 116.5317993164 | test_loss: 5.9137721062 | \n",
      "Epoch: 9251 | train_loss: 116.5316619873 | test_loss: 5.9137568474 | \n",
      "Epoch: 9252 | train_loss: 116.5315322876 | test_loss: 5.9137358665 | \n",
      "Epoch: 9253 | train_loss: 116.5313949585 | test_loss: 5.9137158394 | \n",
      "Epoch: 9254 | train_loss: 116.5313110352 | test_loss: 5.9136939049 | \n",
      "Epoch: 9255 | train_loss: 116.5311508179 | test_loss: 5.9136753082 | \n",
      "Epoch: 9256 | train_loss: 116.5310058594 | test_loss: 5.9136528969 | \n",
      "Epoch: 9257 | train_loss: 116.5308914185 | test_loss: 5.9136328697 | \n",
      "Epoch: 9258 | train_loss: 116.5307617188 | test_loss: 5.9136152267 | \n",
      "Epoch: 9259 | train_loss: 116.5306472778 | test_loss: 5.9135909081 | \n",
      "Epoch: 9260 | train_loss: 116.5305175781 | test_loss: 5.9135742188 | \n",
      "Epoch: 9261 | train_loss: 116.5303726196 | test_loss: 5.9135546684 | \n",
      "Epoch: 9262 | train_loss: 116.5302352905 | test_loss: 5.9135327339 | \n",
      "Epoch: 9263 | train_loss: 116.5301437378 | test_loss: 5.9135131836 | \n",
      "Epoch: 9264 | train_loss: 116.5300064087 | test_loss: 5.9134950638 | \n",
      "Epoch: 9265 | train_loss: 116.5298538208 | test_loss: 5.9134740829 | \n",
      "Epoch: 9266 | train_loss: 116.5297393799 | test_loss: 5.9134564400 | \n",
      "Epoch: 9267 | train_loss: 116.5296096802 | test_loss: 5.9134368896 | \n",
      "Epoch: 9268 | train_loss: 116.5294647217 | test_loss: 5.9134173393 | \n",
      "Epoch: 9269 | train_loss: 116.5293426514 | test_loss: 5.9133973122 | \n",
      "Epoch: 9270 | train_loss: 116.5292282104 | test_loss: 5.9133801460 | \n",
      "Epoch: 9271 | train_loss: 116.5290832520 | test_loss: 5.9133582115 | \n",
      "Epoch: 9272 | train_loss: 116.5289840698 | test_loss: 5.9133367538 | \n",
      "Epoch: 9273 | train_loss: 116.5288467407 | test_loss: 5.9133176804 | \n",
      "Epoch: 9274 | train_loss: 116.5287399292 | test_loss: 5.9133000374 | \n",
      "Epoch: 9275 | train_loss: 116.5286254883 | test_loss: 5.9132814407 | \n",
      "Epoch: 9276 | train_loss: 116.5284729004 | test_loss: 5.9132580757 | \n",
      "Epoch: 9277 | train_loss: 116.5283432007 | test_loss: 5.9132466316 | \n",
      "Epoch: 9278 | train_loss: 116.5281906128 | test_loss: 5.9132218361 | \n",
      "Epoch: 9279 | train_loss: 116.5280990601 | test_loss: 5.9131999016 | \n",
      "Epoch: 9280 | train_loss: 116.5279617310 | test_loss: 5.9131822586 | \n",
      "Epoch: 9281 | train_loss: 116.5278244019 | test_loss: 5.9131617546 | \n",
      "Epoch: 9282 | train_loss: 116.5277175903 | test_loss: 5.9131398201 | \n",
      "Epoch: 9283 | train_loss: 116.5275650024 | test_loss: 5.9131221771 | \n",
      "Epoch: 9284 | train_loss: 116.5274581909 | test_loss: 5.9131026268 | \n",
      "Epoch: 9285 | train_loss: 116.5273437500 | test_loss: 5.9130816460 | \n",
      "Epoch: 9286 | train_loss: 116.5271987915 | test_loss: 5.9130644798 | \n",
      "Epoch: 9287 | train_loss: 116.5270614624 | test_loss: 5.9130458832 | \n",
      "Epoch: 9288 | train_loss: 116.5269241333 | test_loss: 5.9130291939 | \n",
      "Epoch: 9289 | train_loss: 116.5268096924 | test_loss: 5.9130096436 | \n",
      "Epoch: 9290 | train_loss: 116.5266723633 | test_loss: 5.9129877090 | \n",
      "Epoch: 9291 | train_loss: 116.5265655518 | test_loss: 5.9129657745 | \n",
      "Epoch: 9292 | train_loss: 116.5264282227 | test_loss: 5.9129467010 | \n",
      "Epoch: 9293 | train_loss: 116.5262908936 | test_loss: 5.9129247665 | \n",
      "Epoch: 9294 | train_loss: 116.5261688232 | test_loss: 5.9129076004 | \n",
      "Epoch: 9295 | train_loss: 116.5260467529 | test_loss: 5.9128870964 | \n",
      "Epoch: 9296 | train_loss: 116.5259094238 | test_loss: 5.9128742218 | \n",
      "Epoch: 9297 | train_loss: 116.5258102417 | test_loss: 5.9128541946 | \n",
      "Epoch: 9298 | train_loss: 116.5256729126 | test_loss: 5.9128360748 | \n",
      "Epoch: 9299 | train_loss: 116.5255432129 | test_loss: 5.9128150940 | \n",
      "Epoch: 9300 | train_loss: 116.5253982544 | test_loss: 5.9127984047 | \n",
      "Epoch: 9301 | train_loss: 116.5252609253 | test_loss: 5.9127759933 | \n",
      "Epoch: 9302 | train_loss: 116.5251388550 | test_loss: 5.9127573967 | \n",
      "Epoch: 9303 | train_loss: 116.5250167847 | test_loss: 5.9127407074 | \n",
      "Epoch: 9304 | train_loss: 116.5248718262 | test_loss: 5.9127225876 | \n",
      "Epoch: 9305 | train_loss: 116.5247344971 | test_loss: 5.9127025604 | \n",
      "Epoch: 9306 | train_loss: 116.5246276855 | test_loss: 5.9126763344 | \n",
      "Epoch: 9307 | train_loss: 116.5245056152 | test_loss: 5.9126615524 | \n",
      "Epoch: 9308 | train_loss: 116.5243835449 | test_loss: 5.9126343727 | \n",
      "Epoch: 9309 | train_loss: 116.5242462158 | test_loss: 5.9126214981 | \n",
      "Epoch: 9310 | train_loss: 116.5241088867 | test_loss: 5.9125971794 | \n",
      "Epoch: 9311 | train_loss: 116.5239715576 | test_loss: 5.9125809669 | \n",
      "Epoch: 9312 | train_loss: 116.5238494873 | test_loss: 5.9125599861 | \n",
      "Epoch: 9313 | train_loss: 116.5237197876 | test_loss: 5.9125370979 | \n",
      "Epoch: 9314 | train_loss: 116.5235748291 | test_loss: 5.9125146866 | \n",
      "Epoch: 9315 | train_loss: 116.5234375000 | test_loss: 5.9124965668 | \n",
      "Epoch: 9316 | train_loss: 116.5233001709 | test_loss: 5.9124751091 | \n",
      "Epoch: 9317 | train_loss: 116.5232162476 | test_loss: 5.9124565125 | \n",
      "Epoch: 9318 | train_loss: 116.5231170654 | test_loss: 5.9124379158 | \n",
      "Epoch: 9319 | train_loss: 116.5229721069 | test_loss: 5.9124174118 | \n",
      "Epoch: 9320 | train_loss: 116.5228500366 | test_loss: 5.9123954773 | \n",
      "Epoch: 9321 | train_loss: 116.5227279663 | test_loss: 5.9123749733 | \n",
      "Epoch: 9322 | train_loss: 116.5225906372 | test_loss: 5.9123582840 | \n",
      "Epoch: 9323 | train_loss: 116.5224914551 | test_loss: 5.9123401642 | \n",
      "Epoch: 9324 | train_loss: 116.5223541260 | test_loss: 5.9123215675 | \n",
      "Epoch: 9325 | train_loss: 116.5222396851 | test_loss: 5.9122986794 | \n",
      "Epoch: 9326 | train_loss: 116.5220870972 | test_loss: 5.9122829437 | \n",
      "Epoch: 9327 | train_loss: 116.5219726562 | test_loss: 5.9122662544 | \n",
      "Epoch: 9328 | train_loss: 116.5218582153 | test_loss: 5.9122452736 | \n",
      "Epoch: 9329 | train_loss: 116.5217208862 | test_loss: 5.9122319221 | \n",
      "Epoch: 9330 | train_loss: 116.5216140747 | test_loss: 5.9122052193 | \n",
      "Epoch: 9331 | train_loss: 116.5214920044 | test_loss: 5.9121837616 | \n",
      "Epoch: 9332 | train_loss: 116.5213394165 | test_loss: 5.9121680260 | \n",
      "Epoch: 9333 | train_loss: 116.5212097168 | test_loss: 5.9121489525 | \n",
      "Epoch: 9334 | train_loss: 116.5210647583 | test_loss: 5.9121274948 | \n",
      "Epoch: 9335 | train_loss: 116.5209350586 | test_loss: 5.9121074677 | \n",
      "Epoch: 9336 | train_loss: 116.5207901001 | test_loss: 5.9120883942 | \n",
      "Epoch: 9337 | train_loss: 116.5207061768 | test_loss: 5.9120697975 | \n",
      "Epoch: 9338 | train_loss: 116.5205535889 | test_loss: 5.9120507240 | \n",
      "Epoch: 9339 | train_loss: 116.5204391479 | test_loss: 5.9120316505 | \n",
      "Epoch: 9340 | train_loss: 116.5203094482 | test_loss: 5.9120092392 | \n",
      "Epoch: 9341 | train_loss: 116.5201721191 | test_loss: 5.9119858742 | \n",
      "Epoch: 9342 | train_loss: 116.5200271606 | test_loss: 5.9119658470 | \n",
      "Epoch: 9343 | train_loss: 116.5198898315 | test_loss: 5.9119462967 | \n",
      "Epoch: 9344 | train_loss: 116.5197525024 | test_loss: 5.9119310379 | \n",
      "Epoch: 9345 | train_loss: 116.5196075439 | test_loss: 5.9119071960 | \n",
      "Epoch: 9346 | train_loss: 116.5194854736 | test_loss: 5.9118843079 | \n",
      "Epoch: 9347 | train_loss: 116.5193481445 | test_loss: 5.9118638039 | \n",
      "Epoch: 9348 | train_loss: 116.5191955566 | test_loss: 5.9118432999 | \n",
      "Epoch: 9349 | train_loss: 116.5190658569 | test_loss: 5.9118213654 | \n",
      "Epoch: 9350 | train_loss: 116.5189361572 | test_loss: 5.9117975235 | \n",
      "Epoch: 9351 | train_loss: 116.5188064575 | test_loss: 5.9117803574 | \n",
      "Epoch: 9352 | train_loss: 116.5186691284 | test_loss: 5.9117598534 | \n",
      "Epoch: 9353 | train_loss: 116.5185317993 | test_loss: 5.9117393494 | \n",
      "Epoch: 9354 | train_loss: 116.5184173584 | test_loss: 5.9117169380 | \n",
      "Epoch: 9355 | train_loss: 116.5183029175 | test_loss: 5.9116973877 | \n",
      "Epoch: 9356 | train_loss: 116.5181655884 | test_loss: 5.9116783142 | \n",
      "Epoch: 9357 | train_loss: 116.5180282593 | test_loss: 5.9116606712 | \n",
      "Epoch: 9358 | train_loss: 116.5179214478 | test_loss: 5.9116392136 | \n",
      "Epoch: 9359 | train_loss: 116.5177841187 | test_loss: 5.9116210938 | \n",
      "Epoch: 9360 | train_loss: 116.5176849365 | test_loss: 5.9116034508 | \n",
      "Epoch: 9361 | train_loss: 116.5175476074 | test_loss: 5.9115839005 | \n",
      "Epoch: 9362 | train_loss: 116.5174102783 | test_loss: 5.9115710258 | \n",
      "Epoch: 9363 | train_loss: 116.5172882080 | test_loss: 5.9115509987 | \n",
      "Epoch: 9364 | train_loss: 116.5171356201 | test_loss: 5.9115319252 | \n",
      "Epoch: 9365 | train_loss: 116.5170593262 | test_loss: 5.9115180969 | \n",
      "Epoch: 9366 | train_loss: 116.5169143677 | test_loss: 5.9114995003 | \n",
      "Epoch: 9367 | train_loss: 116.5168075562 | test_loss: 5.9114809036 | \n",
      "Epoch: 9368 | train_loss: 116.5166702271 | test_loss: 5.9114623070 | \n",
      "Epoch: 9369 | train_loss: 116.5165252686 | test_loss: 5.9114422798 | \n",
      "Epoch: 9370 | train_loss: 116.5164184570 | test_loss: 5.9114222527 | \n",
      "Epoch: 9371 | train_loss: 116.5162811279 | test_loss: 5.9114031792 | \n",
      "Epoch: 9372 | train_loss: 116.5161056519 | test_loss: 5.9113817215 | \n",
      "Epoch: 9373 | train_loss: 116.5159759521 | test_loss: 5.9113631248 | \n",
      "Epoch: 9374 | train_loss: 116.5158996582 | test_loss: 5.9113397598 | \n",
      "Epoch: 9375 | train_loss: 116.5157546997 | test_loss: 5.9113173485 | \n",
      "Epoch: 9376 | train_loss: 116.5156326294 | test_loss: 5.9112992287 | \n",
      "Epoch: 9377 | train_loss: 116.5155258179 | test_loss: 5.9112792015 | \n",
      "Epoch: 9378 | train_loss: 116.5153732300 | test_loss: 5.9112606049 | \n",
      "Epoch: 9379 | train_loss: 116.5152435303 | test_loss: 5.9112415314 | \n",
      "Epoch: 9380 | train_loss: 116.5151290894 | test_loss: 5.9112281799 | \n",
      "Epoch: 9381 | train_loss: 116.5149612427 | test_loss: 5.9112024307 | \n",
      "Epoch: 9382 | train_loss: 116.5148239136 | test_loss: 5.9111866951 | \n",
      "Epoch: 9383 | train_loss: 116.5147399902 | test_loss: 5.9111633301 | \n",
      "Epoch: 9384 | train_loss: 116.5145874023 | test_loss: 5.9111442566 | \n",
      "Epoch: 9385 | train_loss: 116.5144805908 | test_loss: 5.9111204147 | \n",
      "Epoch: 9386 | train_loss: 116.5143432617 | test_loss: 5.9111061096 | \n",
      "Epoch: 9387 | train_loss: 116.5142211914 | test_loss: 5.9110865593 | \n",
      "Epoch: 9388 | train_loss: 116.5141143799 | test_loss: 5.9110703468 | \n",
      "Epoch: 9389 | train_loss: 116.5139923096 | test_loss: 5.9110469818 | \n",
      "Epoch: 9390 | train_loss: 116.5138015747 | test_loss: 5.9110264778 | \n",
      "Epoch: 9391 | train_loss: 116.5136795044 | test_loss: 5.9110102654 | \n",
      "Epoch: 9392 | train_loss: 116.5135803223 | test_loss: 5.9109950066 | \n",
      "Epoch: 9393 | train_loss: 116.5134735107 | test_loss: 5.9109706879 | \n",
      "Epoch: 9394 | train_loss: 116.5133514404 | test_loss: 5.9109563828 | \n",
      "Epoch: 9395 | train_loss: 116.5132446289 | test_loss: 5.9109330177 | \n",
      "Epoch: 9396 | train_loss: 116.5131072998 | test_loss: 5.9109120369 | \n",
      "Epoch: 9397 | train_loss: 116.5130004883 | test_loss: 5.9108901024 | \n",
      "Epoch: 9398 | train_loss: 116.5128784180 | test_loss: 5.9108719826 | \n",
      "Epoch: 9399 | train_loss: 116.5127258301 | test_loss: 5.9108486176 | \n",
      "Epoch: 9400 | train_loss: 116.5126190186 | test_loss: 5.9108276367 | \n",
      "Epoch: 9401 | train_loss: 116.5125122070 | test_loss: 5.9108104706 | \n",
      "Epoch: 9402 | train_loss: 116.5123748779 | test_loss: 5.9107928276 | \n",
      "Epoch: 9403 | train_loss: 116.5122528076 | test_loss: 5.9107723236 | \n",
      "Epoch: 9404 | train_loss: 116.5121307373 | test_loss: 5.9107570648 | \n",
      "Epoch: 9405 | train_loss: 116.5119705200 | test_loss: 5.9107398987 | \n",
      "Epoch: 9406 | train_loss: 116.5118713379 | test_loss: 5.9107213020 | \n",
      "Epoch: 9407 | train_loss: 116.5117568970 | test_loss: 5.9107041359 | \n",
      "Epoch: 9408 | train_loss: 116.5116271973 | test_loss: 5.9106874466 | \n",
      "Epoch: 9409 | train_loss: 116.5115051270 | test_loss: 5.9106726646 | \n",
      "Epoch: 9410 | train_loss: 116.5113677979 | test_loss: 5.9106507301 | \n",
      "Epoch: 9411 | train_loss: 116.5112304688 | test_loss: 5.9106321335 | \n",
      "Epoch: 9412 | train_loss: 116.5110931396 | test_loss: 5.9106154442 | \n",
      "Epoch: 9413 | train_loss: 116.5109558105 | test_loss: 5.9105906487 | \n",
      "Epoch: 9414 | train_loss: 116.5108108521 | test_loss: 5.9105725288 | \n",
      "Epoch: 9415 | train_loss: 116.5107040405 | test_loss: 5.9105510712 | \n",
      "Epoch: 9416 | train_loss: 116.5105895996 | test_loss: 5.9105262756 | \n",
      "Epoch: 9417 | train_loss: 116.5104370117 | test_loss: 5.9105057716 | \n",
      "Epoch: 9418 | train_loss: 116.5103454590 | test_loss: 5.9104824066 | \n",
      "Epoch: 9419 | train_loss: 116.5101852417 | test_loss: 5.9104661942 | \n",
      "Epoch: 9420 | train_loss: 116.5100708008 | test_loss: 5.9104475975 | \n",
      "Epoch: 9421 | train_loss: 116.5099334717 | test_loss: 5.9104280472 | \n",
      "Epoch: 9422 | train_loss: 116.5098266602 | test_loss: 5.9104089737 | \n",
      "Epoch: 9423 | train_loss: 116.5096969604 | test_loss: 5.9103932381 | \n",
      "Epoch: 9424 | train_loss: 116.5095520020 | test_loss: 5.9103760719 | \n",
      "Epoch: 9425 | train_loss: 116.5094070435 | test_loss: 5.9103593826 | \n",
      "Epoch: 9426 | train_loss: 116.5093002319 | test_loss: 5.9103369713 | \n",
      "Epoch: 9427 | train_loss: 116.5091476440 | test_loss: 5.9103183746 | \n",
      "Epoch: 9428 | train_loss: 116.5090026855 | test_loss: 5.9102983475 | \n",
      "Epoch: 9429 | train_loss: 116.5088882446 | test_loss: 5.9102740288 | \n",
      "Epoch: 9430 | train_loss: 116.5087814331 | test_loss: 5.9102563858 | \n",
      "Epoch: 9431 | train_loss: 116.5086593628 | test_loss: 5.9102320671 | \n",
      "Epoch: 9432 | train_loss: 116.5085449219 | test_loss: 5.9102067947 | \n",
      "Epoch: 9433 | train_loss: 116.5084152222 | test_loss: 5.9101848602 | \n",
      "Epoch: 9434 | train_loss: 116.5083084106 | test_loss: 5.9101700783 | \n",
      "Epoch: 9435 | train_loss: 116.5081634521 | test_loss: 5.9101514816 | \n",
      "Epoch: 9436 | train_loss: 116.5079956055 | test_loss: 5.9101285934 | \n",
      "Epoch: 9437 | train_loss: 116.5078811646 | test_loss: 5.9101128578 | \n",
      "Epoch: 9438 | train_loss: 116.5077514648 | test_loss: 5.9100904465 | \n",
      "Epoch: 9439 | train_loss: 116.5076599121 | test_loss: 5.9100704193 | \n",
      "Epoch: 9440 | train_loss: 116.5075454712 | test_loss: 5.9100551605 | \n",
      "Epoch: 9441 | train_loss: 116.5074310303 | test_loss: 5.9100346565 | \n",
      "Epoch: 9442 | train_loss: 116.5072937012 | test_loss: 5.9100127220 | \n",
      "Epoch: 9443 | train_loss: 116.5071487427 | test_loss: 5.9099974632 | \n",
      "Epoch: 9444 | train_loss: 116.5070343018 | test_loss: 5.9099822044 | \n",
      "Epoch: 9445 | train_loss: 116.5068969727 | test_loss: 5.9099650383 | \n",
      "Epoch: 9446 | train_loss: 116.5067672729 | test_loss: 5.9099483490 | \n",
      "Epoch: 9447 | train_loss: 116.5066375732 | test_loss: 5.9099297523 | \n",
      "Epoch: 9448 | train_loss: 116.5064849854 | test_loss: 5.9099092484 | \n",
      "Epoch: 9449 | train_loss: 116.5063476562 | test_loss: 5.9098887444 | \n",
      "Epoch: 9450 | train_loss: 116.5062332153 | test_loss: 5.9098720551 | \n",
      "Epoch: 9451 | train_loss: 116.5061035156 | test_loss: 5.9098548889 | \n",
      "Epoch: 9452 | train_loss: 116.5059738159 | test_loss: 5.9098320007 | \n",
      "Epoch: 9453 | train_loss: 116.5058593750 | test_loss: 5.9098124504 | \n",
      "Epoch: 9454 | train_loss: 116.5056915283 | test_loss: 5.9097914696 | \n",
      "Epoch: 9455 | train_loss: 116.5056076050 | test_loss: 5.9097719193 | \n",
      "Epoch: 9456 | train_loss: 116.5054702759 | test_loss: 5.9097499847 | \n",
      "Epoch: 9457 | train_loss: 116.5053253174 | test_loss: 5.9097313881 | \n",
      "Epoch: 9458 | train_loss: 116.5051956177 | test_loss: 5.9097156525 | \n",
      "Epoch: 9459 | train_loss: 116.5050354004 | test_loss: 5.9096941948 | \n",
      "Epoch: 9460 | train_loss: 116.5049438477 | test_loss: 5.9096751213 | \n",
      "Epoch: 9461 | train_loss: 116.5048370361 | test_loss: 5.9096589088 | \n",
      "Epoch: 9462 | train_loss: 116.5046997070 | test_loss: 5.9096455574 | \n",
      "Epoch: 9463 | train_loss: 116.5046005249 | test_loss: 5.9096207619 | \n",
      "Epoch: 9464 | train_loss: 116.5044555664 | test_loss: 5.9095983505 | \n",
      "Epoch: 9465 | train_loss: 116.5043487549 | test_loss: 5.9095821381 | \n",
      "Epoch: 9466 | train_loss: 116.5041961670 | test_loss: 5.9095635414 | \n",
      "Epoch: 9467 | train_loss: 116.5040893555 | test_loss: 5.9095392227 | \n",
      "Epoch: 9468 | train_loss: 116.5039749146 | test_loss: 5.9095172882 | \n",
      "Epoch: 9469 | train_loss: 116.5038604736 | test_loss: 5.9095029831 | \n",
      "Epoch: 9470 | train_loss: 116.5037307739 | test_loss: 5.9094886780 | \n",
      "Epoch: 9471 | train_loss: 116.5035858154 | test_loss: 5.9094691277 | \n",
      "Epoch: 9472 | train_loss: 116.5034790039 | test_loss: 5.9094452858 | \n",
      "Epoch: 9473 | train_loss: 116.5033187866 | test_loss: 5.9094300270 | \n",
      "Epoch: 9474 | train_loss: 116.5032119751 | test_loss: 5.9094099998 | \n",
      "Epoch: 9475 | train_loss: 116.5030593872 | test_loss: 5.9093918800 | \n",
      "Epoch: 9476 | train_loss: 116.5029754639 | test_loss: 5.9093713760 | \n",
      "Epoch: 9477 | train_loss: 116.5028152466 | test_loss: 5.9093537331 | \n",
      "Epoch: 9478 | train_loss: 116.5026779175 | test_loss: 5.9093327522 | \n",
      "Epoch: 9479 | train_loss: 116.5025711060 | test_loss: 5.9093170166 | \n",
      "Epoch: 9480 | train_loss: 116.5024108887 | test_loss: 5.9092979431 | \n",
      "Epoch: 9481 | train_loss: 116.5022888184 | test_loss: 5.9092755318 | \n",
      "Epoch: 9482 | train_loss: 116.5021286011 | test_loss: 5.9092512131 | \n",
      "Epoch: 9483 | train_loss: 116.5020446777 | test_loss: 5.9092321396 | \n",
      "Epoch: 9484 | train_loss: 116.5018920898 | test_loss: 5.9092087746 | \n",
      "Epoch: 9485 | train_loss: 116.5017852783 | test_loss: 5.9091906548 | \n",
      "Epoch: 9486 | train_loss: 116.5016326904 | test_loss: 5.9091691971 | \n",
      "Epoch: 9487 | train_loss: 116.5015029907 | test_loss: 5.9091491699 | \n",
      "Epoch: 9488 | train_loss: 116.5013732910 | test_loss: 5.9091310501 | \n",
      "Epoch: 9489 | train_loss: 116.5012512207 | test_loss: 5.9091157913 | \n",
      "Epoch: 9490 | train_loss: 116.5011444092 | test_loss: 5.9091005325 | \n",
      "Epoch: 9491 | train_loss: 116.5010452271 | test_loss: 5.9090785980 | \n",
      "Epoch: 9492 | train_loss: 116.5009078979 | test_loss: 5.9090557098 | \n",
      "Epoch: 9493 | train_loss: 116.5007629395 | test_loss: 5.9090361595 | \n",
      "Epoch: 9494 | train_loss: 116.5006637573 | test_loss: 5.9090199471 | \n",
      "Epoch: 9495 | train_loss: 116.5005111694 | test_loss: 5.9090013504 | \n",
      "Epoch: 9496 | train_loss: 116.5003585815 | test_loss: 5.9089813232 | \n",
      "Epoch: 9497 | train_loss: 116.5002212524 | test_loss: 5.9089617729 | \n",
      "Epoch: 9498 | train_loss: 116.5000762939 | test_loss: 5.9089450836 | \n",
      "Epoch: 9499 | train_loss: 116.4999694824 | test_loss: 5.9089264870 | \n",
      "Epoch: 9500 | train_loss: 116.4998626709 | test_loss: 5.9089078903 | \n",
      "Epoch: 9501 | train_loss: 116.4997329712 | test_loss: 5.9088902473 | \n",
      "Epoch: 9502 | train_loss: 116.4996109009 | test_loss: 5.9088659286 | \n",
      "Epoch: 9503 | train_loss: 116.4994735718 | test_loss: 5.9088497162 | \n",
      "Epoch: 9504 | train_loss: 116.4993362427 | test_loss: 5.9088306427 | \n",
      "Epoch: 9505 | train_loss: 116.4992065430 | test_loss: 5.9088134766 | \n",
      "Epoch: 9506 | train_loss: 116.4990615845 | test_loss: 5.9087920189 | \n",
      "Epoch: 9507 | train_loss: 116.4989547729 | test_loss: 5.9087719917 | \n",
      "Epoch: 9508 | train_loss: 116.4988250732 | test_loss: 5.9087567329 | \n",
      "Epoch: 9509 | train_loss: 116.4986877441 | test_loss: 5.9087352753 | \n",
      "Epoch: 9510 | train_loss: 116.4985961914 | test_loss: 5.9087200165 | \n",
      "Epoch: 9511 | train_loss: 116.4985046387 | test_loss: 5.9087061882 | \n",
      "Epoch: 9512 | train_loss: 116.4983673096 | test_loss: 5.9086866379 | \n",
      "Epoch: 9513 | train_loss: 116.4982452393 | test_loss: 5.9086670876 | \n",
      "Epoch: 9514 | train_loss: 116.4981155396 | test_loss: 5.9086480141 | \n",
      "Epoch: 9515 | train_loss: 116.4979858398 | test_loss: 5.9086298943 | \n",
      "Epoch: 9516 | train_loss: 116.4978637695 | test_loss: 5.9086070061 | \n",
      "Epoch: 9517 | train_loss: 116.4977416992 | test_loss: 5.9085888863 | \n",
      "Epoch: 9518 | train_loss: 116.4976654053 | test_loss: 5.9085721970 | \n",
      "Epoch: 9519 | train_loss: 116.4975280762 | test_loss: 5.9085516930 | \n",
      "Epoch: 9520 | train_loss: 116.4974288940 | test_loss: 5.9085292816 | \n",
      "Epoch: 9521 | train_loss: 116.4973068237 | test_loss: 5.9085130692 | \n",
      "Epoch: 9522 | train_loss: 116.4971466064 | test_loss: 5.9084916115 | \n",
      "Epoch: 9523 | train_loss: 116.4970474243 | test_loss: 5.9084711075 | \n",
      "Epoch: 9524 | train_loss: 116.4969024658 | test_loss: 5.9084563255 | \n",
      "Epoch: 9525 | train_loss: 116.4967803955 | test_loss: 5.9084386826 | \n",
      "Epoch: 9526 | train_loss: 116.4966201782 | test_loss: 5.9084205627 | \n",
      "Epoch: 9527 | train_loss: 116.4965133667 | test_loss: 5.9083986282 | \n",
      "Epoch: 9528 | train_loss: 116.4963760376 | test_loss: 5.9083800316 | \n",
      "Epoch: 9529 | train_loss: 116.4962463379 | test_loss: 5.9083576202 | \n",
      "Epoch: 9530 | train_loss: 116.4961166382 | test_loss: 5.9083380699 | \n",
      "Epoch: 9531 | train_loss: 116.4960174561 | test_loss: 5.9083209038 | \n",
      "Epoch: 9532 | train_loss: 116.4958724976 | test_loss: 5.9083042145 | \n",
      "Epoch: 9533 | train_loss: 116.4957427979 | test_loss: 5.9082865715 | \n",
      "Epoch: 9534 | train_loss: 116.4956359863 | test_loss: 5.9082722664 | \n",
      "Epoch: 9535 | train_loss: 116.4954910278 | test_loss: 5.9082503319 | \n",
      "Epoch: 9536 | train_loss: 116.4953613281 | test_loss: 5.9082355499 | \n",
      "Epoch: 9537 | train_loss: 116.4952316284 | test_loss: 5.9082145691 | \n",
      "Epoch: 9538 | train_loss: 116.4951095581 | test_loss: 5.9081883430 | \n",
      "Epoch: 9539 | train_loss: 116.4949722290 | test_loss: 5.9081754684 | \n",
      "Epoch: 9540 | train_loss: 116.4948501587 | test_loss: 5.9081559181 | \n",
      "Epoch: 9541 | train_loss: 116.4946899414 | test_loss: 5.9081354141 | \n",
      "Epoch: 9542 | train_loss: 116.4945831299 | test_loss: 5.9081201553 | \n",
      "Epoch: 9543 | train_loss: 116.4944534302 | test_loss: 5.9081010818 | \n",
      "Epoch: 9544 | train_loss: 116.4943466187 | test_loss: 5.9080791473 | \n",
      "Epoch: 9545 | train_loss: 116.4942321777 | test_loss: 5.9080619812 | \n",
      "Epoch: 9546 | train_loss: 116.4941024780 | test_loss: 5.9080457687 | \n",
      "Epoch: 9547 | train_loss: 116.4939651489 | test_loss: 5.9080281258 | \n",
      "Epoch: 9548 | train_loss: 116.4938278198 | test_loss: 5.9080085754 | \n",
      "Epoch: 9549 | train_loss: 116.4937438965 | test_loss: 5.9079880714 | \n",
      "Epoch: 9550 | train_loss: 116.4936141968 | test_loss: 5.9079713821 | \n",
      "Epoch: 9551 | train_loss: 116.4934844971 | test_loss: 5.9079527855 | \n",
      "Epoch: 9552 | train_loss: 116.4933624268 | test_loss: 5.9079375267 | \n",
      "Epoch: 9553 | train_loss: 116.4932327271 | test_loss: 5.9079170227 | \n",
      "Epoch: 9554 | train_loss: 116.4931030273 | test_loss: 5.9079003334 | \n",
      "Epoch: 9555 | train_loss: 116.4929656982 | test_loss: 5.9078779221 | \n",
      "Epoch: 9556 | train_loss: 116.4928512573 | test_loss: 5.9078588486 | \n",
      "Epoch: 9557 | train_loss: 116.4927139282 | test_loss: 5.9078412056 | \n",
      "Epoch: 9558 | train_loss: 116.4926300049 | test_loss: 5.9078230858 | \n",
      "Epoch: 9559 | train_loss: 116.4924621582 | test_loss: 5.9078040123 | \n",
      "Epoch: 9560 | train_loss: 116.4923324585 | test_loss: 5.9077863693 | \n",
      "Epoch: 9561 | train_loss: 116.4922103882 | test_loss: 5.9077711105 | \n",
      "Epoch: 9562 | train_loss: 116.4920959473 | test_loss: 5.9077453613 | \n",
      "Epoch: 9563 | train_loss: 116.4919433594 | test_loss: 5.9077272415 | \n",
      "Epoch: 9564 | train_loss: 116.4918441772 | test_loss: 5.9077091217 | \n",
      "Epoch: 9565 | train_loss: 116.4916992188 | test_loss: 5.9076862335 | \n",
      "Epoch: 9566 | train_loss: 116.4915695190 | test_loss: 5.9076666832 | \n",
      "Epoch: 9567 | train_loss: 116.4914627075 | test_loss: 5.9076499939 | \n",
      "Epoch: 9568 | train_loss: 116.4913253784 | test_loss: 5.9076328278 | \n",
      "Epoch: 9569 | train_loss: 116.4912109375 | test_loss: 5.9076137543 | \n",
      "Epoch: 9570 | train_loss: 116.4910583496 | test_loss: 5.9075942039 | \n",
      "Epoch: 9571 | train_loss: 116.4909667969 | test_loss: 5.9075737000 | \n",
      "Epoch: 9572 | train_loss: 116.4908370972 | test_loss: 5.9075560570 | \n",
      "Epoch: 9573 | train_loss: 116.4907150269 | test_loss: 5.9075365067 | \n",
      "Epoch: 9574 | train_loss: 116.4905853271 | test_loss: 5.9075212479 | \n",
      "Epoch: 9575 | train_loss: 116.4904556274 | test_loss: 5.9075036049 | \n",
      "Epoch: 9576 | train_loss: 116.4903411865 | test_loss: 5.9074816704 | \n",
      "Epoch: 9577 | train_loss: 116.4902343750 | test_loss: 5.9074583054 | \n",
      "Epoch: 9578 | train_loss: 116.4901046753 | test_loss: 5.9074468613 | \n",
      "Epoch: 9579 | train_loss: 116.4899902344 | test_loss: 5.9074254036 | \n",
      "Epoch: 9580 | train_loss: 116.4898376465 | test_loss: 5.9074068069 | \n",
      "Epoch: 9581 | train_loss: 116.4897155762 | test_loss: 5.9073886871 | \n",
      "Epoch: 9582 | train_loss: 116.4896087646 | test_loss: 5.9073705673 | \n",
      "Epoch: 9583 | train_loss: 116.4894638062 | test_loss: 5.9073448181 | \n",
      "Epoch: 9584 | train_loss: 116.4893035889 | test_loss: 5.9073286057 | \n",
      "Epoch: 9585 | train_loss: 116.4892120361 | test_loss: 5.9073095322 | \n",
      "Epoch: 9586 | train_loss: 116.4890747070 | test_loss: 5.9072904587 | \n",
      "Epoch: 9587 | train_loss: 116.4889068604 | test_loss: 5.9072709084 | \n",
      "Epoch: 9588 | train_loss: 116.4887771606 | test_loss: 5.9072532654 | \n",
      "Epoch: 9589 | train_loss: 116.4886627197 | test_loss: 5.9072337151 | \n",
      "Epoch: 9590 | train_loss: 116.4885330200 | test_loss: 5.9072127342 | \n",
      "Epoch: 9591 | train_loss: 116.4884109497 | test_loss: 5.9071936607 | \n",
      "Epoch: 9592 | train_loss: 116.4882812500 | test_loss: 5.9071779251 | \n",
      "Epoch: 9593 | train_loss: 116.4881362915 | test_loss: 5.9071626663 | \n",
      "Epoch: 9594 | train_loss: 116.4880142212 | test_loss: 5.9071421623 | \n",
      "Epoch: 9595 | train_loss: 116.4879074097 | test_loss: 5.9071216583 | \n",
      "Epoch: 9596 | train_loss: 116.4877624512 | test_loss: 5.9071049690 | \n",
      "Epoch: 9597 | train_loss: 116.4876251221 | test_loss: 5.9070868492 | \n",
      "Epoch: 9598 | train_loss: 116.4874725342 | test_loss: 5.9070615768 | \n",
      "Epoch: 9599 | train_loss: 116.4873352051 | test_loss: 5.9070458412 | \n",
      "Epoch: 9600 | train_loss: 116.4872589111 | test_loss: 5.9070243835 | \n",
      "Epoch: 9601 | train_loss: 116.4871368408 | test_loss: 5.9070100784 | \n",
      "Epoch: 9602 | train_loss: 116.4870147705 | test_loss: 5.9069895744 | \n",
      "Epoch: 9603 | train_loss: 116.4868621826 | test_loss: 5.9069733620 | \n",
      "Epoch: 9604 | train_loss: 116.4867782593 | test_loss: 5.9069547653 | \n",
      "Epoch: 9605 | train_loss: 116.4866180420 | test_loss: 5.9069404602 | \n",
      "Epoch: 9606 | train_loss: 116.4865341187 | test_loss: 5.9069213867 | \n",
      "Epoch: 9607 | train_loss: 116.4864044189 | test_loss: 5.9069013596 | \n",
      "Epoch: 9608 | train_loss: 116.4862670898 | test_loss: 5.9068803787 | \n",
      "Epoch: 9609 | train_loss: 116.4861450195 | test_loss: 5.9068627357 | \n",
      "Epoch: 9610 | train_loss: 116.4860076904 | test_loss: 5.9068450928 | \n",
      "Epoch: 9611 | train_loss: 116.4859085083 | test_loss: 5.9068274498 | \n",
      "Epoch: 9612 | train_loss: 116.4857788086 | test_loss: 5.9068140984 | \n",
      "Epoch: 9613 | train_loss: 116.4856338501 | test_loss: 5.9067907333 | \n",
      "Epoch: 9614 | train_loss: 116.4855270386 | test_loss: 5.9067759514 | \n",
      "Epoch: 9615 | train_loss: 116.4853744507 | test_loss: 5.9067554474 | \n",
      "Epoch: 9616 | train_loss: 116.4852523804 | test_loss: 5.9067363739 | \n",
      "Epoch: 9617 | train_loss: 116.4851303101 | test_loss: 5.9067168236 | \n",
      "Epoch: 9618 | train_loss: 116.4849929810 | test_loss: 5.9066953659 | \n",
      "Epoch: 9619 | train_loss: 116.4848632812 | test_loss: 5.9066791534 | \n",
      "Epoch: 9620 | train_loss: 116.4847488403 | test_loss: 5.9066634178 | \n",
      "Epoch: 9621 | train_loss: 116.4846572876 | test_loss: 5.9066448212 | \n",
      "Epoch: 9622 | train_loss: 116.4845275879 | test_loss: 5.9066290855 | \n",
      "Epoch: 9623 | train_loss: 116.4844131470 | test_loss: 5.9066133499 | \n",
      "Epoch: 9624 | train_loss: 116.4842834473 | test_loss: 5.9065961838 | \n",
      "Epoch: 9625 | train_loss: 116.4841766357 | test_loss: 5.9065809250 | \n",
      "Epoch: 9626 | train_loss: 116.4840240479 | test_loss: 5.9065594673 | \n",
      "Epoch: 9627 | train_loss: 116.4839096069 | test_loss: 5.9065418243 | \n",
      "Epoch: 9628 | train_loss: 116.4837875366 | test_loss: 5.9065232277 | \n",
      "Epoch: 9629 | train_loss: 116.4836807251 | test_loss: 5.9065065384 | \n",
      "Epoch: 9630 | train_loss: 116.4835739136 | test_loss: 5.9064898491 | \n",
      "Epoch: 9631 | train_loss: 116.4834594727 | test_loss: 5.9064741135 | \n",
      "Epoch: 9632 | train_loss: 116.4833297729 | test_loss: 5.9064550400 | \n",
      "Epoch: 9633 | train_loss: 116.4832153320 | test_loss: 5.9064393044 | \n",
      "Epoch: 9634 | train_loss: 116.4830856323 | test_loss: 5.9064183235 | \n",
      "Epoch: 9635 | train_loss: 116.4829559326 | test_loss: 5.9064002037 | \n",
      "Epoch: 9636 | train_loss: 116.4828414917 | test_loss: 5.9063849449 | \n",
      "Epoch: 9637 | train_loss: 116.4827270508 | test_loss: 5.9063663483 | \n",
      "Epoch: 9638 | train_loss: 116.4825744629 | test_loss: 5.9063434601 | \n",
      "Epoch: 9639 | train_loss: 116.4824676514 | test_loss: 5.9063310623 | \n",
      "Epoch: 9640 | train_loss: 116.4823303223 | test_loss: 5.9063053131 | \n",
      "Epoch: 9641 | train_loss: 116.4822006226 | test_loss: 5.9062895775 | \n",
      "Epoch: 9642 | train_loss: 116.4821090698 | test_loss: 5.9062671661 | \n",
      "Epoch: 9643 | train_loss: 116.4819793701 | test_loss: 5.9062547684 | \n",
      "Epoch: 9644 | train_loss: 116.4818572998 | test_loss: 5.9062347412 | \n",
      "Epoch: 9645 | train_loss: 116.4816970825 | test_loss: 5.9062175751 | \n",
      "Epoch: 9646 | train_loss: 116.4815979004 | test_loss: 5.9062004089 | \n",
      "Epoch: 9647 | train_loss: 116.4814758301 | test_loss: 5.9061794281 | \n",
      "Epoch: 9648 | train_loss: 116.4813766479 | test_loss: 5.9061646461 | \n",
      "Epoch: 9649 | train_loss: 116.4812774658 | test_loss: 5.9061446190 | \n",
      "Epoch: 9650 | train_loss: 116.4811325073 | test_loss: 5.9061293602 | \n",
      "Epoch: 9651 | train_loss: 116.4810180664 | test_loss: 5.9061112404 | \n",
      "Epoch: 9652 | train_loss: 116.4808883667 | test_loss: 5.9060907364 | \n",
      "Epoch: 9653 | train_loss: 116.4807891846 | test_loss: 5.9060692787 | \n",
      "Epoch: 9654 | train_loss: 116.4806671143 | test_loss: 5.9060559273 | \n",
      "Epoch: 9655 | train_loss: 116.4804916382 | test_loss: 5.9060359001 | \n",
      "Epoch: 9656 | train_loss: 116.4803771973 | test_loss: 5.9060144424 | \n",
      "Epoch: 9657 | train_loss: 116.4802474976 | test_loss: 5.9059934616 | \n",
      "Epoch: 9658 | train_loss: 116.4801330566 | test_loss: 5.9059748650 | \n",
      "Epoch: 9659 | train_loss: 116.4799880981 | test_loss: 5.9059576988 | \n",
      "Epoch: 9660 | train_loss: 116.4798660278 | test_loss: 5.9059419632 | \n",
      "Epoch: 9661 | train_loss: 116.4797439575 | test_loss: 5.9059233665 | \n",
      "Epoch: 9662 | train_loss: 116.4796676636 | test_loss: 5.9059033394 | \n",
      "Epoch: 9663 | train_loss: 116.4795379639 | test_loss: 5.9058852196 | \n",
      "Epoch: 9664 | train_loss: 116.4794006348 | test_loss: 5.9058642387 | \n",
      "Epoch: 9665 | train_loss: 116.4792785645 | test_loss: 5.9058508873 | \n",
      "Epoch: 9666 | train_loss: 116.4791488647 | test_loss: 5.9058299065 | \n",
      "Epoch: 9667 | train_loss: 116.4790649414 | test_loss: 5.9058103561 | \n",
      "Epoch: 9668 | train_loss: 116.4789123535 | test_loss: 5.9057893753 | \n",
      "Epoch: 9669 | train_loss: 116.4787902832 | test_loss: 5.9057769775 | \n",
      "Epoch: 9670 | train_loss: 116.4786834717 | test_loss: 5.9057602882 | \n",
      "Epoch: 9671 | train_loss: 116.4785766602 | test_loss: 5.9057369232 | \n",
      "Epoch: 9672 | train_loss: 116.4784393311 | test_loss: 5.9057221413 | \n",
      "Epoch: 9673 | train_loss: 116.4782943726 | test_loss: 5.9057068825 | \n",
      "Epoch: 9674 | train_loss: 116.4781723022 | test_loss: 5.9056911469 | \n",
      "Epoch: 9675 | train_loss: 116.4780502319 | test_loss: 5.9056739807 | \n",
      "Epoch: 9676 | train_loss: 116.4779281616 | test_loss: 5.9056510925 | \n",
      "Epoch: 9677 | train_loss: 116.4778213501 | test_loss: 5.9056358337 | \n",
      "Epoch: 9678 | train_loss: 116.4776763916 | test_loss: 5.9056158066 | \n",
      "Epoch: 9679 | train_loss: 116.4775695801 | test_loss: 5.9055981636 | \n",
      "Epoch: 9680 | train_loss: 116.4774627686 | test_loss: 5.9055805206 | \n",
      "Epoch: 9681 | train_loss: 116.4773559570 | test_loss: 5.9055571556 | \n",
      "Epoch: 9682 | train_loss: 116.4772338867 | test_loss: 5.9055418968 | \n",
      "Epoch: 9683 | train_loss: 116.4770965576 | test_loss: 5.9055261612 | \n",
      "Epoch: 9684 | train_loss: 116.4769515991 | test_loss: 5.9055070877 | \n",
      "Epoch: 9685 | train_loss: 116.4768295288 | test_loss: 5.9054956436 | \n",
      "Epoch: 9686 | train_loss: 116.4767150879 | test_loss: 5.9054756165 | \n",
      "Epoch: 9687 | train_loss: 116.4765701294 | test_loss: 5.9054546356 | \n",
      "Epoch: 9688 | train_loss: 116.4764709473 | test_loss: 5.9054374695 | \n",
      "Epoch: 9689 | train_loss: 116.4763412476 | test_loss: 5.9054169655 | \n",
      "Epoch: 9690 | train_loss: 116.4762115479 | test_loss: 5.9053950310 | \n",
      "Epoch: 9691 | train_loss: 116.4760742188 | test_loss: 5.9053764343 | \n",
      "Epoch: 9692 | train_loss: 116.4759597778 | test_loss: 5.9053606987 | \n",
      "Epoch: 9693 | train_loss: 116.4758071899 | test_loss: 5.9053449631 | \n",
      "Epoch: 9694 | train_loss: 116.4757003784 | test_loss: 5.9053282738 | \n",
      "Epoch: 9695 | train_loss: 116.4755554199 | test_loss: 5.9053134918 | \n",
      "Epoch: 9696 | train_loss: 116.4754486084 | test_loss: 5.9052953720 | \n",
      "Epoch: 9697 | train_loss: 116.4753112793 | test_loss: 5.9052782059 | \n",
      "Epoch: 9698 | train_loss: 116.4752349854 | test_loss: 5.9052600861 | \n",
      "Epoch: 9699 | train_loss: 116.4750900269 | test_loss: 5.9052448273 | \n",
      "Epoch: 9700 | train_loss: 116.4749603271 | test_loss: 5.9052228928 | \n",
      "Epoch: 9701 | train_loss: 116.4748306274 | test_loss: 5.9052066803 | \n",
      "Epoch: 9702 | train_loss: 116.4746780396 | test_loss: 5.9051861763 | \n",
      "Epoch: 9703 | train_loss: 116.4746017456 | test_loss: 5.9051690102 | \n",
      "Epoch: 9704 | train_loss: 116.4744415283 | test_loss: 5.9051485062 | \n",
      "Epoch: 9705 | train_loss: 116.4743652344 | test_loss: 5.9051294327 | \n",
      "Epoch: 9706 | train_loss: 116.4742279053 | test_loss: 5.9051084518 | \n",
      "Epoch: 9707 | train_loss: 116.4741134644 | test_loss: 5.9050931931 | \n",
      "Epoch: 9708 | train_loss: 116.4739837646 | test_loss: 5.9050731659 | \n",
      "Epoch: 9709 | train_loss: 116.4738540649 | test_loss: 5.9050531387 | \n",
      "Epoch: 9710 | train_loss: 116.4736938477 | test_loss: 5.9050364494 | \n",
      "Epoch: 9711 | train_loss: 116.4735870361 | test_loss: 5.9050140381 | \n",
      "Epoch: 9712 | train_loss: 116.4734573364 | test_loss: 5.9050045013 | \n",
      "Epoch: 9713 | train_loss: 116.4733428955 | test_loss: 5.9049839973 | \n",
      "Epoch: 9714 | train_loss: 116.4732055664 | test_loss: 5.9049663544 | \n",
      "Epoch: 9715 | train_loss: 116.4731140137 | test_loss: 5.9049506187 | \n",
      "Epoch: 9716 | train_loss: 116.4729995728 | test_loss: 5.9049296379 | \n",
      "Epoch: 9717 | train_loss: 116.4728240967 | test_loss: 5.9049139023 | \n",
      "Epoch: 9718 | train_loss: 116.4727249146 | test_loss: 5.9048957825 | \n",
      "Epoch: 9719 | train_loss: 116.4726257324 | test_loss: 5.9048752785 | \n",
      "Epoch: 9720 | train_loss: 116.4724884033 | test_loss: 5.9048542976 | \n",
      "Epoch: 9721 | train_loss: 116.4723663330 | test_loss: 5.9048380852 | \n",
      "Epoch: 9722 | train_loss: 116.4722442627 | test_loss: 5.9048194885 | \n",
      "Epoch: 9723 | train_loss: 116.4721221924 | test_loss: 5.9047975540 | \n",
      "Epoch: 9724 | train_loss: 116.4720153809 | test_loss: 5.9047770500 | \n",
      "Epoch: 9725 | train_loss: 116.4718780518 | test_loss: 5.9047627449 | \n",
      "Epoch: 9726 | train_loss: 116.4717102051 | test_loss: 5.9047436714 | \n",
      "Epoch: 9727 | train_loss: 116.4716186523 | test_loss: 5.9047193527 | \n",
      "Epoch: 9728 | train_loss: 116.4714736938 | test_loss: 5.9046983719 | \n",
      "Epoch: 9729 | train_loss: 116.4713592529 | test_loss: 5.9046821594 | \n",
      "Epoch: 9730 | train_loss: 116.4712295532 | test_loss: 5.9046635628 | \n",
      "Epoch: 9731 | train_loss: 116.4710922241 | test_loss: 5.9046440125 | \n",
      "Epoch: 9732 | train_loss: 116.4710083008 | test_loss: 5.9046263695 | \n",
      "Epoch: 9733 | train_loss: 116.4708557129 | test_loss: 5.9046111107 | \n",
      "Epoch: 9734 | train_loss: 116.4707412720 | test_loss: 5.9045896530 | \n",
      "Epoch: 9735 | train_loss: 116.4705963135 | test_loss: 5.9045720100 | \n",
      "Epoch: 9736 | train_loss: 116.4704971313 | test_loss: 5.9045548439 | \n",
      "Epoch: 9737 | train_loss: 116.4703826904 | test_loss: 5.9045348167 | \n",
      "Epoch: 9738 | train_loss: 116.4702224731 | test_loss: 5.9045128822 | \n",
      "Epoch: 9739 | train_loss: 116.4700622559 | test_loss: 5.9044966698 | \n",
      "Epoch: 9740 | train_loss: 116.4699707031 | test_loss: 5.9044799805 | \n",
      "Epoch: 9741 | train_loss: 116.4698562622 | test_loss: 5.9044609070 | \n",
      "Epoch: 9742 | train_loss: 116.4697265625 | test_loss: 5.9044413567 | \n",
      "Epoch: 9743 | train_loss: 116.4696121216 | test_loss: 5.9044256210 | \n",
      "Epoch: 9744 | train_loss: 116.4695053101 | test_loss: 5.9044055939 | \n",
      "Epoch: 9745 | train_loss: 116.4693450928 | test_loss: 5.9043912888 | \n",
      "Epoch: 9746 | train_loss: 116.4692153931 | test_loss: 5.9043760300 | \n",
      "Epoch: 9747 | train_loss: 116.4691238403 | test_loss: 5.9043540955 | \n",
      "Epoch: 9748 | train_loss: 116.4690017700 | test_loss: 5.9043383598 | \n",
      "Epoch: 9749 | train_loss: 116.4688644409 | test_loss: 5.9043178558 | \n",
      "Epoch: 9750 | train_loss: 116.4687194824 | test_loss: 5.9043011665 | \n",
      "Epoch: 9751 | train_loss: 116.4685974121 | test_loss: 5.9042844772 | \n",
      "Epoch: 9752 | train_loss: 116.4685058594 | test_loss: 5.9042620659 | \n",
      "Epoch: 9753 | train_loss: 116.4683914185 | test_loss: 5.9042482376 | \n",
      "Epoch: 9754 | train_loss: 116.4682693481 | test_loss: 5.9042296410 | \n",
      "Epoch: 9755 | train_loss: 116.4681320190 | test_loss: 5.9042115211 | \n",
      "Epoch: 9756 | train_loss: 116.4680099487 | test_loss: 5.9041972160 | \n",
      "Epoch: 9757 | train_loss: 116.4678802490 | test_loss: 5.9041810036 | \n",
      "Epoch: 9758 | train_loss: 116.4677963257 | test_loss: 5.9041624069 | \n",
      "Epoch: 9759 | train_loss: 116.4676589966 | test_loss: 5.9041442871 | \n",
      "Epoch: 9760 | train_loss: 116.4675369263 | test_loss: 5.9041242599 | \n",
      "Epoch: 9761 | train_loss: 116.4674148560 | test_loss: 5.9041075706 | \n",
      "Epoch: 9762 | train_loss: 116.4672927856 | test_loss: 5.9040899277 | \n",
      "Epoch: 9763 | train_loss: 116.4671783447 | test_loss: 5.9040765762 | \n",
      "Epoch: 9764 | train_loss: 116.4670639038 | test_loss: 5.9040589333 | \n",
      "Epoch: 9765 | train_loss: 116.4669265747 | test_loss: 5.9040379524 | \n",
      "Epoch: 9766 | train_loss: 116.4667892456 | test_loss: 5.9040217400 | \n",
      "Epoch: 9767 | train_loss: 116.4666900635 | test_loss: 5.9040074348 | \n",
      "Epoch: 9768 | train_loss: 116.4665832520 | test_loss: 5.9039897919 | \n",
      "Epoch: 9769 | train_loss: 116.4664535522 | test_loss: 5.9039769173 | \n",
      "Epoch: 9770 | train_loss: 116.4664001465 | test_loss: 5.9039583206 | \n",
      "Epoch: 9771 | train_loss: 116.4662322998 | test_loss: 5.9039373398 | \n",
      "Epoch: 9772 | train_loss: 116.4661254883 | test_loss: 5.9039244652 | \n",
      "Epoch: 9773 | train_loss: 116.4660034180 | test_loss: 5.9039058685 | \n",
      "Epoch: 9774 | train_loss: 116.4658508301 | test_loss: 5.9038872719 | \n",
      "Epoch: 9775 | train_loss: 116.4657211304 | test_loss: 5.9038701057 | \n",
      "Epoch: 9776 | train_loss: 116.4656219482 | test_loss: 5.9038496017 | \n",
      "Epoch: 9777 | train_loss: 116.4655075073 | test_loss: 5.9038352966 | \n",
      "Epoch: 9778 | train_loss: 116.4653320312 | test_loss: 5.9038171768 | \n",
      "Epoch: 9779 | train_loss: 116.4652328491 | test_loss: 5.9038014412 | \n",
      "Epoch: 9780 | train_loss: 116.4651107788 | test_loss: 5.9037890434 | \n",
      "Epoch: 9781 | train_loss: 116.4649963379 | test_loss: 5.9037613869 | \n",
      "Epoch: 9782 | train_loss: 116.4648742676 | test_loss: 5.9037427902 | \n",
      "Epoch: 9783 | train_loss: 116.4647521973 | test_loss: 5.9037227631 | \n",
      "Epoch: 9784 | train_loss: 116.4646453857 | test_loss: 5.9037103653 | \n",
      "Epoch: 9785 | train_loss: 116.4644927979 | test_loss: 5.9036912918 | \n",
      "Epoch: 9786 | train_loss: 116.4643783569 | test_loss: 5.9036750793 | \n",
      "Epoch: 9787 | train_loss: 116.4642639160 | test_loss: 5.9036583900 | \n",
      "Epoch: 9788 | train_loss: 116.4641342163 | test_loss: 5.9036388397 | \n",
      "Epoch: 9789 | train_loss: 116.4640121460 | test_loss: 5.9036183357 | \n",
      "Epoch: 9790 | train_loss: 116.4639053345 | test_loss: 5.9036011696 | \n",
      "Epoch: 9791 | train_loss: 116.4637985229 | test_loss: 5.9035820961 | \n",
      "Epoch: 9792 | train_loss: 116.4636993408 | test_loss: 5.9035658836 | \n",
      "Epoch: 9793 | train_loss: 116.4635925293 | test_loss: 5.9035496712 | \n",
      "Epoch: 9794 | train_loss: 116.4634704590 | test_loss: 5.9035315514 | \n",
      "Epoch: 9795 | train_loss: 116.4633407593 | test_loss: 5.9035143852 | \n",
      "Epoch: 9796 | train_loss: 116.4632263184 | test_loss: 5.9035010338 | \n",
      "Epoch: 9797 | train_loss: 116.4631042480 | test_loss: 5.9034843445 | \n",
      "Epoch: 9798 | train_loss: 116.4630050659 | test_loss: 5.9034667015 | \n",
      "Epoch: 9799 | train_loss: 116.4628601074 | test_loss: 5.9034509659 | \n",
      "Epoch: 9800 | train_loss: 116.4627609253 | test_loss: 5.9034361839 | \n",
      "Epoch: 9801 | train_loss: 116.4626464844 | test_loss: 5.9034180641 | \n",
      "Epoch: 9802 | train_loss: 116.4625701904 | test_loss: 5.9034013748 | \n",
      "Epoch: 9803 | train_loss: 116.4624404907 | test_loss: 5.9033799171 | \n",
      "Epoch: 9804 | train_loss: 116.4623260498 | test_loss: 5.9033675194 | \n",
      "Epoch: 9805 | train_loss: 116.4621887207 | test_loss: 5.9033527374 | \n",
      "Epoch: 9806 | train_loss: 116.4620971680 | test_loss: 5.9033298492 | \n",
      "Epoch: 9807 | train_loss: 116.4619827271 | test_loss: 5.9033145905 | \n",
      "Epoch: 9808 | train_loss: 116.4618682861 | test_loss: 5.9032988548 | \n",
      "Epoch: 9809 | train_loss: 116.4617614746 | test_loss: 5.9032783508 | \n",
      "Epoch: 9810 | train_loss: 116.4616317749 | test_loss: 5.9032659531 | \n",
      "Epoch: 9811 | train_loss: 116.4615325928 | test_loss: 5.9032483101 | \n",
      "Epoch: 9812 | train_loss: 116.4613876343 | test_loss: 5.9032292366 | \n",
      "Epoch: 9813 | train_loss: 116.4612808228 | test_loss: 5.9032154083 | \n",
      "Epoch: 9814 | train_loss: 116.4611587524 | test_loss: 5.9031977654 | \n",
      "Epoch: 9815 | train_loss: 116.4610137939 | test_loss: 5.9031810760 | \n",
      "Epoch: 9816 | train_loss: 116.4608917236 | test_loss: 5.9031581879 | \n",
      "Epoch: 9817 | train_loss: 116.4607849121 | test_loss: 5.9031352997 | \n",
      "Epoch: 9818 | train_loss: 116.4606246948 | test_loss: 5.9031147957 | \n",
      "Epoch: 9819 | train_loss: 116.4605255127 | test_loss: 5.9030961990 | \n",
      "Epoch: 9820 | train_loss: 116.4604034424 | test_loss: 5.9030799866 | \n",
      "Epoch: 9821 | train_loss: 116.4602508545 | test_loss: 5.9030570984 | \n",
      "Epoch: 9822 | train_loss: 116.4601440430 | test_loss: 5.9030399323 | \n",
      "Epoch: 9823 | train_loss: 116.4600372314 | test_loss: 5.9030113220 | \n",
      "Epoch: 9824 | train_loss: 116.4598846436 | test_loss: 5.9029936790 | \n",
      "Epoch: 9825 | train_loss: 116.4597930908 | test_loss: 5.9029788971 | \n",
      "Epoch: 9826 | train_loss: 116.4596481323 | test_loss: 5.9029636383 | \n",
      "Epoch: 9827 | train_loss: 116.4594726562 | test_loss: 5.9029412270 | \n",
      "Epoch: 9828 | train_loss: 116.4594039917 | test_loss: 5.9029278755 | \n",
      "Epoch: 9829 | train_loss: 116.4592895508 | test_loss: 5.9029097557 | \n",
      "Epoch: 9830 | train_loss: 116.4591369629 | test_loss: 5.9028925896 | \n",
      "Epoch: 9831 | train_loss: 116.4590148926 | test_loss: 5.9028778076 | \n",
      "Epoch: 9832 | train_loss: 116.4588928223 | test_loss: 5.9028620720 | \n",
      "Epoch: 9833 | train_loss: 116.4588012695 | test_loss: 5.9028401375 | \n",
      "Epoch: 9834 | train_loss: 116.4586791992 | test_loss: 5.9028224945 | \n",
      "Epoch: 9835 | train_loss: 116.4585342407 | test_loss: 5.9027991295 | \n",
      "Epoch: 9836 | train_loss: 116.4583969116 | test_loss: 5.9027833939 | \n",
      "Epoch: 9837 | train_loss: 116.4582901001 | test_loss: 5.9027662277 | \n",
      "Epoch: 9838 | train_loss: 116.4581451416 | test_loss: 5.9027509689 | \n",
      "Epoch: 9839 | train_loss: 116.4580459595 | test_loss: 5.9027338028 | \n",
      "Epoch: 9840 | train_loss: 116.4579391479 | test_loss: 5.9027128220 | \n",
      "Epoch: 9841 | train_loss: 116.4578247070 | test_loss: 5.9026966095 | \n",
      "Epoch: 9842 | train_loss: 116.4577255249 | test_loss: 5.9026837349 | \n",
      "Epoch: 9843 | train_loss: 116.4576110840 | test_loss: 5.9026646614 | \n",
      "Epoch: 9844 | train_loss: 116.4574813843 | test_loss: 5.9026474953 | \n",
      "Epoch: 9845 | train_loss: 116.4573669434 | test_loss: 5.9026341438 | \n",
      "Epoch: 9846 | train_loss: 116.4572525024 | test_loss: 5.9026169777 | \n",
      "Epoch: 9847 | train_loss: 116.4571990967 | test_loss: 5.9026007652 | \n",
      "Epoch: 9848 | train_loss: 116.4570388794 | test_loss: 5.9025855064 | \n",
      "Epoch: 9849 | train_loss: 116.4569168091 | test_loss: 5.9025645256 | \n",
      "Epoch: 9850 | train_loss: 116.4567947388 | test_loss: 5.9025435448 | \n",
      "Epoch: 9851 | train_loss: 116.4566726685 | test_loss: 5.9025330544 | \n",
      "Epoch: 9852 | train_loss: 116.4565811157 | test_loss: 5.9025115967 | \n",
      "Epoch: 9853 | train_loss: 116.4564666748 | test_loss: 5.9024977684 | \n",
      "Epoch: 9854 | train_loss: 116.4563598633 | test_loss: 5.9024796486 | \n",
      "Epoch: 9855 | train_loss: 116.4562225342 | test_loss: 5.9024629593 | \n",
      "Epoch: 9856 | train_loss: 116.4560852051 | test_loss: 5.9024491310 | \n",
      "Epoch: 9857 | train_loss: 116.4559783936 | test_loss: 5.9024310112 | \n",
      "Epoch: 9858 | train_loss: 116.4558868408 | test_loss: 5.9024128914 | \n",
      "Epoch: 9859 | train_loss: 116.4557647705 | test_loss: 5.9023938179 | \n",
      "Epoch: 9860 | train_loss: 116.4556427002 | test_loss: 5.9023742676 | \n",
      "Epoch: 9861 | train_loss: 116.4554901123 | test_loss: 5.9023571014 | \n",
      "Epoch: 9862 | train_loss: 116.4553680420 | test_loss: 5.9023380280 | \n",
      "Epoch: 9863 | train_loss: 116.4552459717 | test_loss: 5.9023227692 | \n",
      "Epoch: 9864 | train_loss: 116.4551467896 | test_loss: 5.9023065567 | \n",
      "Epoch: 9865 | train_loss: 116.4550399780 | test_loss: 5.9022870064 | \n",
      "Epoch: 9866 | train_loss: 116.4548797607 | test_loss: 5.9022765160 | \n",
      "Epoch: 9867 | train_loss: 116.4547424316 | test_loss: 5.9022617340 | \n",
      "Epoch: 9868 | train_loss: 116.4546508789 | test_loss: 5.9022493362 | \n",
      "Epoch: 9869 | train_loss: 116.4544906616 | test_loss: 5.9022340775 | \n",
      "Epoch: 9870 | train_loss: 116.4543762207 | test_loss: 5.9022212029 | \n",
      "Epoch: 9871 | train_loss: 116.4542541504 | test_loss: 5.9022064209 | \n",
      "Epoch: 9872 | train_loss: 116.4541320801 | test_loss: 5.9021916389 | \n",
      "Epoch: 9873 | train_loss: 116.4540328979 | test_loss: 5.9021697044 | \n",
      "Epoch: 9874 | train_loss: 116.4539260864 | test_loss: 5.9021525383 | \n",
      "Epoch: 9875 | train_loss: 116.4537963867 | test_loss: 5.9021358490 | \n",
      "Epoch: 9876 | train_loss: 116.4536895752 | test_loss: 5.9021153450 | \n",
      "Epoch: 9877 | train_loss: 116.4535369873 | test_loss: 5.9020938873 | \n",
      "Epoch: 9878 | train_loss: 116.4534301758 | test_loss: 5.9020838737 | \n",
      "Epoch: 9879 | train_loss: 116.4533004761 | test_loss: 5.9020566940 | \n",
      "Epoch: 9880 | train_loss: 116.4531936646 | test_loss: 5.9020371437 | \n",
      "Epoch: 9881 | train_loss: 116.4530487061 | test_loss: 5.9020252228 | \n",
      "Epoch: 9882 | train_loss: 116.4529113770 | test_loss: 5.9020066261 | \n",
      "Epoch: 9883 | train_loss: 116.4528121948 | test_loss: 5.9019885063 | \n",
      "Epoch: 9884 | train_loss: 116.4526672363 | test_loss: 5.9019689560 | \n",
      "Epoch: 9885 | train_loss: 116.4525451660 | test_loss: 5.9019541740 | \n",
      "Epoch: 9886 | train_loss: 116.4524307251 | test_loss: 5.9019379616 | \n",
      "Epoch: 9887 | train_loss: 116.4523239136 | test_loss: 5.9019203186 | \n",
      "Epoch: 9888 | train_loss: 116.4522094727 | test_loss: 5.9019036293 | \n",
      "Epoch: 9889 | train_loss: 116.4521102905 | test_loss: 5.9018859863 | \n",
      "Epoch: 9890 | train_loss: 116.4519805908 | test_loss: 5.9018669128 | \n",
      "Epoch: 9891 | train_loss: 116.4518661499 | test_loss: 5.9018526077 | \n",
      "Epoch: 9892 | train_loss: 116.4517517090 | test_loss: 5.9018387794 | \n",
      "Epoch: 9893 | train_loss: 116.4516525269 | test_loss: 5.9018158913 | \n",
      "Epoch: 9894 | train_loss: 116.4515304565 | test_loss: 5.9017920494 | \n",
      "Epoch: 9895 | train_loss: 116.4514083862 | test_loss: 5.9017772675 | \n",
      "Epoch: 9896 | train_loss: 116.4512786865 | test_loss: 5.9017524719 | \n",
      "Epoch: 9897 | train_loss: 116.4511795044 | test_loss: 5.9017391205 | \n",
      "Epoch: 9898 | train_loss: 116.4510345459 | test_loss: 5.9017205238 | \n",
      "Epoch: 9899 | train_loss: 116.4509201050 | test_loss: 5.9017024040 | \n",
      "Epoch: 9900 | train_loss: 116.4508132935 | test_loss: 5.9016838074 | \n",
      "Epoch: 9901 | train_loss: 116.4506835938 | test_loss: 5.9016723633 | \n",
      "Epoch: 9902 | train_loss: 116.4505462646 | test_loss: 5.9016542435 | \n",
      "Epoch: 9903 | train_loss: 116.4504394531 | test_loss: 5.9016356468 | \n",
      "Epoch: 9904 | train_loss: 116.4503326416 | test_loss: 5.9016199112 | \n",
      "Epoch: 9905 | train_loss: 116.4502029419 | test_loss: 5.9016075134 | \n",
      "Epoch: 9906 | train_loss: 116.4501037598 | test_loss: 5.9015893936 | \n",
      "Epoch: 9907 | train_loss: 116.4499893188 | test_loss: 5.9015784264 | \n",
      "Epoch: 9908 | train_loss: 116.4498748779 | test_loss: 5.9015583992 | \n",
      "Epoch: 9909 | train_loss: 116.4497451782 | test_loss: 5.9015464783 | \n",
      "Epoch: 9910 | train_loss: 116.4496383667 | test_loss: 5.9015278816 | \n",
      "Epoch: 9911 | train_loss: 116.4494705200 | test_loss: 5.9015026093 | \n",
      "Epoch: 9912 | train_loss: 116.4493637085 | test_loss: 5.9014854431 | \n",
      "Epoch: 9913 | train_loss: 116.4492950439 | test_loss: 5.9014711380 | \n",
      "Epoch: 9914 | train_loss: 116.4491577148 | test_loss: 5.9014544487 | \n",
      "Epoch: 9915 | train_loss: 116.4490661621 | test_loss: 5.9014329910 | \n",
      "Epoch: 9916 | train_loss: 116.4489746094 | test_loss: 5.9014186859 | \n",
      "Epoch: 9917 | train_loss: 116.4488372803 | test_loss: 5.9014024734 | \n",
      "Epoch: 9918 | train_loss: 116.4487228394 | test_loss: 5.9013810158 | \n",
      "Epoch: 9919 | train_loss: 116.4485855103 | test_loss: 5.9013686180 | \n",
      "Epoch: 9920 | train_loss: 116.4484863281 | test_loss: 5.9013481140 | \n",
      "Epoch: 9921 | train_loss: 116.4483871460 | test_loss: 5.9013261795 | \n",
      "Epoch: 9922 | train_loss: 116.4482498169 | test_loss: 5.9013080597 | \n",
      "Epoch: 9923 | train_loss: 116.4481353760 | test_loss: 5.9012908936 | \n",
      "Epoch: 9924 | train_loss: 116.4479980469 | test_loss: 5.9012727737 | \n",
      "Epoch: 9925 | train_loss: 116.4478607178 | test_loss: 5.9012584686 | \n",
      "Epoch: 9926 | train_loss: 116.4477539062 | test_loss: 5.9012403488 | \n",
      "Epoch: 9927 | train_loss: 116.4476623535 | test_loss: 5.9012207985 | \n",
      "Epoch: 9928 | train_loss: 116.4475326538 | test_loss: 5.9012036324 | \n",
      "Epoch: 9929 | train_loss: 116.4473953247 | test_loss: 5.9011878967 | \n",
      "Epoch: 9930 | train_loss: 116.4472732544 | test_loss: 5.9011688232 | \n",
      "Epoch: 9931 | train_loss: 116.4471511841 | test_loss: 5.9011530876 | \n",
      "Epoch: 9932 | train_loss: 116.4470291138 | test_loss: 5.9011344910 | \n",
      "Epoch: 9933 | train_loss: 116.4469451904 | test_loss: 5.9011225700 | \n",
      "Epoch: 9934 | train_loss: 116.4468231201 | test_loss: 5.9011006355 | \n",
      "Epoch: 9935 | train_loss: 116.4467010498 | test_loss: 5.9010834694 | \n",
      "Epoch: 9936 | train_loss: 116.4465789795 | test_loss: 5.9010710716 | \n",
      "Epoch: 9937 | train_loss: 116.4464569092 | test_loss: 5.9010534286 | \n",
      "Epoch: 9938 | train_loss: 116.4463348389 | test_loss: 5.9010343552 | \n",
      "Epoch: 9939 | train_loss: 116.4461898804 | test_loss: 5.9010176659 | \n",
      "Epoch: 9940 | train_loss: 116.4461135864 | test_loss: 5.9009995461 | \n",
      "Epoch: 9941 | train_loss: 116.4459762573 | test_loss: 5.9009828568 | \n",
      "Epoch: 9942 | train_loss: 116.4458694458 | test_loss: 5.9009666443 | \n",
      "Epoch: 9943 | train_loss: 116.4457397461 | test_loss: 5.9009513855 | \n",
      "Epoch: 9944 | train_loss: 116.4456176758 | test_loss: 5.9009313583 | \n",
      "Epoch: 9945 | train_loss: 116.4454956055 | test_loss: 5.9009160995 | \n",
      "Epoch: 9946 | train_loss: 116.4454193115 | test_loss: 5.9008970261 | \n",
      "Epoch: 9947 | train_loss: 116.4452667236 | test_loss: 5.9008808136 | \n",
      "Epoch: 9948 | train_loss: 116.4451675415 | test_loss: 5.9008660316 | \n",
      "Epoch: 9949 | train_loss: 116.4450225830 | test_loss: 5.9008488655 | \n",
      "Epoch: 9950 | train_loss: 116.4448852539 | test_loss: 5.9008293152 | \n",
      "Epoch: 9951 | train_loss: 116.4447631836 | test_loss: 5.9008107185 | \n",
      "Epoch: 9952 | train_loss: 116.4446563721 | test_loss: 5.9007930756 | \n",
      "Epoch: 9953 | train_loss: 116.4445419312 | test_loss: 5.9007744789 | \n",
      "Epoch: 9954 | train_loss: 116.4444122314 | test_loss: 5.9007573128 | \n",
      "Epoch: 9955 | train_loss: 116.4443283081 | test_loss: 5.9007391930 | \n",
      "Epoch: 9956 | train_loss: 116.4442062378 | test_loss: 5.9007201195 | \n",
      "Epoch: 9957 | train_loss: 116.4440536499 | test_loss: 5.9007019997 | \n",
      "Epoch: 9958 | train_loss: 116.4439620972 | test_loss: 5.9006872177 | \n",
      "Epoch: 9959 | train_loss: 116.4438171387 | test_loss: 5.9006700516 | \n",
      "Epoch: 9960 | train_loss: 116.4436950684 | test_loss: 5.9006547928 | \n",
      "Epoch: 9961 | train_loss: 116.4435729980 | test_loss: 5.9006419182 | \n",
      "Epoch: 9962 | train_loss: 116.4434509277 | test_loss: 5.9006218910 | \n",
      "Epoch: 9963 | train_loss: 116.4433212280 | test_loss: 5.9006085396 | \n",
      "Epoch: 9964 | train_loss: 116.4431838989 | test_loss: 5.9005908966 | \n",
      "Epoch: 9965 | train_loss: 116.4430770874 | test_loss: 5.9005794525 | \n",
      "Epoch: 9966 | train_loss: 116.4429779053 | test_loss: 5.9005627632 | \n",
      "Epoch: 9967 | train_loss: 116.4428253174 | test_loss: 5.9005389214 | \n",
      "Epoch: 9968 | train_loss: 116.4427337646 | test_loss: 5.9005260468 | \n",
      "Epoch: 9969 | train_loss: 116.4426193237 | test_loss: 5.9005088806 | \n",
      "Epoch: 9970 | train_loss: 116.4424819946 | test_loss: 5.9004945755 | \n",
      "Epoch: 9971 | train_loss: 116.4423751831 | test_loss: 5.9004778862 | \n",
      "Epoch: 9972 | train_loss: 116.4422531128 | test_loss: 5.9004602432 | \n",
      "Epoch: 9973 | train_loss: 116.4421310425 | test_loss: 5.9004411697 | \n",
      "Epoch: 9974 | train_loss: 116.4420242310 | test_loss: 5.9004168510 | \n",
      "Epoch: 9975 | train_loss: 116.4418563843 | test_loss: 5.9003996849 | \n",
      "Epoch: 9976 | train_loss: 116.4417800903 | test_loss: 5.9003801346 | \n",
      "Epoch: 9977 | train_loss: 116.4416580200 | test_loss: 5.9003648758 | \n",
      "Epoch: 9978 | train_loss: 116.4415206909 | test_loss: 5.9003520012 | \n",
      "Epoch: 9979 | train_loss: 116.4414062500 | test_loss: 5.9003338814 | \n",
      "Epoch: 9980 | train_loss: 116.4412918091 | test_loss: 5.9003224373 | \n",
      "Epoch: 9981 | train_loss: 116.4411621094 | test_loss: 5.9003047943 | \n",
      "Epoch: 9982 | train_loss: 116.4410705566 | test_loss: 5.9002857208 | \n",
      "Epoch: 9983 | train_loss: 116.4409484863 | test_loss: 5.9002680779 | \n",
      "Epoch: 9984 | train_loss: 116.4408187866 | test_loss: 5.9002485275 | \n",
      "Epoch: 9985 | train_loss: 116.4406814575 | test_loss: 5.9002337456 | \n",
      "Epoch: 9986 | train_loss: 116.4406051636 | test_loss: 5.9002170563 | \n",
      "Epoch: 9987 | train_loss: 116.4404525757 | test_loss: 5.9001979828 | \n",
      "Epoch: 9988 | train_loss: 116.4403305054 | test_loss: 5.9001808167 | \n",
      "Epoch: 9989 | train_loss: 116.4402160645 | test_loss: 5.9001631737 | \n",
      "Epoch: 9990 | train_loss: 116.4400863647 | test_loss: 5.9001479149 | \n",
      "Epoch: 9991 | train_loss: 116.4399719238 | test_loss: 5.9001283646 | \n",
      "Epoch: 9992 | train_loss: 116.4398727417 | test_loss: 5.9001135826 | \n",
      "Epoch: 9993 | train_loss: 116.4397201538 | test_loss: 5.9000964165 | \n",
      "Epoch: 9994 | train_loss: 116.4396514893 | test_loss: 5.9000816345 | \n",
      "Epoch: 9995 | train_loss: 116.4394912720 | test_loss: 5.9000663757 | \n",
      "Epoch: 9996 | train_loss: 116.4394149780 | test_loss: 5.9000487328 | \n",
      "Epoch: 9997 | train_loss: 116.4393005371 | test_loss: 5.9000291824 | \n",
      "Epoch: 9998 | train_loss: 116.4392013550 | test_loss: 5.9000148773 | \n",
      "Epoch: 9999 | train_loss: 116.4391021729 | test_loss: 5.9000010490 | \n",
      "Total training time: 4667.360 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set number of epochs\n",
    "NUM_EPOCHS = 10000\n",
    "\n",
    "# Create model_1\n",
    "model_1 = RNOG_CNN_alpha_1(input_shape=1, # number of color channels (3 for RGB) \n",
    "                  hidden_units=10, \n",
    "                  output_shape=1)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "#loss_fn = nn.HuberLoss(delta=0.01)\n",
    "#loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(params=model_1.parameters())\n",
    "scheduler = False#ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_1_results = train(model=model_1, \n",
    "                        train_dataloader=train_data_loader,\n",
    "                        test_dataloader=test_data_loader,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS,\n",
    "                        targ_loss=10)\n",
    "    \n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss curves with improved aesthetics:\n",
    "epochs = np.arange(NUM_EPOCHS)\n",
    "train_loss = model_1_results['train_loss']\n",
    "test_loss = model_1_results['test_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='-', marker='o')\n",
    "plt.plot(epochs, test_loss, label='Test Loss', color='red', linestyle='--', marker='x')\n",
    "\n",
    "# Plot minimum loss horizontal lines\n",
    "min_train_loss = min(train_loss)\n",
    "min_test_loss = min(test_loss)\n",
    "plt.axhline(y=min_train_loss, color='blue', linestyle=':', label=f'Min Train Loss: {min_train_loss:.6f}')\n",
    "plt.axhline(y=min_test_loss, color='red', linestyle=':', label=f'Min Test Loss: {min_test_loss:.6f}')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Testing Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Point 1: Loss = 0.534524\n",
      "Data Point 2: Loss = 0.000316\n",
      "Data Point 3: Loss = 1.849320\n",
      "Data Point 4: Loss = 0.269094\n",
      "Data Point 5: Loss = 0.485888\n",
      "Data Point 6: Loss = 0.000406\n",
      "Data Point 7: Loss = 0.107115\n",
      "Data Point 8: Loss = 0.052681\n",
      "Data Point 9: Loss = 0.117072\n",
      "Data Point 10: Loss = 0.078168\n",
      "Data Point 11: Loss = 0.059000\n",
      "Data Point 12: Loss = 0.811219\n",
      "Data Point 13: Loss = 1.232294\n",
      "Data Point 14: Loss = 1.117391\n",
      "Data Point 15: Loss = 1.128440\n",
      "Data Point 16: Loss = 0.001551\n",
      "Data Point 17: Loss = 0.042643\n",
      "Data Point 18: Loss = 0.168467\n",
      "Data Point 19: Loss = 1.147440\n",
      "Data Point 20: Loss = 0.065877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/i3home/ssued/venv_ubu22.04/lib/python3.10/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "eventbatch = utils.obtain_evb('eventbatch.pkl')\n",
    "# Select 20 random events from eventbatch\n",
    "random_events = dict(random.choices(list(eventbatch.items()), k=20))\n",
    "\n",
    "random_data = EventtoData(random_events)\n",
    "\n",
    "random_dataloader = DataLoader(random_data,1,False)\n",
    "\n",
    "model_1.eval()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, (data, snr) in enumerate(random_dataloader):\n",
    "        data, snr = data.to(device), snr.to(device)\n",
    "        pred = model_1(data)\n",
    "        loss = loss_fn(pred, snr)\n",
    "        print(f\"Data Point {i + 1}: Loss = {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 predictions (Predicted SNR, True SNR):\n",
      "Sample 1: Predicted = 2.6972529888153076, True = 3.4283645153045654\n",
      "Sample 2: Predicted = 3.3044443130493164, True = 3.3222167491912842\n",
      "Sample 3: Predicted = 4.5270366668701172, True = 3.1671395301818848\n",
      "Sample 4: Predicted = 2.5971875190734863, True = 3.1159298419952393\n",
      "Sample 5: Predicted = 2.6253314018249512, True = 3.3223881721496582\n",
      "Sample 6: Predicted = 3.2968165874481201, True = 3.3169670104980469\n",
      "Sample 7: Predicted = 3.5233242511749268, True = 3.1960403919219971\n",
      "Sample 8: Predicted = 4.3242530822753906, True = 4.5537772178649902\n",
      "Sample 9: Predicted = 3.6842732429504395, True = 3.3421146869659424\n",
      "Sample 10: Predicted = 2.9123375415802002, True = 3.1919224262237549\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_predictions(model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, device: torch.device = device, plot: bool = False, verbose: bool = False):\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for data, snr in dataloader:\n",
    "            data = data.to(device)\n",
    "            pred = model(data)\n",
    "            predictions.append((pred.cpu().item(), snr.cpu().item()))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"First 10 predictions (Predicted SNR, True SNR):\")\n",
    "        for i, (pred, true) in enumerate(predictions[:10]):\n",
    "            print(f\"Sample {i + 1}: Predicted = {pred:.16f}, True = {true:.16f}\")\n",
    "\n",
    "    if plot:\n",
    "        pred_values, true_values = zip(*predictions)\n",
    "        samples = np.arange(len(pred_values))\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(samples, pred_values, label='Predicted SNR', marker='o', linestyle='-', color='blue')\n",
    "        plt.plot(samples, true_values, label='True SNR', marker='x', linestyle='--', color='red')\n",
    "        plt.xlabel('Data Sample', fontsize=14)\n",
    "        plt.ylabel('SNR', fontsize=14)\n",
    "        plt.title('Predicted vs True SNR', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.show()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "preds = make_predictions(model_1, random_dataloader, device=device, plot=True,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv_block_1.0.weight',\n",
       "              tensor([[[[-4.4475e-03, -5.5405e-03,  6.6097e-04,  1.2123e-02,  1.5490e-02,\n",
       "                          9.6816e-03,  9.1348e-03,  9.3362e-03,  2.8032e-02,  3.8444e-02,\n",
       "                          2.7945e-02,  3.1782e-02],\n",
       "                        [ 1.6817e-02,  2.3898e-02,  4.1770e-02,  3.6497e-02,  5.1481e-02,\n",
       "                          4.8207e-02,  1.9920e-02,  3.9855e-02,  4.0332e-02,  5.2953e-02,\n",
       "                          2.0110e-03,  4.5147e-02],\n",
       "                        [ 1.5508e-02,  1.5697e-02,  2.2490e-02,  1.2571e-02,  8.5291e-03,\n",
       "                         -2.8816e-03, -8.2365e-03, -1.0834e-02, -4.3245e-03,  2.6685e-02,\n",
       "                          1.1218e-02,  1.6262e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5107e-01, -1.8877e-01, -1.9741e-01, -5.4915e-01, -2.8792e-01,\n",
       "                          1.2915e-01, -2.2034e-01, -7.7627e-01,  9.0210e-02,  1.1021e+00,\n",
       "                          8.1549e-01, -1.1554e+00],\n",
       "                        [ 2.8983e-01, -4.0240e-01,  1.6990e-02, -1.6948e-01,  1.1783e-01,\n",
       "                          1.2365e-01,  5.5132e-02, -1.4223e-01, -2.3834e-01,  3.1596e-01,\n",
       "                          4.0648e-01, -3.9924e-01],\n",
       "                        [ 1.9381e+00,  4.6414e-01,  2.9310e-01, -2.6034e-01, -2.0326e-02,\n",
       "                          8.5772e-01, -3.7803e-02, -1.4507e-01, -2.0739e-01,  1.1059e+00,\n",
       "                          1.2209e+00,  9.8128e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8329e-01, -1.7827e-01, -1.8890e-01,  2.6724e-01, -9.4357e-02,\n",
       "                          5.8583e-01, -2.1690e-02,  1.1133e-01, -3.2724e-02,  5.9350e-01,\n",
       "                          1.0461e+00, -2.6599e-01],\n",
       "                        [-4.0511e-01,  5.1604e-02,  1.8814e-01,  4.6128e-01, -1.2771e-01,\n",
       "                          3.3149e-01,  1.0672e-01,  1.5696e-01,  3.5496e-01,  3.2997e-01,\n",
       "                          4.2398e-01,  2.3619e-02],\n",
       "                        [-4.9743e-01, -3.8013e-01, -2.5444e-01,  2.7652e-01, -1.1547e-01,\n",
       "                          3.0040e-01, -8.9370e-02, -1.3209e-01,  1.6678e-01,  8.1988e-01,\n",
       "                          9.6678e-01, -3.6393e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2630e-04, -6.0416e-03,  1.3115e-03,  8.9209e-03,  1.4265e-02,\n",
       "                          6.7832e-03,  5.7665e-03,  7.6428e-03,  2.4123e-02,  3.1354e-02,\n",
       "                          2.4623e-02,  2.5932e-02],\n",
       "                        [ 1.6876e-02,  2.1688e-02,  3.8501e-02,  3.1853e-02,  4.7373e-02,\n",
       "                          4.0211e-02,  1.6723e-02,  3.4210e-02,  3.4944e-02,  4.3129e-02,\n",
       "                          4.1497e-03,  3.7287e-02],\n",
       "                        [ 1.2792e-02,  1.4798e-02,  1.9135e-02,  7.8686e-03,  8.3266e-03,\n",
       "                         -4.8091e-03, -6.4199e-03, -7.5811e-03, -2.2425e-03,  1.9065e-02,\n",
       "                          1.1684e-02,  1.3753e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.5636e-01, -5.4782e-01, -1.3215e-02, -8.8465e-02, -4.4758e-01,\n",
       "                          1.6999e-01, -3.9780e-01,  2.1623e-01, -5.5666e-02,  5.0294e-01,\n",
       "                          1.1863e+00,  8.3735e-01],\n",
       "                        [-7.5002e-02, -1.4021e-01, -1.2352e-01, -2.6998e-01,  1.9403e-01,\n",
       "                         -2.0835e-01,  2.7444e-02, -2.3899e-02,  1.2521e-01,  2.5414e-01,\n",
       "                          9.0776e-01,  9.3556e-01],\n",
       "                        [-7.4192e-01, -5.8881e-01, -1.6185e-01,  3.1843e-01, -2.1473e-01,\n",
       "                         -3.4141e-01, -4.4154e-02, -2.7214e-01, -8.9896e-02,  7.5062e-01,\n",
       "                          1.3245e+00,  1.9393e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.7710e-01, -3.5349e-01,  1.8923e-01,  3.2551e-02, -3.4516e-01,\n",
       "                          1.9161e-01, -1.1267e-01,  3.5226e-01, -9.2283e-02, -8.9882e-01,\n",
       "                         -8.5779e-01, -5.6028e-01],\n",
       "                        [-5.8559e-02,  5.5599e-02, -1.0286e-01,  2.4267e-02, -1.0041e-01,\n",
       "                          1.1488e-01,  1.9732e-02, -2.6878e-02,  7.0893e-03, -1.5919e-01,\n",
       "                         -2.6141e-02, -1.8087e-01],\n",
       "                        [-2.1159e-01,  1.0438e-01,  1.4859e-01, -1.8168e-01, -6.7451e-02,\n",
       "                          8.2874e-02, -5.5424e-02,  2.3623e-01,  3.6302e-02, -4.6995e-01,\n",
       "                         -1.3104e-01, -6.5538e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6975e+00,  6.9329e-01, -1.9888e-01,  1.0726e-02, -4.6201e-01,\n",
       "                          3.2852e-01, -1.8761e-01, -3.8809e-01,  1.5810e-02, -8.4658e-01,\n",
       "                         -1.7855e-01,  2.9583e-02],\n",
       "                        [ 7.2477e-01,  3.0284e-01, -1.6286e-01,  2.5014e-01, -4.5422e-01,\n",
       "                         -4.2205e-02, -9.4437e-02, -2.8448e-01, -2.2510e-01, -3.5577e-02,\n",
       "                          3.2358e-01, -1.7826e-01],\n",
       "                        [ 8.1722e-01,  4.7135e-01,  4.4300e-01,  9.7405e-02, -5.2393e-01,\n",
       "                          4.3012e-01,  8.8218e-02, -4.4498e-01,  5.1580e-02,  3.3326e-01,\n",
       "                          6.4809e-01,  1.4227e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.6301e-03,  6.3743e-03, -1.3296e-03, -9.7081e-03, -1.5022e-02,\n",
       "                         -7.4445e-03, -7.0332e-03, -8.5161e-03, -2.5525e-02, -3.3339e-02,\n",
       "                         -2.6223e-02, -2.7770e-02],\n",
       "                        [-1.6708e-02, -2.2662e-02, -3.9974e-02, -3.3046e-02, -4.9328e-02,\n",
       "                         -4.2482e-02, -1.8136e-02, -3.6058e-02, -3.7240e-02, -4.6056e-02,\n",
       "                         -3.9576e-03, -3.9782e-02],\n",
       "                        [-1.3720e-02, -1.5439e-02, -2.0165e-02, -9.2456e-03, -9.1138e-03,\n",
       "                          4.8510e-03,  6.7665e-03,  8.3531e-03,  2.6723e-03, -2.0895e-02,\n",
       "                         -1.2118e-02, -1.4433e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.1233e-01, -3.7226e-01, -3.5681e-02, -1.6290e-01,  1.7136e-01,\n",
       "                         -3.0980e-02, -2.3380e-01, -4.0342e-01, -1.9511e-01,  9.1337e-01,\n",
       "                          1.3233e+00,  1.5326e+00],\n",
       "                        [-3.7022e-01,  1.0550e-01, -1.0713e-02,  1.5049e-01,  2.5158e-01,\n",
       "                          4.2989e-02, -2.3547e-02,  6.7355e-02,  8.0916e-02,  2.1351e-01,\n",
       "                          1.9949e-01,  4.6903e-01],\n",
       "                        [-2.7308e-01, -2.2000e-01,  1.0744e-01, -3.2494e-01,  9.8322e-02,\n",
       "                          3.7195e-02,  1.9561e-02, -1.1500e-01, -8.4284e-02,  6.7431e-01,\n",
       "                          8.7207e-01,  7.0226e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0886e-02,  5.6263e-03, -1.9159e-03, -1.9772e-02, -2.1601e-02,\n",
       "                         -1.5450e-02, -1.7486e-02, -1.3957e-02, -3.7578e-02, -5.0494e-02,\n",
       "                         -3.7220e-02, -4.3312e-02],\n",
       "                        [-1.5077e-02, -2.9070e-02, -5.0266e-02, -4.4805e-02, -6.3674e-02,\n",
       "                         -6.3769e-02, -2.8958e-02, -5.2519e-02, -5.6181e-02, -7.2178e-02,\n",
       "                          4.8894e-04, -6.0746e-02],\n",
       "                        [-2.0049e-02, -1.9578e-02, -3.1079e-02, -2.4494e-02, -1.4029e-02,\n",
       "                          1.8899e-03,  1.1432e-02,  1.7710e-02,  7.9819e-03, -3.9421e-02,\n",
       "                         -1.2365e-02, -1.9997e-02]]]])),\n",
       "             ('conv_block_1.0.bias',\n",
       "              tensor([-0.3509, -2.0247, -1.9249, -0.3037, -2.1399,  2.1965, -2.2047,  0.3205,\n",
       "                      -2.2652,  0.4612])),\n",
       "             ('conv_block_1.1.weight',\n",
       "              tensor([[[[ 8.8110e-05, -1.8591e-03, -5.4953e-03, -9.6002e-04, -1.8035e-03,\n",
       "                         -5.9579e-03, -2.4812e-03, -3.3029e-03, -1.1059e-02, -5.6543e-03,\n",
       "                         -1.5975e-03, -4.8194e-03, -4.2320e-03, -1.4490e-03],\n",
       "                        [ 5.4721e-03, -1.6306e-03, -4.1507e-03,  5.9805e-03,  3.9232e-03,\n",
       "                         -5.9380e-04,  8.3193e-03,  1.2538e-02,  3.5282e-03,  4.7669e-03,\n",
       "                          7.9272e-03, -2.3068e-03, -8.1778e-04,  2.2444e-03]],\n",
       "              \n",
       "                       [[-3.1421e-01, -7.0037e-01,  2.3539e-01,  1.5762e-01,  2.3747e-01,\n",
       "                          2.5145e-02, -3.0131e-02,  2.3823e-01, -1.9634e-02, -2.3577e-01,\n",
       "                          6.5125e-03,  1.0491e+00,  5.9375e-01, -5.5835e-01],\n",
       "                        [-9.0165e-01, -1.5064e+00,  2.3859e-02,  4.5166e-01,  6.1729e-01,\n",
       "                          8.6142e-02,  5.5757e-01,  1.8146e-01, -3.5737e-02,  1.9391e-01,\n",
       "                         -1.4267e-01,  3.0805e-01, -4.7446e-01, -1.0112e+00]],\n",
       "              \n",
       "                       [[ 1.8097e-02,  1.2835e-01,  2.9713e-01, -3.7073e-01, -1.8212e-01,\n",
       "                         -8.9703e-02,  1.5676e-01,  1.3841e-01, -4.2846e-01,  2.7912e-01,\n",
       "                         -6.5727e-01,  6.5271e-01,  3.5099e-02, -5.2638e-01],\n",
       "                        [ 8.1488e-01, -2.1282e-01,  3.1981e-01, -3.4532e-01, -1.0529e-01,\n",
       "                         -8.9848e-02, -5.2382e-01,  4.0533e-01, -2.7953e-01,  4.2025e-01,\n",
       "                         -2.8942e-01,  6.1832e-01,  1.2634e-01, -7.3865e-01]],\n",
       "              \n",
       "                       [[ 4.1425e-04, -1.3606e-03, -5.3758e-03, -1.2150e-03, -1.3432e-03,\n",
       "                         -6.0200e-03, -2.4524e-03, -3.6404e-03, -1.1026e-02, -5.9270e-03,\n",
       "                         -1.9123e-03, -4.8588e-03, -3.9576e-03, -1.6296e-03],\n",
       "                        [ 5.9236e-03, -1.4787e-03, -4.2368e-03,  5.1364e-03,  4.1462e-03,\n",
       "                         -1.6562e-03,  7.8657e-03,  1.1739e-02,  3.4079e-03,  4.3598e-03,\n",
       "                          7.6615e-03, -2.7347e-03, -1.2235e-03,  1.7570e-03]],\n",
       "              \n",
       "                       [[ 1.1402e+00,  1.4784e-01, -3.6587e-01, -3.5458e-01, -7.1539e-02,\n",
       "                         -5.4047e-01,  1.9065e-01,  4.6711e-01, -7.2508e-01,  1.0617e+00,\n",
       "                          1.3218e-01, -1.0433e+00, -1.1901e+00, -1.4635e+00],\n",
       "                        [ 1.8459e+00,  8.0815e-01,  8.7392e-01,  4.9380e-01, -4.5453e-01,\n",
       "                          6.3595e-01, -3.1714e-01, -3.2913e-01,  2.5675e-01,  3.7568e-01,\n",
       "                          9.8689e-01, -1.0135e+00, -2.0912e+00, -1.9096e+00]],\n",
       "              \n",
       "                       [[ 1.3107e+00,  1.6768e+00,  8.3855e-01,  2.2576e-01,  2.2542e-01,\n",
       "                         -1.4465e-02,  9.4549e-01,  5.1989e-01, -8.1195e-01,  6.6898e-01,\n",
       "                         -5.2782e-01,  2.5531e-01,  7.5000e-01,  1.8484e+00],\n",
       "                        [-4.9774e-02,  1.2500e+00,  7.7121e-01,  2.0380e-01,  7.4505e-01,\n",
       "                         -3.8466e-02,  1.0339e+00,  8.1241e-01,  1.2347e-01,  1.3696e+00,\n",
       "                         -6.3366e-02,  8.0120e-01,  6.6890e-01,  1.0610e+00]],\n",
       "              \n",
       "                       [[-1.8918e+00, -9.8619e-01, -1.0623e+00,  8.6567e-01, -3.6005e-01,\n",
       "                         -3.4908e-01, -2.0148e-01, -8.2617e-01, -1.5514e-01, -4.9637e-01,\n",
       "                         -6.1031e-01,  2.3684e-01, -4.6451e-01,  7.0173e-01],\n",
       "                        [-1.1844e+00, -5.4220e-01, -7.2531e-01,  5.7001e-01, -7.2572e-01,\n",
       "                         -4.2518e-01,  6.0379e-02, -8.3808e-01, -1.8505e-01,  3.3200e-02,\n",
       "                         -5.8210e-02, -9.8258e-01, -1.7351e+00, -1.2454e+00]],\n",
       "              \n",
       "                       [[-2.5873e-04,  1.5710e-03,  5.3634e-03,  1.1914e-03,  1.4539e-03,\n",
       "                          6.1046e-03,  2.3796e-03,  3.5487e-03,  1.1071e-02,  5.8788e-03,\n",
       "                          1.7403e-03,  4.9187e-03,  3.9750e-03,  1.6133e-03],\n",
       "                        [-5.7584e-03,  1.4932e-03,  4.2103e-03, -5.3895e-03, -4.1808e-03,\n",
       "                          1.4056e-03, -8.1743e-03, -1.1974e-02, -3.5300e-03, -4.4229e-03,\n",
       "                         -7.8919e-03,  2.6630e-03,  1.0672e-03, -1.9477e-03]],\n",
       "              \n",
       "                       [[-3.1374e-02,  2.3420e-02, -8.1817e-01, -2.3703e-01, -4.6191e-01,\n",
       "                         -4.5341e-01, -4.9386e-01, -5.6109e-01, -8.4026e-01, -6.6877e-01,\n",
       "                          3.6935e-01, -1.4614e+00, -2.1929e+00, -3.0733e+00],\n",
       "                        [-3.4608e-01, -4.0521e-01,  1.0813e-01, -8.4387e-02, -3.5988e-01,\n",
       "                         -3.0199e-01, -4.4210e-01, -4.5379e-03, -4.1193e-01, -3.2018e-01,\n",
       "                          2.4004e-01, -5.8363e-01, -1.1542e+00, -1.6560e+00]],\n",
       "              \n",
       "                       [[ 7.8085e-04,  2.8167e-03,  5.1735e-03,  6.8981e-04,  2.4622e-03,\n",
       "                          5.8694e-03,  2.0234e-03,  2.6587e-03,  1.0644e-02,  5.1503e-03,\n",
       "                          5.6153e-04,  4.9742e-03,  4.4936e-03,  1.3277e-03],\n",
       "                        [-4.1916e-03,  1.5199e-03,  3.7785e-03, -7.4406e-03, -3.9071e-03,\n",
       "                         -1.4617e-03, -9.3211e-03, -1.3530e-02, -3.9248e-03, -4.8764e-03,\n",
       "                         -8.4169e-03,  1.4795e-03, -1.0426e-04, -3.2063e-03]]]])),\n",
       "             ('conv_block_1.1.bias', tensor([2.1083]))])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create model directory path\n",
    "MODEL_PATH = Path('models')\n",
    "MODEL_PATH.mkdir(parents=True,\n",
    "                 exist_ok=True)\n",
    "\n",
    "# Create model save path\n",
    "MODEL_NAME='RNOG_alpha_model_1.pth'\n",
    "MODEL_SAVE_PATH=MODEL_PATH/MODEL_NAME\n",
    "\n",
    "print(f'Saving model to: {MODEL_SAVE_PATH}')\n",
    "torch.save(obj=model_1.state_dict(),f=MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ubu22.04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
