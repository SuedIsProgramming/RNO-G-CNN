{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09/22/2024\n",
    "\n",
    "I haven't touched much of this project in a while due to the start of classes. I have been reading over the project to try to refresh a few of the materials that need work. So far I have identified that the next step is to try to get the data set up similar to the CIFAR-10 dataset so that I can utilize a lot of the infrastructure of the CIFAR-10 dataset to undergo machine learning.\n",
    "\n",
    "For each saved trace I should have an associated average SNR label. I should have a dictionary with two entries {data, labels} Where the nth label corresponds to the nth data entry, which itself should be a [eventnum,data]. After this, I plan to bin these arrays and save them into batches.\n",
    "\n",
    "But what should data be? Right now, data is a [4(num_channels),2(t,V),ndata]. And we may need to keep it that way, as the number of data points are inconsistent from the NuRadioMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09/29/2024\n",
    "\n",
    "I started working on the bin matrix, I made the `bin_matrix() function, which will automatically set up my data in [4,bins] matrix and has an input \"plotting\" that will print out a heatmap of the matrix.\n",
    "\n",
    "TODO: Finish commenting the function. And I will want to setup a way to do this recursively with all of the data. I also should start looking into how to calculate the mean SNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10/7/2024\n",
    "\n",
    "Started work on step4, realized that calculate noise function works with bins. I am now working on the overall saving of these binned matrices + their mean SNR in an orderly fashion. I calculate the SNR by max V / rms noise, however I am not sure if the noise should be from the bins or from the original samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 channels in simstation because of em and hadronic showers and because of direct and refracted ray tracing solutions. Sum everything up to obtain max voltage. \n",
    "\n",
    "Try to look for sum of all channels function.\n",
    "Try to look for noise parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/2/2024\n",
    "\n",
    "I am looking into all of the different functions of NuRadioMC. I have discovered many useful things such as an integrated hilbert_voltage function, SNR, etc. \n",
    "\n",
    "Although it is not possible to index events and stations and channels. They can be iterated over with the next() function. To restart interation from 0, simply restate the iterable call. Perhaps a python package exists that streamlines this, but for now this is enough.\n",
    "\n",
    "I also discovered that the simstation has raytracing channels, particle channels and id channels separations, which may be useful for future calculations.\n",
    "\n",
    "In the essence of time, and because I believe that the CNN's main objective is to accurately measure the \"REAL\" SNR with noise included, I have decided to utilize the \"SNR\" parameter of the channels.\n",
    "\n",
    "I have started construction of the CIFAR-10 dictionary which will have the shape of {data, SNR}\n",
    "\n",
    "Did a big overhaul of most things.\n",
    "Added new aliases to streamline process:\n",
    "\n",
    "alias gostepsCNN='cd /data/i3home/ssued/RNOGCnn/CNN_steps'\n",
    "alias goCNN='cd /data/i3home/ssued/RNOGCnn'\n",
    "alias runstep1='python /data/i3home/ssued/RNOGCnn/CNN_steps/step1_generate_event_list.py'\n",
    "alias runstep2='python /data/i3home/ssued/tutorials-rnog/plot_sim_event/step2_run_simulation.py'\n",
    "alias runstep3='python /data/i3home/ssued/RNOGCnn/CNN_steps/step3_save_hilbert_and_SNR.py'\n",
    "alias runstep4='/data/i3home/ssued/RNOGCnn/CNN_steps/step4_save_binned_data.py'\n",
    "\n",
    "Also finished up data processing. I decided to use integrated power as it was stated by my mentor that it is similar to SNR for hilbert and because it came prepacked from NuRadioMC.\n",
    "\n",
    "Have to debug step 4 and next step is starting on the CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/3/2024\n",
    "\n",
    "Im basically done with the data processing part. We can begin with data collection, but I will have to make everything much more streamlined and pretty.\n",
    "\n",
    "TO PRESENT:\n",
    "\n",
    " Wrote aliases to streamline process in my shell, maybe not so useful in condor.\n",
    "> runstep1, runstep2, runstep3, runstep4\n",
    "\n",
    " step1: setup simulation, randomness because seed = 0\n",
    " step2: run simulation, random noise generated\n",
    " step3: Will save dictionary with {mean_integrated_power,data} using functions native to NuRadioMC\n",
    " step4: Will open the saved dictionary, obtain the integrated power and use pre-built functions bin_matrix and bin_v to bin the voltage\n",
    "which will then be saved to a new: {mean_integrated_power,bin_time,data} dictionary\n",
    "\n",
    "Decided to save integrated power as it was stated that it was similar to SNR for bin hilbert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/17/2024\n",
    "\n",
    "Done with data processing. Cut out fourth step per request of my mentor and decided to simply make the third step bin and save the data, RAW DATA WILL NOT BE SAVED now.\n",
    "\n",
    "Also modified mean_integrated_power to peak_amplitude, as according to the NuRadioMC code, peak_amplitude = SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11/24/2024\n",
    "\n",
    "Huge changes, I finally understood how to setup and run my steps on condor. I still have a lot of things to flesh out:\n",
    "* How do I submit multiple of the same request? Workaround solution right now.\n",
    "* SSH for condor? Why is it a shell?\n",
    "* How do I establish what specs to utilize for these requests?\n",
    "* Not sure how to establish checks in case my code breaks. Will it halt incase it fails? Or will it keep going with the next step, if so thats bad.\n",
    "\n",
    "All of the changes are in the \"jobs\" folder that I added to the CNN_steps file. I am not sure if I should gitignore this folder however, because it doesn't really contain any real code, just how to run it in condor00. Right now, the code runs and cycles the files in the \"data\" folder. I would probably benefit from making my file outputs for the dagman more concise/easier to read but I am not sure yet how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "request_memory no more than 1 gb\n",
    "request_disk no more than 1 gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/21/2024\n",
    "\n",
    "Checklist for Winter (I will update every Monday):\n",
    "\n",
    "**(Week Dec.23 - Dec. 27)** Figure out trace data storage / Implement CNN\n",
    "* Figure out DAGMAN multiple jobs\n",
    "* Figure out memory accomodations\n",
    "* Corroborate traces are being saved properly through condor\n",
    "* Work on CIFAR10 replica model\n",
    "\n",
    "**(Week Dec.30 - Jan. 3)** Finish trace data storage / Work on test CNN\n",
    "* Finish step 4 + DAGMAN multiple jobs\n",
    "* Figure out memory accomodations\n",
    "* Corroborate traces are being saved properly through condor\n",
    "* Work on CIFAR10 replica model\n",
    "\n",
    "**(Week Jan. 6 - Jan. 10)** Hopefully begin work on reconstruction CNN\n",
    "\n",
    "**(Week Jan. 13 - Jan. 17)** TBA\n",
    "\n",
    "**(Week Jan. 20 - Jan. 24)** TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/23/2024\n",
    "\n",
    "I have begun working on consolidating the data. I also would like to make sure I can test certain things easily, so I will invest some time into that as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/24/2024\n",
    "I made a grave mistake yesterday and accidentally deleted an important file without any backup. I have now rebuilt it so all should be fine, I even included a few upgrades.\n",
    "\n",
    "CNN_steps file now has:\n",
    "* symdata: This file is where the data files for the simulations are stored. Everytime a simulation is run, the files are overwritten but this cycle is contained inside of this file.\n",
    "* eventdata: This file contains all of the data produced by step3\n",
    "* jobs: This file contains anything related to jobs and DAGMAN\n",
    "\n",
    "For using condor:\n",
    "* first ssh condor00\n",
    "* Use alias gotojobs to go to the job directory\n",
    "* type condor_submit_dag -f dagman.dag\n",
    "\n",
    "Next steps:\n",
    "* Write step 4 which will periodically save the created eventbatch_x files to the cummulative eventbatch dictionary.\n",
    "* Begin CNN implementation with event_dict.pkl events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/29/2024\n",
    "\n",
    "Have made considerable progress on data processing.\n",
    "\n",
    "Additions:\n",
    "* Modified utils.py to include a conjoin events function used in step 4.\n",
    "* Wrote step 4\n",
    "* Added a new \"create_dagman.py\" file that will create a dagman.dag file with multiple simulation runs. (NOT FINISHED, RIGHT NOW WILL RUN STEP 4 after every simulation which is not ideal.)\n",
    "\n",
    "Next steps:\n",
    "* Finish create_dagman.py and verify datastorage works.\n",
    "* Begin CNN!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/30/2024\n",
    "\n",
    "Observations:\n",
    "* For the DAGMAN, CHILD and PARENT dependencies order does not matter.\n",
    "* watch -n0 \"command\" will periodically run a command, good for watching condor queue.\n",
    "\n",
    "Changes:\n",
    "* Modified utils.conjoinevents() so that it would not create a new dictionary everytime, heavily speeding up time for bigger number dictionaries.\n",
    "* Finished DAGMAN steps!\n",
    "\n",
    "Issues:\n",
    "* Cannot run ONE simulation in the create_dagman.py file\n",
    "* Have not looked at memory and GPU necessities yet\n",
    "* Too many damn files for jobs/dagman! How to organize?\n",
    "* Comment step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/30/2024\n",
    "\n",
    "Observations:\n",
    "* For the DAGMAN, CHILD and PARENT dependencies order does not matter.\n",
    "* watch -n0 \"command\" will periodically run a command, good for watching condor queue.\n",
    "\n",
    "Changes:\n",
    "* Modified utils.conjoinevents() so that it would not create a new dictionary everytime, heavily speeding up time for bigger number dictionaries.\n",
    "* Finished DAGMAN steps!\n",
    "\n",
    "Issues:\n",
    "* Cannot run ONE simulation in the create_dagman.py file ✅\n",
    "* Have not looked at memory and GPU necessities yet ✅\n",
    "* Too many damn files for jobs/dagman! How to organize? ❔\n",
    "* Comment step4 ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1/2/2025\n",
    "\n",
    "According to testing, maximum memory consuption is step2 with 225MB, I am limiting memory requirements to 400MB, which should be enough. I have not yet found a way to organize the dagfiles, but for now it should be ok. I also suppressed the output of individual jobs to limit file sizes.\n",
    "\n",
    "**Observation**\n",
    "* Will have to clear out dagman. and job. files lest they get big. After every simulation, the job data will be appended."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
